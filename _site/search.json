[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "On this personal site, I share some side projects where I apply my own data learnings to interesting questions in health care, sports, the outdoors, or whatever strikes my fancy. Opinions shared here reflect my own views and not those of my employer."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "health policy, data, sports, et cetera",
    "section": "",
    "text": "Posit Table Contest Submission: Amtrak System\n\n\n45 min\n\n\n\ngeospatial\n\n\nreactable\n\n\ntransportation\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Oddities of Public School Districts\n\n\n11 min\n\n\n\ngeospatial\n\n\neducation\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the claimsdb package for teaching and learning\n\n\n10 min\n\n\n\nhealth care\n\n\nclaims\n\n\nMedicare\n\n\ndatabases\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolitely Scraping Health Insurance Data\n\n\n24 min\n\n\n\nwebscraping\n\n\nrvest\n\n\npolite\n\n\ngganimate\n\n\ngt\n\n\n\n\nNov 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiveThirtyEight Riddler Challenge: Can You Find The Fish In State Names?\n\n\n1 min\n\n\n\nfivethirtyeight riddler\n\n\n\n\nMay 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRayshading the Big Sur Marathon\n\n\n14 min\n\n\n\nrayshader\n\n\nrunning\n\n\nmarathons\n\n\ngeospatial\n\n\n\n\nMay 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollege Football Bar Chart Races\n\n\n10 min\n\n\n\ncollege football\n\n\nbar chart race\n\n\n\n\nJul 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’\n\n\n6 min\n\n\n\nfivethirtyeight riddler\n\n\nbasketball\n\n\n\n\nApr 14, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html",
    "href": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "",
    "text": "This weekend, I attempted to solve my first FiveThirtyEight Riddler challenge. Many of these challenges require a bit more probability theory than I’m comfortable with, but this week’s classic challenge hit a subject that I care too much about: college basketball national champions and the bragging rights that come from beating the champ:\n\nOn Sunday, the Baylor Lady Bears won the 2019 NCAA women’s basketball championship, and on Monday, the Virginia Cavaliers did the same on the men’s side. But what about all of the unsung transitive champions? For example, earlier in the season, Florida State beat Virginia, thereby laying claim to a transitive championship for the Seminoles. And Boston College beat Florida State, claiming one for the Eagles. And IUPUI beat Boston College, and Ball State beat IUPUI, and so on and so on. Baylor, meanwhile, only lost once, to Stanford, who lost to five teams, and so on. How many transitive national champions were there this season in the women’s and men’s games? Or, maybe more descriptively, how many teams weren’t transitive national champions? You should include tournament losses in your calculations.\n\nThe author (Oliver Roeder) then supplies links to the results of women’s and men’s basketball for the 2018-2019 season from masseyratings.com. On first inspection of the game results, they seem to be in a text format that could be scrapped. In addition, I noticed that the women’s link includes 27,266 games, while the men’s link contains only 6,048. The women’s results page includes several junior colleges and even my alma mater Hastings College, who I know compete at the NAIA level, not NCAA Division I. The men’s results page includes only Division I teams, and since the challenge only mentions Baylor and Virginia, I’m assuming we want to compare Division I transitive champions. I used another link to pull the women’s results for 5,638 Division I games."
  },
  {
    "objectID": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html#scraping-the-game-results-data",
    "href": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html#scraping-the-game-results-data",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "Scraping the game results data",
    "text": "Scraping the game results data\nI decided to tackle this challenge using the tidyverse family of R packages that can scrap the data and wrangle it into a tidy format for further analysis.\nOne major challenge is that I wrote a function to parse the college names and scores from the Massey Rating text. This involves some very gnarly regular expression writing as you can see.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rvest)\n\n\nbb_games_process <- function(x){\n  read_html(x) %>% \n    html_node(xpath = '/html/body/pre/text()[1]') %>% \n    html_text() %>% \n    enframe() %>% \n    separate_rows(value, sep = \"\\n\") %>% \n    mutate(value = str_squish(value),\n           game = substring(value, 12),\n           date = ymd(substr(value,1,10)),\n           win_team = str_extract(game, \"^[\\\\@]{0,1}[:alpha:]{1,}[:blank:]{0,1}[:punct:]{0,1}[:blank:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}\"),\n           win_pts = as.integer(str_extract(game, \"(?<=[:alpha:]{1,100}[:blank:]{1})[:digit:]{1,3}\")),\n           lose_team = str_extract(game, \"(?<=[:digit:]{1,3}[:blank:]{1})[\\\\@]{0,1}[:alpha:]{1,}[:blank:]{0,1}[:punct:]{0,1}[:blank:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}\"),\n           lose_pts = as.integer(str_extract(game, \"(?<=[:digit:]{1,3}[:blank:]{1}[\\\\@]{0,1}[:alpha:]{1,100}[:blank:]{0,1}[:punct:]{0,1}[:blank:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,100}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,100}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{1})[:digit:]{1,3}\"))) %>% \n    select(-name, -value) %>% \n    filter(!is.na(date)) %>% \n    mutate(home_team = if_else(str_detect(win_team, \"\\\\@\"), win_team, \n                               if_else(str_detect(lose_team, \"\\\\@\"), lose_team, \"Neutral Court\"))) %>% \n    mutate_at(vars(contains('team')), list(~str_remove(., \"\\\\@\"))) %>% \n    cbind(x)\n}\n\n#wbb_url <- \"https://www.masseyratings.com/scores.php?s=305973&sub=305973&all=1\" #Original URL (All WBB games)\nwbb_url <- \"https://www.masseyratings.com/scores.php?s=305973&sub=11590&all=1\" #Corrected URL (only D1 WBB games)\nmbb_url <- \"https://www.masseyratings.com/scores.php?s=cb2019&sub=ncaa-d1&all=1&sch=1\" #Original URL (only D1 MBB games)\nurls <- c(wbb_url, mbb_url)\n\nbb_games <- map_dfr(urls, bb_games_process) %>% \n  as_tibble() %>% \n  mutate(sport = if_else(x == wbb_url, \"WBB\", \"MBB\")) %>% \n  select(-x)\n\nbb_games %>% tail(n=5)\n\n# A tibble: 5 x 8\n  game                  date       win_t~1 win_pts lose_~2 lose_~3 home_~4 sport\n  <chr>                 <date>     <chr>     <int> <chr>     <int> <chr>   <chr>\n1 Texas 81 Lipscomb 66~ 2019-04-04 Texas        81 Lipsco~      66 Neutra~ MBB  \n2 South Florida 77 @De~ 2019-04-05 South ~      77 DePaul       65 DePaul  MBB  \n3 Virginia 63 Auburn 6~ 2019-04-06 Virgin~      63 Auburn       62 Neutra~ MBB  \n4 Texas Tech 61 Michig~ 2019-04-06 Texas ~      61 Michig~      51 Neutra~ MBB  \n5 Virginia 85 Texas Te~ 2019-04-08 Virgin~      85 Texas ~      77 Neutra~ MBB  \n# ... with abbreviated variable names 1: win_team, 2: lose_team, 3: lose_pts,\n#   4: home_team\n\n\nUsing this prepped data, we can identify that there were 543 teams that played in women’s games and 650 that played in men’s games during the last season."
  },
  {
    "objectID": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html#calculating-the-number-of-transitive-champions",
    "href": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html#calculating-the-number-of-transitive-champions",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "Calculating the number of transitive champions",
    "text": "Calculating the number of transitive champions\nTo identify each “transitive champion” in each sport, I looked for where the nation champion lost and pulled a vector of the opponent(s) who beat the champion during the season. I then looped (ugh, I know, I know) through multiple rounds to see who defeated those teams, and so on and so forth. With each loop, I also pasted the number of unique transitive champions who were identified in each round into a data frame for further analysis.\n\nSetup\n\nrounds <- 25\nwbb_n_champ <- \"Baylor\"\nmbb_n_champ <- \"Virginia\"\n\n\n\nWBB calculations\n\nwbb_t_champs <- bb_games %>% \n  filter(sport == \"WBB\" & lose_team %in% wbb_n_champ) %>% \n  pull(win_team) %>% \n  unique()\n\nwbb_degree_sep <- wbb_t_champs %>% length\n\nfor(x in 1:rounds){\n  wbb_t_champ_beaters <- bb_games %>% \n    filter(sport == \"WBB\" & lose_team %in% wbb_t_champs) %>% \n    pull(win_team) %>% \n    unique()\n  wbb_t_champs <- c(wbb_t_champs, wbb_t_champ_beaters)\n  wbb_t_champs <- wbb_t_champs[!wbb_t_champs == wbb_n_champ] #Remove the national champion from the transitive champs vector\n  wbb_degree_sep <- rbind(wbb_degree_sep, wbb_t_champs %>% unique() %>% length)\n}\n\nwbb_transitive_champs <- wbb_degree_sep %>% \n  as.data.frame() %>% \n  cbind(total_wbb_teams) %>%\n  rename(transitive_champions = V1,\n         total_teams = total_wbb_teams) %>% \n  mutate(degree_of_separation = row_number(),\n         transitive_champ_pct = transitive_champions / total_teams,\n         sport = \"WBB\") \n\n\n\nMBB calculations\n\nmbb_t_champs <- bb_games %>% \n  filter(sport == \"MBB\" & lose_team %in% mbb_n_champ) %>% \n  pull(win_team) %>% \n  unique()\n\nmbb_degree_sep <- mbb_t_champs %>% length\n\nfor(x in 1:rounds){\n  mbb_t_champ_beaters <- bb_games %>% \n    filter(sport == \"MBB\" & lose_team %in% mbb_t_champs) %>% \n    pull(win_team) %>% \n    unique()\n  mbb_t_champs <- c(mbb_t_champs, mbb_t_champ_beaters)\n  mbb_t_champs <- mbb_t_champs[!mbb_t_champs == mbb_n_champ] #Remove the national champion from the transitive champs vector\n  mbb_degree_sep <- rbind(mbb_degree_sep, mbb_t_champs %>% unique() %>% length)\n}\n\nmbb_transitive_champs <- mbb_degree_sep %>% \n  as.data.frame() %>% \n  cbind(total_mbb_teams) %>%\n  rename(transitive_champions = V1,\n         total_teams = total_mbb_teams) %>% \n  mutate(degree_of_separation = row_number(),\n         transitive_champ_pct = transitive_champions / total_teams,\n         sport = \"MBB\") \n\n\n\nBringing the transitive champion data together\n\ntransitive_champs <- bind_rows(\n  wbb_transitive_champs,\n  mbb_transitive_champs) %>% \n  mutate(sport = as_factor(sport))"
  },
  {
    "objectID": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html#results",
    "href": "posts/2019-04-14-fivethirtyeight-riddler-challenge-transitive-champions/index.html#results",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "Results!",
    "text": "Results!\nFor the 2018-2019, I identified the following number of “transitive champs”:\n\nWomen’s Basketball: 360 transitive champions\nMen’s Basketball: 358 transitive champions\n\nEach sport reached the total number of transitive champs within 8 degrees of separation of the national champion. However, transitive champs comprise 66% of total women’s Division I basketball teams compared to 55% in the men’s game, as the plot below shows. This could be due to an effect of major conference teams playing each other more in men’s basketball (limiting opportunities for minor conference teams to grab a transitive championship), but that hypothesis would have to be tested in further analysis.\n\ntransitive_champs %>% \n  ggplot(aes(x=degree_of_separation, y=transitive_champ_pct, color = sport)) +\n  labs(title = \"How many basketball teams beat a team, who beat a team, \\nwho.... beat the national champ?\",\n       subtitle = \"Analysis of 2018-2019 college basketball games\") + \n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  facet_grid(rows = vars(sport), scales = \"free\") +\n  scale_x_continuous(\"Degrees of Separation from Actual National Champion\") +\n  scale_y_continuous(\"Cumulative % of Teams\", labels = scales::percent) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html",
    "title": "College Football Bar Chart Races",
    "section": "",
    "text": "cfb-rolling-wins"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#installation-and-components-of-the-claimsdb-package",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#installation-and-components-of-the-claimsdb-package",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Installation and components of the claimsdb package",
    "text": "Installation and components of the claimsdb package\nYou can install the development version of claimsdb from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"jfangmeier/claimsdb\")\n\nYou can then load claimsdb alongside your other packages. We will be using the tidyverse packages in our later examples.\n\nlibrary(tidyverse)\nlibrary(claimsdb)\n\nThe tables are available after loading the claimsdb package. This includes a schema that describes each of the tables and the included variables from the CMS DE-SynPUF. You can see that claimsdb includes five tables. One of the tables, bene contains beneficiary records, while the others include specific types of claims data.\n\nschema\n\n# A tibble: 5 x 5\n  TABLE      TABLE_TITLE                                TABLE~1 UNIT_~2 PROPER~3\n  <chr>      <chr>                                      <chr>   <chr>   <list>  \n1 bene       CMS Beneficiary Summary DE-SynPUF          Synthe~ Benefi~ <tibble>\n2 carrier    CMS Carrier Claims DE-SynPUF               Synthe~ claim   <tibble>\n3 inpatient  CMS Inpatient Claims DE-SynPUF             Synthe~ claim   <tibble>\n4 outpatient CMS Outpatient Claims DE-SynPUF            Synthe~ claim   <tibble>\n5 pde        CMS Prescription Drug Events (PDE) DE-Syn~ Synthe~ claim   <tibble>\n# ... with abbreviated variable names 1: TABLE_DESCRIPTION, 2: UNIT_OF_RECORD,\n#   3: PROPERTIES\n\n\nYou can access details on the variables in each of the tables like in this example with the inpatient table. You can see that this table contains 35 fields, including a beneficiary code to identify members across tables as well as detailed information on each inpatient claim.\n\nschema %>% \n  filter(TABLE == \"inpatient\") %>% \n  pull(PROPERTIES)\n\n[[1]]\n# A tibble: 35 x 3\n   VARIABLE                 TYPE    DESCRIPTION                                 \n   <chr>                    <chr>   <chr>                                       \n 1 DESYNPUF_ID              string  Beneficiary Code                            \n 2 CLM_ID                   string  Claim ID                                    \n 3 SEGMENT                  numeric Claim Line Segment                          \n 4 CLM_FROM_DT              date    Claims start date                           \n 5 CLM_THRU_DT              date    Claims end date                             \n 6 PRVDR_NUM                string  Provider Institution                        \n 7 CLM_PMT_AMT              numeric Claim Payment Amount                        \n 8 NCH_PRMRY_PYR_CLM_PD_AMT numeric NCH Primary Payer Claim Paid Amount         \n 9 AT_PHYSN_NPI             string  Attending Physician National Provider Ident~\n10 OP_PHYSN_NPI             string  Operating Physician National Provider Ident~\n# ... with 25 more rows"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#access-claims-data-as-a-remote-database",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#access-claims-data-as-a-remote-database",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Access claims data as a remote database",
    "text": "Access claims data as a remote database\nMany organizations store claims data in a remote database, so claimsdb also includes all of the tables as an in-memory DuckDB database. This can be a great way to practice working with this type of data, including building queries with dplyr code using dbplyr.\nTo setup the in-memory database, you need to create a database connection using claims_connect() and create connections to each of the tables you want to use.\n\nlibrary(dbplyr)\n\n# Setup connection to duckDB database\ncon <- claims_connect()\n\n# Setup connections to each of the enrollment and claims tables in the database\nbene_rmt <- tbl(con, \"bene\")\ninpatient_rmt <- tbl(con, \"inpatient\")\noutpatient_rmt <- tbl(con, \"outpatient\")\ncarrier_rmt <- tbl(con, \"carrier\")\npde_rmt <- tbl(con, \"pde\")\n\nYou can then preview your connection to each of the remote tables.\n\n# Preview the prescription drug event table in the database\npde_rmt\n\n# Source:   table<pde> [?? x 8]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      PDE_ID    SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5\n   <chr>            <chr>     <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 00E040C6ECE8F878 83014463~ 2008-12-20 492880~      30      30       0      10\n 2 00E040C6ECE8F878 83594465~ 2009-04-25 529590~      20      30       0       0\n 3 00E040C6ECE8F878 83144465~ 2009-09-22 000834~      80      30      40      80\n 4 00E040C6ECE8F878 83614464~ 2009-10-03 634810~      60      10       0      10\n 5 00E040C6ECE8F878 83014461~ 2009-11-16 511294~      60      30       0      20\n 6 00E040C6ECE8F878 83234464~ 2009-12-11 580160~     270      30       0      10\n 7 014F2C07689C173B 83294462~ 2009-09-14 009045~      90      30       0      60\n 8 014F2C07689C173B 83874466~ 2009-10-11 596040~      40      20       0     570\n 9 014F2C07689C173B 83314462~ 2009-11-24 510790~      30      30       0     150\n10 014F2C07689C173B 83874467~ 2009-12-29 511293~      30      30       0      10\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#examples-of-using-claimsdb-for-analysis",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#examples-of-using-claimsdb-for-analysis",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Examples of using claimsdb for analysis",
    "text": "Examples of using claimsdb for analysis\nTo analyze and explore the claims and beneficiary data, you can execute your own SQL code on the database using the DBI package.\n\nDBI::dbGetQuery(\n  con, \n  paste0(\n    'SELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"',\n    'FROM \"bene\"',\n    'WHERE \"BENE_SEX_IDENT_CD\" = 1.0',\n    'LIMIT 10'\n  )\n)\n\n        DESYNPUF_ID BENE_BIRTH_DT\n1  001115EAB83B19BB    1939-12-01\n2  03ADA78C0FEF79F4    1923-02-01\n3  040A12AB5EAA444C    1943-10-01\n4  0507DE00BC6E6CD6    1932-07-01\n5  05672CCCED56BCAD    1937-07-01\n6  060CDE3A044F64BA    1943-02-01\n7  061B5B3D9A459675    1932-04-01\n8  08C8E0A0C6EAC884    1954-06-01\n9  09EEB5C4C4FAEF10    1935-04-01\n10 0A58C6D6B9BE67CF    1942-07-01\n\n\nHowever, in the following examples, we will use the dbplyr package which translates dplyr code into SQL that can be executed against the database. You can see that the results below with dplyr functions match the results above that used a SQL query.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT\n   <chr>            <date>       \n 1 001115EAB83B19BB 1939-12-01   \n 2 03ADA78C0FEF79F4 1923-02-01   \n 3 040A12AB5EAA444C 1943-10-01   \n 4 0507DE00BC6E6CD6 1932-07-01   \n 5 05672CCCED56BCAD 1937-07-01   \n 6 060CDE3A044F64BA 1943-02-01   \n 7 061B5B3D9A459675 1932-04-01   \n 8 08C8E0A0C6EAC884 1954-06-01   \n 9 09EEB5C4C4FAEF10 1935-04-01   \n10 0A58C6D6B9BE67CF 1942-07-01   \n# ... with more rows\n\n\nWe can use the show_query() function to see the SQL code that dbplyr created and that it closely matches the SQL code above.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT) %>% \n  show_query()\n\n<SQL>\nSELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"\nFROM \"bene\"\nWHERE (\"BENE_SEX_IDENT_CD\" = 1.0)\n\n\n\nFirst, a note about working with dates/times in databases\ndbplyr is an amazing tool for working with databases, especially if you want to use many functions from the dplyr and tidyr packages. However, it does not currently have SQL translations for all functions in the tidyverse family of packages. For example, the lubridate package’s date and time functions work on local dataframes but cannot be translated to work on remote tables at this time. In the example below, you can see that the lubridate function year() (for parsing the year from a date) works on the local dataframe but generates an error on the remote table with the same data.\n\nbene %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# A tibble: 998 x 3\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with 988 more rows\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\nFortunately, dbplyr allows you pass along SQL functions in your dplyr code, and it will include these functions in the generated query. For date/time functions, we need to consult the documentation from DuckDB on date functions. To parse a part of a date (e.g., the year), we need to use the date_part() function for DuckDB.\nThe function to do this task may vary across database backends, so if you are doing this with a different database (Oracle, SQL Server, etc.), you will need to read the documentation for that database management system.\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = date_part('year', BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\n\n\nExample 1: which members had the highest prescription drug costs for 2008?\nFor this first example, we are going to identify the beneficiaries with the highest total prescription drug costs in 2008. We need to use the pde table that has claims on prescription drug events and the bene table that has beneficiary records. We create an object that is the aggregated costs for prescription drugs at the beneficiary level in 2008. Note that we had to use date_part() to parse the year from the service date.\n\n# Calculate rx costs for utilizing members in 2008\nrx_costs_rmt <- pde_rmt %>% \n  mutate(BENE_YEAR = date_part('year', SRVC_DT)) %>% \n  filter(BENE_YEAR == 2008) %>% \n  group_by(BENE_YEAR, DESYNPUF_ID) %>% \n  summarize(TOTAL_RX_COST = sum(TOT_RX_CST_AMT, na.rm = T), .groups = \"drop\") %>% \n  ungroup()\n\nrx_costs_rmt\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 00E040C6ECE8F878            10\n 2      2008 03ADA78C0FEF79F4          5020\n 3      2008 040A12AB5EAA444C           120\n 4      2008 043AAAE41C9A37B7           810\n 5      2008 05672CCCED56BCAD             0\n 6      2008 061B5B3D9A459675           270\n 7      2008 08BB74BA9DFD5C06            20\n 8      2008 08C8E0A0C6EAC884          5860\n 9      2008 09EEB5C4C4FAEF10          1590\n10      2008 0A58C6D6B9BE67CF           930\n# ... with more rows\n\n\nThen we join the aggregated cost data to the beneficiary table. This is necessary because the pde table does not include beneficiaries who didn’t use any prescription drugs. After joining the table we reassign missing cost data to zero for those beneficiaries with no utilization. We can then use collect() to retrieve the results as a local dataframe.\n\n# Join the rx costs data to the beneficiary file and include members with no costs\nrx_bene_rmt <- bene_rmt %>% \n  filter(BENE_YEAR == 2008) %>% \n  select(\n    BENE_YEAR,\n    DESYNPUF_ID\n  ) %>% \n  left_join(\n    rx_costs_rmt, by = c(\"BENE_YEAR\", \"DESYNPUF_ID\")\n  ) %>% \n  mutate(TOTAL_RX_COST = ifelse(is.na(TOTAL_RX_COST), 0, TOTAL_RX_COST)) %>% \n  arrange(desc(TOTAL_RX_COST))\n\nrx_bene_rmt\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n# Ordered by: desc(TOTAL_RX_COST)\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with more rows\n\n# You can use collect() to bring the results into a local dataframe\nrx_bene_df <- rx_bene_rmt %>% \n  collect()\n\nrx_bene_df\n\n# A tibble: 500 x 3\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with 490 more rows\n\n\n\n\nExample 2: what percent of beneficiaries received an office visit within 30 days of discharge from a hospital?\nIn the next example, we are identifying which beneficiaries had an office visit within 30 days of being discharged. We will start with the inpatient table that contains records for all inpatient stays, including when a beneficiary was discharged. We create an object that includes the beneficiary ID, the discharge date, and the date 30 days after discharge. Note that for DuckDB we need to coerce “30” to an integer to calculate the new date.\n\n# Identify all member discharge dates and the dates 30 days after discharge from the inpatient table\nip_discharges <- inpatient_rmt %>% \n  transmute(\n    DESYNPUF_ID, \n    DSCHRG_DT = NCH_BENE_DSCHRG_DT,\n    DSCHRG_DT_30 = NCH_BENE_DSCHRG_DT + as.integer(30)) %>% \n  distinct()\n\nip_discharges\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      DSCHRG_DT  DSCHRG_DT_30\n   <chr>            <date>     <date>      \n 1 014F2C07689C173B 2009-09-20 2009-10-20  \n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24  \n 3 060CDE3A044F64BA 2008-10-18 2008-11-17  \n 4 08BB74BA9DFD5C06 2009-03-07 2009-04-06  \n 5 08BB74BA9DFD5C06 2009-03-08 2009-04-07  \n 6 08C8E0A0C6EAC884 2008-02-22 2008-03-23  \n 7 08C8E0A0C6EAC884 2009-05-28 2009-06-27  \n 8 08C8E0A0C6EAC884 2009-06-14 2009-07-14  \n 9 08C8E0A0C6EAC884 2009-07-07 2009-08-06  \n10 08C8E0A0C6EAC884 2009-08-23 2009-09-22  \n# ... with more rows\n\n\nNext, we need to identify office visits from the carrier table. I created a vector of five office visit codes for this example. Since these codes must match the values in the “HCPCS” columns, we reshape the table with pivot_longer() then filter for the office visit codes.\n\n# Create vector of office visit codes\noff_vis_cds <- as.character(99211:99215)\n\n# Identify members who had an office visit from the carrier table\noff_visit <- carrier_rmt %>% \n  select(DESYNPUF_ID, CLM_ID, CLM_FROM_DT, matches(\"HCPCS\")) %>% \n  pivot_longer(cols = matches(\"HCPCS\"), names_to = \"LINE\", values_to = \"HCPCS\") %>% \n  filter(HCPCS %in% off_vis_cds) %>% \n  distinct(DESYNPUF_ID, CLM_FROM_DT) %>% \n  mutate(OFFICE_VISIT = 1)\n\noff_visit\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      CLM_FROM_DT OFFICE_VISIT\n   <chr>            <date>             <dbl>\n 1 00E040C6ECE8F878 2009-01-17             1\n 2 00E040C6ECE8F878 2009-07-20             1\n 3 00E040C6ECE8F878 2009-07-26             1\n 4 029A22E4A3AAEE15 2008-01-25             1\n 5 029A22E4A3AAEE15 2008-01-30             1\n 6 029A22E4A3AAEE15 2008-08-28             1\n 7 029A22E4A3AAEE15 2008-11-19             1\n 8 029A22E4A3AAEE15 2009-04-01             1\n 9 029A22E4A3AAEE15 2009-04-12             1\n10 029A22E4A3AAEE15 2009-05-13             1\n# ... with more rows\n\n\nFinally, we join the office visits object to the discharges object. We can use the sql_on option in left_join() to inject some custom SQL to join when the office visit date is within 30 days of the discharge date.\n\n# Join discharges data and office visit data\ndischarge_offvis <- \n  ip_discharges %>% \n  left_join(\n    off_visit,\n    sql_on = paste0(\n      \"(LHS.DESYNPUF_ID = RHS.DESYNPUF_ID) AND\",\n      \"(RHS.CLM_FROM_DT >= LHS.DSCHRG_DT) AND\",\n      \"(RHS.CLM_FROM_DT <= LHS.DSCHRG_DT_30)\"\n      )\n  )\n\ndischarge_offvis\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID.x    DSCHRG_DT  DSCHRG_DT_30 DESYNPUF_ID.y    CLM_FROM_DT OFFIC~1\n   <chr>            <date>     <date>       <chr>            <date>        <dbl>\n 1 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-25        1\n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-30        1\n 3 060CDE3A044F64BA 2008-10-18 2008-11-17   060CDE3A044F64BA 2008-11-01        1\n 4 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-07        1\n 5 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-27        1\n 6 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-05-04        1\n 7 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-19        1\n 8 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-24        1\n 9 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-06-22        1\n10 202F481BD60B4F1C 2009-04-28 2009-05-28   202F481BD60B4F1C 2009-05-03        1\n# ... with more rows, and abbreviated variable name 1: OFFICE_VISIT\n\n\nAfter the join is complete, we can calculate the share of discharges with a timely office visit.\n\n# Find the percent of members with a office visit within 30 days of discharge\ndischarge_offvis %>% \n  distinct(\n    DESYNPUF_ID.x,\n    DSCHRG_DT,\n    OFFICE_VISIT\n  ) %>% \n  mutate(OFFICE_VISIT = ifelse(is.na(OFFICE_VISIT), 0, OFFICE_VISIT)) %>% \n  summarize(\n    OFV_RATE = mean(OFFICE_VISIT, na.rm = T)\n  )\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n  OFV_RATE\n     <dbl>\n1    0.540\n\n\n\n\nExample 3: how well are beneficiaries filling their hypertension drug prescriptions?\nFor this final example, we need to identify hypertension medications from the pde table and calculate medication adherence rates for each beneficiary. To isolate the hypertension medications, we can borrow from the HEDIS medication list for ACE inhibitors and ARB medications (which are commonly used to treat hypertension). This medication list is for 2018/2019, so it likely includes new drugs that did not exist in 2008/2009 and may not include older drugs that are no longer used (ideally it’s best to use external lists that match the time frame of the claims data).\n\nhedis_wb <- tempfile()\n\ndownload.file(\n  url = \"https://www.ncqa.org/hedis-2019-ndc-mld-directory-complete-workbook-final-11-1-2018-3/\",\n  destfile = hedis_wb,\n  mode = \"wb\"\n)\n\nhedis_df <- readxl::read_excel(\n  path = hedis_wb,\n  sheet = \"Medications List to NDC Codes\"\n)\n\nhyp_ndc_df <- \n  hedis_df %>% \n  filter(`Medication List` == \"ACE Inhibitor/ARB Medications\") %>% \n  select(\n    LIST = `Medication List`,\n    PRODUCTID = `NDC Code`\n  )\n\nhyp_ndc_df\n\n# A tibble: 3,230 x 2\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with 3,220 more rows\n\n\nOne of the cool features of dbplyr is that we can copy this local dataframe to the database as a temporary table using the copy_to() function.\n\nhyp_ndc_rmt <- copy_to(con, hyp_ndc_df, overwrite = T)\n\nhyp_ndc_rmt\n\n# Source:   table<hyp_ndc_df> [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with more rows\n\n\nWith the hypertension codes in the database, we can use inner_join() to find the matching drugs from the pde table.\n\npde_hyp_rmt <- \n  pde_rmt %>% \n  inner_join(\n    hyp_ndc_rmt, by = c(\"PROD_SRVC_ID\" = \"PRODUCTID\")\n  )\n\npde_hyp_rmt\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID   PDE_ID SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5 LIST \n   <chr>         <chr>  <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>\n 1 03ADA78C0FEF~ 83554~ 2008-02-01 510790~      30      30       0      10 ACE ~\n 2 040A12AB5EAA~ 83974~ 2009-11-27 638740~      90      30       0      10 ACE ~\n 3 043AAAE41C9A~ 83024~ 2008-08-11 002472~     200      90      10      80 ACE ~\n 4 0D540BEBC45A~ 83454~ 2009-02-18 604290~      60      90      10      10 ACE ~\n 5 0D540BEBC45A~ 83874~ 2009-06-15 661050~      30      30       0      10 ACE ~\n 6 0E0A0107787B~ 83714~ 2008-06-30 661050~     100      90      70      60 ACE ~\n 7 109D67D11477~ 83064~ 2008-11-21 134110~      30      60       0      70 ACE ~\n 8 109D67D11477~ 83034~ 2009-07-03 666850~      30      30       0      10 ACE ~\n 9 10D75CDD5B4A~ 83804~ 2008-07-01 002472~      90      90       0      10 ACE ~\n10 1183CA4884F8~ 83934~ 2009-01-25 001850~      90      20       0     110 ACE ~\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT\n\n\nWe can now use collect() to retrieve the data as a local dataframe. As a dataframe, we can now use the AdhereR package to calculate the medication possession ratio (MPR) for members who filled a hypertension medication. MPR is a commonly used metric of medication adherence, measuring if beneficiaries have gaps between their prescription fills.\nWe can see the MPR for each member who filled one of the medications, and we can calculate the median and mean MPRs across these beneficiaries.\n\n# install.packages(\"AdhereR\")\nlibrary(AdhereR)\n\npde_hyp_df <- pde_hyp_rmt %>% \n  collect()\n\nhyp_adhere <- CMA4(\n  data = pde_hyp_df,\n  ID.colname = 'DESYNPUF_ID',\n  event.date.colname = 'SRVC_DT',\n  event.duration.colname = 'DAYS_SUPLY_NUM',\n  date.format = \"%Y-%m-%d\"\n)\n\nhyp_adhere_cma <- hyp_adhere$CMA %>% tibble()\n\nhyp_adhere_cma\n\n# A tibble: 134 x 2\n   DESYNPUF_ID         CMA\n   <chr>             <dbl>\n 1 03ADA78C0FEF79F4 0.0411\n 2 040A12AB5EAA444C 0.0411\n 3 043AAAE41C9A37B7 0.123 \n 4 0D540BEBC45ADCA7 0.164 \n 5 0E0A0107787B32AA 0.123 \n 6 109D67D114778917 0.123 \n 7 10D75CDD5B4AD3B0 0.123 \n 8 1183CA4884F8A0A8 0.0274\n 9 11C45CF0DDD03ACE 0.0822\n10 1226AD9944384B64 0.0411\n# ... with 124 more rows\n\nhyp_adhere_cma %>% \n  summarize(\n    median_mpr = median(CMA),\n    mean_mpr = mean(CMA)\n  )\n\n# A tibble: 1 x 2\n  median_mpr mean_mpr\n       <dbl>    <dbl>\n1     0.0411   0.0848"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#data-limitations",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#data-limitations",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Data Limitations",
    "text": "Data Limitations\nWhile data included in claimsdb is useful for many types of analyses, it does include a few notable limitations. - As mentioned earlier, the data is a small sample (500 beneficiaries) and is not intended to be representative of the Medicare population. In addition, the data is synthetic and should not be used for drawing inferences on the Medicare population. - Since the data is more than 10 years old, it doesn’t capture newer medications or procedures. It also includes procedure codes that have been retired or replaced. This is a challenge when applying external code lists that are much newer. - The diagnosis fields in the data use the International Classification of Diseases, Ninth Revision (ICD-9), but the United States converted to ICD-10 in 2015. If you are interesting in a mapping between ICD-9 and ICD-10, CMS has resources to consider. - The Medicare population is mostly Americans aged 65 and over, so the data will not have claims on certain specialties such as pediatrics or maternity care."
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "",
    "text": "In the fields of health informatics and health services research, health insurance claims data are a valuable resource to help answer questions about health care access and financing. However, claims data in the real world often contains both sensitive (protected health information) and proprietary (trade secrets) elements. For most students and educators seeking opportunities to learn how to use claims data, there are few available sources for practice.\nTo help with this problem, claimsdb 📦 provides easy access to a sample of health insurance enrollment and claims data from the Centers for Medicare and Medicaid Services (CMS) Data Entrepreneurs’ Synthetic Public Use File (DE-SynPUF), as a set of relational tables or as an in-memory database using DuckDB. All the data is contained within a single package, so users do not need to download any external data. This package is inspired by and based on the starwarsdb package.\nThe data are structured as actual Medicare claims data but are fully “synthetic,” after a process of alterations meant to reduce the risk of re-identification of real Medicare beneficiaries. The synthetic process that CMS adopted changes the co-variation across variables, so analysts should be cautious about drawing inferences about the actual Medicare population.\nThe data included in claimsdb comes from 500 randomly selected 2008 Medicare beneficiaries from Sample 2 of the DE-SynPUF, and it includes all the associated claims for these members for 2008-2009. CMS provides resources, including a codebook, FAQs, and other documents with more information about this data.\nTo introduce claimsdb, this post covers the following topics: - Installation of the claimsdb package - How to setup a remote database with the included data - Examples of how to use claimsdb for analysis - An overview of the limitations of the data included in package"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#installation-and-components-of-the-claimsdb-package",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#installation-and-components-of-the-claimsdb-package",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Installation and components of the claimsdb package",
    "text": "Installation and components of the claimsdb package\nYou can install the development version of claimsdb from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"jfangmeier/claimsdb\")\n\nYou can then load claimsdb alongside your other packages. We will be using the tidyverse packages in our later examples.\n\nlibrary(tidyverse)\nlibrary(claimsdb)\n\nThe tables are available after loading the claimsdb package. This includes a schema that describes each of the tables and the included variables from the CMS DE-SynPUF. You can see that claimsdb includes five tables. One of the tables, bene contains beneficiary records, while the others include specific types of claims data.\n\nschema\n\n# A tibble: 5 x 5\n  TABLE      TABLE_TITLE                                TABLE~1 UNIT_~2 PROPER~3\n  <chr>      <chr>                                      <chr>   <chr>   <list>  \n1 bene       CMS Beneficiary Summary DE-SynPUF          Synthe~ Benefi~ <tibble>\n2 carrier    CMS Carrier Claims DE-SynPUF               Synthe~ claim   <tibble>\n3 inpatient  CMS Inpatient Claims DE-SynPUF             Synthe~ claim   <tibble>\n4 outpatient CMS Outpatient Claims DE-SynPUF            Synthe~ claim   <tibble>\n5 pde        CMS Prescription Drug Events (PDE) DE-Syn~ Synthe~ claim   <tibble>\n# ... with abbreviated variable names 1: TABLE_DESCRIPTION, 2: UNIT_OF_RECORD,\n#   3: PROPERTIES\n\n\nYou can access details on the variables in each of the tables like in this example with the inpatient table. You can see that this table contains 35 fields, including a beneficiary code to identify members across tables as well as detailed information on each inpatient claim.\n\nschema %>% \n  filter(TABLE == \"inpatient\") %>% \n  pull(PROPERTIES)\n\n[[1]]\n# A tibble: 35 x 3\n   VARIABLE                 TYPE    DESCRIPTION                                 \n   <chr>                    <chr>   <chr>                                       \n 1 DESYNPUF_ID              string  Beneficiary Code                            \n 2 CLM_ID                   string  Claim ID                                    \n 3 SEGMENT                  numeric Claim Line Segment                          \n 4 CLM_FROM_DT              date    Claims start date                           \n 5 CLM_THRU_DT              date    Claims end date                             \n 6 PRVDR_NUM                string  Provider Institution                        \n 7 CLM_PMT_AMT              numeric Claim Payment Amount                        \n 8 NCH_PRMRY_PYR_CLM_PD_AMT numeric NCH Primary Payer Claim Paid Amount         \n 9 AT_PHYSN_NPI             string  Attending Physician National Provider Ident~\n10 OP_PHYSN_NPI             string  Operating Physician National Provider Ident~\n# ... with 25 more rows"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#access-claims-data-as-a-remote-database",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#access-claims-data-as-a-remote-database",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Access claims data as a remote database",
    "text": "Access claims data as a remote database\nMany organizations store claims data in a remote database, so claimsdb also includes all of the tables as an in-memory DuckDB database. This can be a great way to practice working with this type of data, including building queries with dplyr code using dbplyr.\nTo setup the in-memory database, you need to create a database connection using claims_connect() and create connections to each of the tables you want to use.\n\nlibrary(dbplyr)\n\n# Setup connection to duckDB database\ncon <- claims_connect()\n\n# Setup connections to each of the enrollment and claims tables in the database\nbene_rmt <- tbl(con, \"bene\")\ninpatient_rmt <- tbl(con, \"inpatient\")\noutpatient_rmt <- tbl(con, \"outpatient\")\ncarrier_rmt <- tbl(con, \"carrier\")\npde_rmt <- tbl(con, \"pde\")\n\nYou can then preview your connection to each of the remote tables.\n\n# Preview the prescription drug event table in the database\npde_rmt\n\n# Source:   table<pde> [?? x 8]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      PDE_ID    SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5\n   <chr>            <chr>     <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 00E040C6ECE8F878 83014463~ 2008-12-20 492880~      30      30       0      10\n 2 00E040C6ECE8F878 83594465~ 2009-04-25 529590~      20      30       0       0\n 3 00E040C6ECE8F878 83144465~ 2009-09-22 000834~      80      30      40      80\n 4 00E040C6ECE8F878 83614464~ 2009-10-03 634810~      60      10       0      10\n 5 00E040C6ECE8F878 83014461~ 2009-11-16 511294~      60      30       0      20\n 6 00E040C6ECE8F878 83234464~ 2009-12-11 580160~     270      30       0      10\n 7 014F2C07689C173B 83294462~ 2009-09-14 009045~      90      30       0      60\n 8 014F2C07689C173B 83874466~ 2009-10-11 596040~      40      20       0     570\n 9 014F2C07689C173B 83314462~ 2009-11-24 510790~      30      30       0     150\n10 014F2C07689C173B 83874467~ 2009-12-29 511293~      30      30       0      10\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#examples-of-using-claimsdb-for-analysis",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#examples-of-using-claimsdb-for-analysis",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Examples of using claimsdb for analysis",
    "text": "Examples of using claimsdb for analysis\nTo analyze and explore the claims and beneficiary data, you can execute your own SQL code on the database using the DBI package.\n\nDBI::dbGetQuery(\n  con, \n  paste0(\n    'SELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"',\n    'FROM \"bene\"',\n    'WHERE \"BENE_SEX_IDENT_CD\" = 1.0',\n    'LIMIT 10'\n  )\n)\n\n        DESYNPUF_ID BENE_BIRTH_DT\n1  001115EAB83B19BB    1939-12-01\n2  03ADA78C0FEF79F4    1923-02-01\n3  040A12AB5EAA444C    1943-10-01\n4  0507DE00BC6E6CD6    1932-07-01\n5  05672CCCED56BCAD    1937-07-01\n6  060CDE3A044F64BA    1943-02-01\n7  061B5B3D9A459675    1932-04-01\n8  08C8E0A0C6EAC884    1954-06-01\n9  09EEB5C4C4FAEF10    1935-04-01\n10 0A58C6D6B9BE67CF    1942-07-01\n\n\nHowever, in the following examples, we will use the dbplyr package which translates dplyr code into SQL that can be executed against the database. You can see that the results below with dplyr functions match the results above that used a SQL query.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT\n   <chr>            <date>       \n 1 001115EAB83B19BB 1939-12-01   \n 2 03ADA78C0FEF79F4 1923-02-01   \n 3 040A12AB5EAA444C 1943-10-01   \n 4 0507DE00BC6E6CD6 1932-07-01   \n 5 05672CCCED56BCAD 1937-07-01   \n 6 060CDE3A044F64BA 1943-02-01   \n 7 061B5B3D9A459675 1932-04-01   \n 8 08C8E0A0C6EAC884 1954-06-01   \n 9 09EEB5C4C4FAEF10 1935-04-01   \n10 0A58C6D6B9BE67CF 1942-07-01   \n# ... with more rows\n\n\nWe can use the show_query() function to see the SQL code that dbplyr created and that it closely matches the SQL code above.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT) %>% \n  show_query()\n\n<SQL>\nSELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"\nFROM \"bene\"\nWHERE (\"BENE_SEX_IDENT_CD\" = 1.0)\n\n\n\nFirst, a note about working with dates/times in databases\ndbplyr is an amazing tool for working with databases, especially if you want to use many functions from the dplyr and tidyr packages. However, it does not currently have SQL translations for all functions in the tidyverse family of packages. For example, the lubridate package’s date and time functions work on local dataframes but cannot be translated to work on remote tables at this time. In the example below, you can see that the lubridate function year() (for parsing the year from a date) works on the local dataframe but generates an error on the remote table with the same data.\n\nbene %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# A tibble: 998 x 3\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with 988 more rows\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\nFortunately, dbplyr allows you pass along SQL functions in your dplyr code, and it will include these functions in the generated query. For date/time functions, we need to consult the documentation from DuckDB on date functions. To parse a part of a date (e.g., the year), we need to use the date_part() function for DuckDB.\nThe function to do this task may vary across database backends, so if you are doing this with a different database (Oracle, SQL Server, etc.), you will need to read the documentation for that database management system.\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = date_part('year', BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\n\n\nExample 1: which members had the highest prescription drug costs for 2008?\nFor this first example, we are going to identify the beneficiaries with the highest total prescription drug costs in 2008. We need to use the pde table that has claims on prescription drug events and the bene table that has beneficiary records. We create an object that is the aggregated costs for prescription drugs at the beneficiary level in 2008. Note that we had to use date_part() to parse the year from the service date.\n\n# Calculate rx costs for utilizing members in 2008\nrx_costs_rmt <- pde_rmt %>% \n  mutate(BENE_YEAR = date_part('year', SRVC_DT)) %>% \n  filter(BENE_YEAR == 2008) %>% \n  group_by(BENE_YEAR, DESYNPUF_ID) %>% \n  summarize(TOTAL_RX_COST = sum(TOT_RX_CST_AMT, na.rm = T), .groups = \"drop\") %>% \n  ungroup()\n\nrx_costs_rmt\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 00E040C6ECE8F878            10\n 2      2008 03ADA78C0FEF79F4          5020\n 3      2008 040A12AB5EAA444C           120\n 4      2008 043AAAE41C9A37B7           810\n 5      2008 05672CCCED56BCAD             0\n 6      2008 061B5B3D9A459675           270\n 7      2008 08BB74BA9DFD5C06            20\n 8      2008 08C8E0A0C6EAC884          5860\n 9      2008 09EEB5C4C4FAEF10          1590\n10      2008 0A58C6D6B9BE67CF           930\n# ... with more rows\n\n\nThen we join the aggregated cost data to the beneficiary table. This is necessary because the pde table does not include beneficiaries who didn’t use any prescription drugs. After joining the table we reassign missing cost data to zero for those beneficiaries with no utilization. We can then use collect() to retrieve the results as a local dataframe.\n\n# Join the rx costs data to the beneficiary file and include members with no costs\nrx_bene_rmt <- bene_rmt %>% \n  filter(BENE_YEAR == 2008) %>% \n  select(\n    BENE_YEAR,\n    DESYNPUF_ID\n  ) %>% \n  left_join(\n    rx_costs_rmt, by = c(\"BENE_YEAR\", \"DESYNPUF_ID\")\n  ) %>% \n  mutate(TOTAL_RX_COST = ifelse(is.na(TOTAL_RX_COST), 0, TOTAL_RX_COST)) %>% \n  arrange(desc(TOTAL_RX_COST))\n\nrx_bene_rmt\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n# Ordered by: desc(TOTAL_RX_COST)\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with more rows\n\n# You can use collect() to bring the results into a local dataframe\nrx_bene_df <- rx_bene_rmt %>% \n  collect()\n\nrx_bene_df\n\n# A tibble: 500 x 3\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with 490 more rows\n\n\n\n\nExample 2: what percent of beneficiaries received an office visit within 30 days of discharge from a hospital?\nIn the next example, we are identifying which beneficiaries had an office visit within 30 days of being discharged. We will start with the inpatient table that contains records for all inpatient stays, including when a beneficiary was discharged. We create an object that includes the beneficiary ID, the discharge date, and the date 30 days after discharge. Note that for DuckDB we need to coerce “30” to an integer to calculate the new date.\n\n# Identify all member discharge dates and the dates 30 days after discharge from the inpatient table\nip_discharges <- inpatient_rmt %>% \n  transmute(\n    DESYNPUF_ID, \n    DSCHRG_DT = NCH_BENE_DSCHRG_DT,\n    DSCHRG_DT_30 = NCH_BENE_DSCHRG_DT + as.integer(30)) %>% \n  distinct()\n\nip_discharges\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      DSCHRG_DT  DSCHRG_DT_30\n   <chr>            <date>     <date>      \n 1 014F2C07689C173B 2009-09-20 2009-10-20  \n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24  \n 3 060CDE3A044F64BA 2008-10-18 2008-11-17  \n 4 08BB74BA9DFD5C06 2009-03-07 2009-04-06  \n 5 08BB74BA9DFD5C06 2009-03-08 2009-04-07  \n 6 08C8E0A0C6EAC884 2008-02-22 2008-03-23  \n 7 08C8E0A0C6EAC884 2009-05-28 2009-06-27  \n 8 08C8E0A0C6EAC884 2009-06-14 2009-07-14  \n 9 08C8E0A0C6EAC884 2009-07-07 2009-08-06  \n10 08C8E0A0C6EAC884 2009-08-23 2009-09-22  \n# ... with more rows\n\n\nNext, we need to identify office visits from the carrier table. I created a vector of five office visit codes for this example. Since these codes must match the values in the “HCPCS” columns, we reshape the table with pivot_longer() then filter for the office visit codes.\n\n# Create vector of office visit codes\noff_vis_cds <- as.character(99211:99215)\n\n# Identify members who had an office visit from the carrier table\noff_visit <- carrier_rmt %>% \n  select(DESYNPUF_ID, CLM_ID, CLM_FROM_DT, matches(\"HCPCS\")) %>% \n  pivot_longer(cols = matches(\"HCPCS\"), names_to = \"LINE\", values_to = \"HCPCS\") %>% \n  filter(HCPCS %in% off_vis_cds) %>% \n  distinct(DESYNPUF_ID, CLM_FROM_DT) %>% \n  mutate(OFFICE_VISIT = 1)\n\noff_visit\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      CLM_FROM_DT OFFICE_VISIT\n   <chr>            <date>             <dbl>\n 1 00E040C6ECE8F878 2009-01-17             1\n 2 00E040C6ECE8F878 2009-07-20             1\n 3 00E040C6ECE8F878 2009-07-26             1\n 4 029A22E4A3AAEE15 2008-01-25             1\n 5 029A22E4A3AAEE15 2008-01-30             1\n 6 029A22E4A3AAEE15 2008-08-28             1\n 7 029A22E4A3AAEE15 2008-11-19             1\n 8 029A22E4A3AAEE15 2009-04-01             1\n 9 029A22E4A3AAEE15 2009-04-12             1\n10 029A22E4A3AAEE15 2009-05-13             1\n# ... with more rows\n\n\nFinally, we join the office visits object to the discharges object. We can use the sql_on option in left_join() to inject some custom SQL to join when the office visit date is within 30 days of the discharge date.\n\n# Join discharges data and office visit data\ndischarge_offvis <- \n  ip_discharges %>% \n  left_join(\n    off_visit,\n    sql_on = paste0(\n      \"(LHS.DESYNPUF_ID = RHS.DESYNPUF_ID) AND\",\n      \"(RHS.CLM_FROM_DT >= LHS.DSCHRG_DT) AND\",\n      \"(RHS.CLM_FROM_DT <= LHS.DSCHRG_DT_30)\"\n      )\n  )\n\ndischarge_offvis\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID.x    DSCHRG_DT  DSCHRG_DT_30 DESYNPUF_ID.y    CLM_FROM_DT OFFIC~1\n   <chr>            <date>     <date>       <chr>            <date>        <dbl>\n 1 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-25        1\n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-30        1\n 3 060CDE3A044F64BA 2008-10-18 2008-11-17   060CDE3A044F64BA 2008-11-01        1\n 4 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-07        1\n 5 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-27        1\n 6 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-05-04        1\n 7 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-19        1\n 8 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-24        1\n 9 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-06-22        1\n10 202F481BD60B4F1C 2009-04-28 2009-05-28   202F481BD60B4F1C 2009-05-03        1\n# ... with more rows, and abbreviated variable name 1: OFFICE_VISIT\n\n\nAfter the join is complete, we can calculate the share of discharges with a timely office visit.\n\n# Find the percent of members with a office visit within 30 days of discharge\ndischarge_offvis %>% \n  distinct(\n    DESYNPUF_ID.x,\n    DSCHRG_DT,\n    OFFICE_VISIT\n  ) %>% \n  mutate(OFFICE_VISIT = ifelse(is.na(OFFICE_VISIT), 0, OFFICE_VISIT)) %>% \n  summarize(\n    OFV_RATE = mean(OFFICE_VISIT, na.rm = T)\n  )\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n  OFV_RATE\n     <dbl>\n1    0.540\n\n\n\n\nExample 3: how well are beneficiaries filling their hypertension drug prescriptions?\nFor this final example, we need to identify hypertension medications from the pde table and calculate medication adherence rates for each beneficiary. To isolate the hypertension medications, we can borrow from the HEDIS medication list for ACE inhibitors and ARB medications (which are commonly used to treat hypertension). This medication list is for 2018/2019, so it likely includes new drugs that did not exist in 2008/2009 and may not include older drugs that are no longer used (ideally it’s best to use external lists that match the time frame of the claims data).\n\nhedis_wb <- tempfile()\n\ndownload.file(\n  url = \"https://www.ncqa.org/hedis-2019-ndc-mld-directory-complete-workbook-final-11-1-2018-3/\",\n  destfile = hedis_wb,\n  mode = \"wb\"\n)\n\nhedis_df <- readxl::read_excel(\n  path = hedis_wb,\n  sheet = \"Medications List to NDC Codes\"\n)\n\nhyp_ndc_df <- \n  hedis_df %>% \n  filter(`Medication List` == \"ACE Inhibitor/ARB Medications\") %>% \n  select(\n    LIST = `Medication List`,\n    PRODUCTID = `NDC Code`\n  )\n\nhyp_ndc_df\n\n# A tibble: 3,230 x 2\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with 3,220 more rows\n\n\nOne of the cool features of dbplyr is that we can copy this local dataframe to the database as a temporary table using the copy_to() function.\n\nhyp_ndc_rmt <- copy_to(con, hyp_ndc_df, overwrite = T)\n\nhyp_ndc_rmt\n\n# Source:   table<hyp_ndc_df> [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with more rows\n\n\nWith the hypertension codes in the database, we can use inner_join() to find the matching drugs from the pde table.\n\npde_hyp_rmt <- \n  pde_rmt %>% \n  inner_join(\n    hyp_ndc_rmt, by = c(\"PROD_SRVC_ID\" = \"PRODUCTID\")\n  )\n\npde_hyp_rmt\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID   PDE_ID SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5 LIST \n   <chr>         <chr>  <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>\n 1 03ADA78C0FEF~ 83554~ 2008-02-01 510790~      30      30       0      10 ACE ~\n 2 040A12AB5EAA~ 83974~ 2009-11-27 638740~      90      30       0      10 ACE ~\n 3 043AAAE41C9A~ 83024~ 2008-08-11 002472~     200      90      10      80 ACE ~\n 4 0D540BEBC45A~ 83454~ 2009-02-18 604290~      60      90      10      10 ACE ~\n 5 0D540BEBC45A~ 83874~ 2009-06-15 661050~      30      30       0      10 ACE ~\n 6 0E0A0107787B~ 83714~ 2008-06-30 661050~     100      90      70      60 ACE ~\n 7 109D67D11477~ 83064~ 2008-11-21 134110~      30      60       0      70 ACE ~\n 8 109D67D11477~ 83034~ 2009-07-03 666850~      30      30       0      10 ACE ~\n 9 10D75CDD5B4A~ 83804~ 2008-07-01 002472~      90      90       0      10 ACE ~\n10 1183CA4884F8~ 83934~ 2009-01-25 001850~      90      20       0     110 ACE ~\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT\n\n\nWe can now use collect() to retrieve the data as a local dataframe. As a dataframe, we can now use the AdhereR package to calculate the medication possession ratio (MPR) for members who filled a hypertension medication. MPR is a commonly used metric of medication adherence, measuring if beneficiaries have gaps between their prescription fills.\nWe can see the MPR for each member who filled one of the medications, and we can calculate the median and mean MPRs across these beneficiaries.\n\n# install.packages(\"AdhereR\")\nlibrary(AdhereR)\n\npde_hyp_df <- pde_hyp_rmt %>% \n  collect()\n\nhyp_adhere <- CMA4(\n  data = pde_hyp_df,\n  ID.colname = 'DESYNPUF_ID',\n  event.date.colname = 'SRVC_DT',\n  event.duration.colname = 'DAYS_SUPLY_NUM',\n  date.format = \"%Y-%m-%d\"\n)\n\nhyp_adhere_cma <- hyp_adhere$CMA %>% tibble()\n\nhyp_adhere_cma\n\n# A tibble: 134 x 2\n   DESYNPUF_ID         CMA\n   <chr>             <dbl>\n 1 03ADA78C0FEF79F4 0.0411\n 2 040A12AB5EAA444C 0.0411\n 3 043AAAE41C9A37B7 0.123 \n 4 0D540BEBC45ADCA7 0.164 \n 5 0E0A0107787B32AA 0.123 \n 6 109D67D114778917 0.123 \n 7 10D75CDD5B4AD3B0 0.123 \n 8 1183CA4884F8A0A8 0.0274\n 9 11C45CF0DDD03ACE 0.0822\n10 1226AD9944384B64 0.0411\n# ... with 124 more rows\n\nhyp_adhere_cma %>% \n  summarize(\n    median_mpr = median(CMA),\n    mean_mpr = mean(CMA)\n  )\n\n# A tibble: 1 x 2\n  median_mpr mean_mpr\n       <dbl>    <dbl>\n1     0.0411   0.0848"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#data-limitations",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#data-limitations",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Data Limitations",
    "text": "Data Limitations\nWhile data included in claimsdb is useful for many types of analyses, it does include a few notable limitations. - As mentioned earlier, the data is a small sample (500 beneficiaries) and is not intended to be representative of the Medicare population. In addition, the data is synthetic and should not be used for drawing inferences on the Medicare population. - Since the data is more than 10 years old, it doesn’t capture newer medications or procedures. It also includes procedure codes that have been retired or replaced. This is a challenge when applying external code lists that are much newer. - The diagnosis fields in the data use the International Classification of Diseases, Ninth Revision (ICD-9), but the United States converted to ICD-10 in 2015. If you are interesting in a mapping between ICD-9 and ICD-10, CMS has resources to consider. - The Medicare population is mostly Americans aged 65 and over, so the data will not have claims on certain specialties such as pediatrics or maternity care."
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html",
    "title": "FiveThirtyEight Riddler Challenge: Can You Find The Fish In State Names?",
    "section": "",
    "text": "This weekend, I attempted to solve my second FiveThirtyEight Riddler challenge:\n\nOhio is the only state whose name doesn’t share any letters with the word “mackerel.” It’s strange, but it’s true. But that isn’t the only pairing of a state and a word you can say that about — it’s not even the only fish! Kentucky has “goldfish” to itself, Montana has “jellyfish” and Delaware has “monkfish,” just to name a few. What is the longest “mackerel?” That is, what is the longest word that doesn’t share any letters with exactly one state? (If multiple “mackerels” are tied for being the longest, can you find them all?) Extra credit: Which state has the most “mackerels?” That is, which state has the most words for which it is the only state without any letters in common with those words?"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#installation-and-components-of-the-claimsdb-package",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#installation-and-components-of-the-claimsdb-package",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Installation and components of the claimsdb package",
    "text": "Installation and components of the claimsdb package\nYou can install the development version of claimsdb from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"jfangmeier/claimsdb\")\n\nYou can then load claimsdb alongside your other packages. We will be using the tidyverse packages in our later examples.\n\nlibrary(tidyverse)\nlibrary(claimsdb)\n\nThe tables are available after loading the claimsdb package. This includes a schema that describes each of the tables and the included variables from the CMS DE-SynPUF. You can see that claimsdb includes five tables. One of the tables, bene contains beneficiary records, while the others include specific types of claims data.\n\nschema\n\n# A tibble: 5 x 5\n  TABLE      TABLE_TITLE                                TABLE~1 UNIT_~2 PROPER~3\n  <chr>      <chr>                                      <chr>   <chr>   <list>  \n1 bene       CMS Beneficiary Summary DE-SynPUF          Synthe~ Benefi~ <tibble>\n2 carrier    CMS Carrier Claims DE-SynPUF               Synthe~ claim   <tibble>\n3 inpatient  CMS Inpatient Claims DE-SynPUF             Synthe~ claim   <tibble>\n4 outpatient CMS Outpatient Claims DE-SynPUF            Synthe~ claim   <tibble>\n5 pde        CMS Prescription Drug Events (PDE) DE-Syn~ Synthe~ claim   <tibble>\n# ... with abbreviated variable names 1: TABLE_DESCRIPTION, 2: UNIT_OF_RECORD,\n#   3: PROPERTIES\n\n\nYou can access details on the variables in each of the tables like in this example with the inpatient table. You can see that this table contains 35 fields, including a beneficiary code to identify members across tables as well as detailed information on each inpatient claim.\n\nschema %>% \n  filter(TABLE == \"inpatient\") %>% \n  pull(PROPERTIES)\n\n[[1]]\n# A tibble: 35 x 3\n   VARIABLE                 TYPE    DESCRIPTION                                 \n   <chr>                    <chr>   <chr>                                       \n 1 DESYNPUF_ID              string  Beneficiary Code                            \n 2 CLM_ID                   string  Claim ID                                    \n 3 SEGMENT                  numeric Claim Line Segment                          \n 4 CLM_FROM_DT              date    Claims start date                           \n 5 CLM_THRU_DT              date    Claims end date                             \n 6 PRVDR_NUM                string  Provider Institution                        \n 7 CLM_PMT_AMT              numeric Claim Payment Amount                        \n 8 NCH_PRMRY_PYR_CLM_PD_AMT numeric NCH Primary Payer Claim Paid Amount         \n 9 AT_PHYSN_NPI             string  Attending Physician National Provider Ident~\n10 OP_PHYSN_NPI             string  Operating Physician National Provider Ident~\n# ... with 25 more rows"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#access-claims-data-as-a-remote-database",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#access-claims-data-as-a-remote-database",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Access claims data as a remote database",
    "text": "Access claims data as a remote database\nMany organizations store claims data in a remote database, so claimsdb also includes all of the tables as an in-memory DuckDB database. This can be a great way to practice working with this type of data, including building queries with dplyr code using dbplyr.\nTo setup the in-memory database, you need to create a database connection using claims_connect() and create connections to each of the tables you want to use.\n\nlibrary(dbplyr)\n\n# Setup connection to duckDB database\ncon <- claims_connect()\n\n# Setup connections to each of the enrollment and claims tables in the database\nbene_rmt <- tbl(con, \"bene\")\ninpatient_rmt <- tbl(con, \"inpatient\")\noutpatient_rmt <- tbl(con, \"outpatient\")\ncarrier_rmt <- tbl(con, \"carrier\")\npde_rmt <- tbl(con, \"pde\")\n\nYou can then preview your connection to each of the remote tables.\n\n# Preview the prescription drug event table in the database\npde_rmt\n\n# Source:   table<pde> [?? x 8]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      PDE_ID    SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5\n   <chr>            <chr>     <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 00E040C6ECE8F878 83014463~ 2008-12-20 492880~      30      30       0      10\n 2 00E040C6ECE8F878 83594465~ 2009-04-25 529590~      20      30       0       0\n 3 00E040C6ECE8F878 83144465~ 2009-09-22 000834~      80      30      40      80\n 4 00E040C6ECE8F878 83614464~ 2009-10-03 634810~      60      10       0      10\n 5 00E040C6ECE8F878 83014461~ 2009-11-16 511294~      60      30       0      20\n 6 00E040C6ECE8F878 83234464~ 2009-12-11 580160~     270      30       0      10\n 7 014F2C07689C173B 83294462~ 2009-09-14 009045~      90      30       0      60\n 8 014F2C07689C173B 83874466~ 2009-10-11 596040~      40      20       0     570\n 9 014F2C07689C173B 83314462~ 2009-11-24 510790~      30      30       0     150\n10 014F2C07689C173B 83874467~ 2009-12-29 511293~      30      30       0      10\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#examples-of-using-claimsdb-for-analysis",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#examples-of-using-claimsdb-for-analysis",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Examples of using claimsdb for analysis",
    "text": "Examples of using claimsdb for analysis\nTo analyze and explore the claims and beneficiary data, you can execute your own SQL code on the database using the DBI package.\n\nDBI::dbGetQuery(\n  con, \n  paste0(\n    'SELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"',\n    'FROM \"bene\"',\n    'WHERE \"BENE_SEX_IDENT_CD\" = 1.0',\n    'LIMIT 10'\n  )\n)\n\n        DESYNPUF_ID BENE_BIRTH_DT\n1  001115EAB83B19BB    1939-12-01\n2  03ADA78C0FEF79F4    1923-02-01\n3  040A12AB5EAA444C    1943-10-01\n4  0507DE00BC6E6CD6    1932-07-01\n5  05672CCCED56BCAD    1937-07-01\n6  060CDE3A044F64BA    1943-02-01\n7  061B5B3D9A459675    1932-04-01\n8  08C8E0A0C6EAC884    1954-06-01\n9  09EEB5C4C4FAEF10    1935-04-01\n10 0A58C6D6B9BE67CF    1942-07-01\n\n\nHowever, in the following examples, we will use the dbplyr package which translates dplyr code into SQL that can be executed against the database. You can see that the results below with dplyr functions match the results above that used a SQL query.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT\n   <chr>            <date>       \n 1 001115EAB83B19BB 1939-12-01   \n 2 03ADA78C0FEF79F4 1923-02-01   \n 3 040A12AB5EAA444C 1943-10-01   \n 4 0507DE00BC6E6CD6 1932-07-01   \n 5 05672CCCED56BCAD 1937-07-01   \n 6 060CDE3A044F64BA 1943-02-01   \n 7 061B5B3D9A459675 1932-04-01   \n 8 08C8E0A0C6EAC884 1954-06-01   \n 9 09EEB5C4C4FAEF10 1935-04-01   \n10 0A58C6D6B9BE67CF 1942-07-01   \n# ... with more rows\n\n\nWe can use the show_query() function to see the SQL code that dbplyr created and that it closely matches the SQL code above.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT) %>% \n  show_query()\n\n<SQL>\nSELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"\nFROM \"bene\"\nWHERE (\"BENE_SEX_IDENT_CD\" = 1.0)\n\n\n\nFirst, a note about working with dates/times in databases\ndbplyr is an amazing tool for working with databases, especially if you want to use many functions from the dplyr and tidyr packages. However, it does not currently have SQL translations for all functions in the tidyverse family of packages. For example, the lubridate package’s date and time functions work on local dataframes but cannot be translated to work on remote tables at this time. In the example below, you can see that the lubridate function year() (for parsing the year from a date) works on the local dataframe but generates an error on the remote table with the same data.\n\nbene %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# A tibble: 998 x 3\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with 988 more rows\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\nFortunately, dbplyr allows you pass along SQL functions in your dplyr code, and it will include these functions in the generated query. For date/time functions, we need to consult the documentation from DuckDB on date functions. To parse a part of a date (e.g., the year), we need to use the date_part() function for DuckDB.\nThe function to do this task may vary across database backends, so if you are doing this with a different database (Oracle, SQL Server, etc.), you will need to read the documentation for that database management system.\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = date_part('year', BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\n\n\nExample 1: which members had the highest prescription drug costs for 2008?\nFor this first example, we are going to identify the beneficiaries with the highest total prescription drug costs in 2008. We need to use the pde table that has claims on prescription drug events and the bene table that has beneficiary records. We create an object that is the aggregated costs for prescription drugs at the beneficiary level in 2008. Note that we had to use date_part() to parse the year from the service date.\n\n# Calculate rx costs for utilizing members in 2008\nrx_costs_rmt <- pde_rmt %>% \n  mutate(BENE_YEAR = date_part('year', SRVC_DT)) %>% \n  filter(BENE_YEAR == 2008) %>% \n  group_by(BENE_YEAR, DESYNPUF_ID) %>% \n  summarize(TOTAL_RX_COST = sum(TOT_RX_CST_AMT, na.rm = T), .groups = \"drop\") %>% \n  ungroup()\n\nrx_costs_rmt\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 00E040C6ECE8F878            10\n 2      2008 03ADA78C0FEF79F4          5020\n 3      2008 040A12AB5EAA444C           120\n 4      2008 043AAAE41C9A37B7           810\n 5      2008 05672CCCED56BCAD             0\n 6      2008 061B5B3D9A459675           270\n 7      2008 08BB74BA9DFD5C06            20\n 8      2008 08C8E0A0C6EAC884          5860\n 9      2008 09EEB5C4C4FAEF10          1590\n10      2008 0A58C6D6B9BE67CF           930\n# ... with more rows\n\n\nThen we join the aggregated cost data to the beneficiary table. This is necessary because the pde table does not include beneficiaries who didn’t use any prescription drugs. After joining the table we reassign missing cost data to zero for those beneficiaries with no utilization. We can then use collect() to retrieve the results as a local dataframe.\n\n# Join the rx costs data to the beneficiary file and include members with no costs\nrx_bene_rmt <- bene_rmt %>% \n  filter(BENE_YEAR == 2008) %>% \n  select(\n    BENE_YEAR,\n    DESYNPUF_ID\n  ) %>% \n  left_join(\n    rx_costs_rmt, by = c(\"BENE_YEAR\", \"DESYNPUF_ID\")\n  ) %>% \n  mutate(TOTAL_RX_COST = ifelse(is.na(TOTAL_RX_COST), 0, TOTAL_RX_COST)) %>% \n  arrange(desc(TOTAL_RX_COST))\n\nrx_bene_rmt\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n# Ordered by: desc(TOTAL_RX_COST)\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with more rows\n\n# You can use collect() to bring the results into a local dataframe\nrx_bene_df <- rx_bene_rmt %>% \n  collect()\n\nrx_bene_df\n\n# A tibble: 500 x 3\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with 490 more rows\n\n\n\n\nExample 2: what percent of beneficiaries received an office visit within 30 days of discharge from a hospital?\nIn the next example, we are identifying which beneficiaries had an office visit within 30 days of being discharged. We will start with the inpatient table that contains records for all inpatient stays, including when a beneficiary was discharged. We create an object that includes the beneficiary ID, the discharge date, and the date 30 days after discharge. Note that for DuckDB we need to coerce “30” to an integer to calculate the new date.\n\n# Identify all member discharge dates and the dates 30 days after discharge from the inpatient table\nip_discharges <- inpatient_rmt %>% \n  transmute(\n    DESYNPUF_ID, \n    DSCHRG_DT = NCH_BENE_DSCHRG_DT,\n    DSCHRG_DT_30 = NCH_BENE_DSCHRG_DT + as.integer(30)) %>% \n  distinct()\n\nip_discharges\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      DSCHRG_DT  DSCHRG_DT_30\n   <chr>            <date>     <date>      \n 1 014F2C07689C173B 2009-09-20 2009-10-20  \n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24  \n 3 060CDE3A044F64BA 2008-10-18 2008-11-17  \n 4 08BB74BA9DFD5C06 2009-03-07 2009-04-06  \n 5 08BB74BA9DFD5C06 2009-03-08 2009-04-07  \n 6 08C8E0A0C6EAC884 2008-02-22 2008-03-23  \n 7 08C8E0A0C6EAC884 2009-05-28 2009-06-27  \n 8 08C8E0A0C6EAC884 2009-06-14 2009-07-14  \n 9 08C8E0A0C6EAC884 2009-07-07 2009-08-06  \n10 08C8E0A0C6EAC884 2009-08-23 2009-09-22  \n# ... with more rows\n\n\nNext, we need to identify office visits from the carrier table. I created a vector of five office visit codes for this example. Since these codes must match the values in the “HCPCS” columns, we reshape the table with pivot_longer() then filter for the office visit codes.\n\n# Create vector of office visit codes\noff_vis_cds <- as.character(99211:99215)\n\n# Identify members who had an office visit from the carrier table\noff_visit <- carrier_rmt %>% \n  select(DESYNPUF_ID, CLM_ID, CLM_FROM_DT, matches(\"HCPCS\")) %>% \n  pivot_longer(cols = matches(\"HCPCS\"), names_to = \"LINE\", values_to = \"HCPCS\") %>% \n  filter(HCPCS %in% off_vis_cds) %>% \n  distinct(DESYNPUF_ID, CLM_FROM_DT) %>% \n  mutate(OFFICE_VISIT = 1)\n\noff_visit\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      CLM_FROM_DT OFFICE_VISIT\n   <chr>            <date>             <dbl>\n 1 00E040C6ECE8F878 2009-01-17             1\n 2 00E040C6ECE8F878 2009-07-20             1\n 3 00E040C6ECE8F878 2009-07-26             1\n 4 029A22E4A3AAEE15 2008-01-25             1\n 5 029A22E4A3AAEE15 2008-01-30             1\n 6 029A22E4A3AAEE15 2008-08-28             1\n 7 029A22E4A3AAEE15 2008-11-19             1\n 8 029A22E4A3AAEE15 2009-04-01             1\n 9 029A22E4A3AAEE15 2009-04-12             1\n10 029A22E4A3AAEE15 2009-05-13             1\n# ... with more rows\n\n\nFinally, we join the office visits object to the discharges object. We can use the sql_on option in left_join() to inject some custom SQL to join when the office visit date is within 30 days of the discharge date.\n\n# Join discharges data and office visit data\ndischarge_offvis <- \n  ip_discharges %>% \n  left_join(\n    off_visit,\n    sql_on = paste0(\n      \"(LHS.DESYNPUF_ID = RHS.DESYNPUF_ID) AND\",\n      \"(RHS.CLM_FROM_DT >= LHS.DSCHRG_DT) AND\",\n      \"(RHS.CLM_FROM_DT <= LHS.DSCHRG_DT_30)\"\n      )\n  )\n\ndischarge_offvis\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID.x    DSCHRG_DT  DSCHRG_DT_30 DESYNPUF_ID.y    CLM_FROM_DT OFFIC~1\n   <chr>            <date>     <date>       <chr>            <date>        <dbl>\n 1 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-25        1\n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-30        1\n 3 060CDE3A044F64BA 2008-10-18 2008-11-17   060CDE3A044F64BA 2008-11-01        1\n 4 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-07        1\n 5 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-27        1\n 6 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-05-04        1\n 7 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-19        1\n 8 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-24        1\n 9 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-06-22        1\n10 202F481BD60B4F1C 2009-04-28 2009-05-28   202F481BD60B4F1C 2009-05-03        1\n# ... with more rows, and abbreviated variable name 1: OFFICE_VISIT\n\n\nAfter the join is complete, we can calculate the share of discharges with a timely office visit.\n\n# Find the percent of members with a office visit within 30 days of discharge\ndischarge_offvis %>% \n  distinct(\n    DESYNPUF_ID.x,\n    DSCHRG_DT,\n    OFFICE_VISIT\n  ) %>% \n  mutate(OFFICE_VISIT = ifelse(is.na(OFFICE_VISIT), 0, OFFICE_VISIT)) %>% \n  summarize(\n    OFV_RATE = mean(OFFICE_VISIT, na.rm = T)\n  )\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n  OFV_RATE\n     <dbl>\n1    0.540\n\n\n\n\nExample 3: how well are beneficiaries filling their hypertension drug prescriptions?\nFor this final example, we need to identify hypertension medications from the pde table and calculate medication adherence rates for each beneficiary. To isolate the hypertension medications, we can borrow from the HEDIS medication list for ACE inhibitors and ARB medications (which are commonly used to treat hypertension). This medication list is for 2018/2019, so it likely includes new drugs that did not exist in 2008/2009 and may not include older drugs that are no longer used (ideally it’s best to use external lists that match the time frame of the claims data).\n\nhedis_wb <- tempfile()\n\ndownload.file(\n  url = \"https://www.ncqa.org/hedis-2019-ndc-mld-directory-complete-workbook-final-11-1-2018-3/\",\n  destfile = hedis_wb,\n  mode = \"wb\"\n)\n\nhedis_df <- readxl::read_excel(\n  path = hedis_wb,\n  sheet = \"Medications List to NDC Codes\"\n)\n\nhyp_ndc_df <- \n  hedis_df %>% \n  filter(`Medication List` == \"ACE Inhibitor/ARB Medications\") %>% \n  select(\n    LIST = `Medication List`,\n    PRODUCTID = `NDC Code`\n  )\n\nhyp_ndc_df\n\n# A tibble: 3,230 x 2\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with 3,220 more rows\n\n\nOne of the cool features of dbplyr is that we can copy this local dataframe to the database as a temporary table using the copy_to() function.\n\nhyp_ndc_rmt <- copy_to(con, hyp_ndc_df, overwrite = T)\n\nhyp_ndc_rmt\n\n# Source:   table<hyp_ndc_df> [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with more rows\n\n\nWith the hypertension codes in the database, we can use inner_join() to find the matching drugs from the pde table.\n\npde_hyp_rmt <- \n  pde_rmt %>% \n  inner_join(\n    hyp_ndc_rmt, by = c(\"PROD_SRVC_ID\" = \"PRODUCTID\")\n  )\n\npde_hyp_rmt\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID   PDE_ID SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5 LIST \n   <chr>         <chr>  <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>\n 1 03ADA78C0FEF~ 83554~ 2008-02-01 510790~      30      30       0      10 ACE ~\n 2 040A12AB5EAA~ 83974~ 2009-11-27 638740~      90      30       0      10 ACE ~\n 3 043AAAE41C9A~ 83024~ 2008-08-11 002472~     200      90      10      80 ACE ~\n 4 0D540BEBC45A~ 83454~ 2009-02-18 604290~      60      90      10      10 ACE ~\n 5 0D540BEBC45A~ 83874~ 2009-06-15 661050~      30      30       0      10 ACE ~\n 6 0E0A0107787B~ 83714~ 2008-06-30 661050~     100      90      70      60 ACE ~\n 7 109D67D11477~ 83064~ 2008-11-21 134110~      30      60       0      70 ACE ~\n 8 109D67D11477~ 83034~ 2009-07-03 666850~      30      30       0      10 ACE ~\n 9 10D75CDD5B4A~ 83804~ 2008-07-01 002472~      90      90       0      10 ACE ~\n10 1183CA4884F8~ 83934~ 2009-01-25 001850~      90      20       0     110 ACE ~\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT\n\n\nWe can now use collect() to retrieve the data as a local dataframe. As a dataframe, we can now use the AdhereR package to calculate the medication possession ratio (MPR) for members who filled a hypertension medication. MPR is a commonly used metric of medication adherence, measuring if beneficiaries have gaps between their prescription fills.\nWe can see the MPR for each member who filled one of the medications, and we can calculate the median and mean MPRs across these beneficiaries.\n\n# install.packages(\"AdhereR\")\nlibrary(AdhereR)\n\npde_hyp_df <- pde_hyp_rmt %>% \n  collect()\n\nhyp_adhere <- CMA4(\n  data = pde_hyp_df,\n  ID.colname = 'DESYNPUF_ID',\n  event.date.colname = 'SRVC_DT',\n  event.duration.colname = 'DAYS_SUPLY_NUM',\n  date.format = \"%Y-%m-%d\"\n)\n\nhyp_adhere_cma <- hyp_adhere$CMA %>% tibble()\n\nhyp_adhere_cma\n\n# A tibble: 134 x 2\n   DESYNPUF_ID         CMA\n   <chr>             <dbl>\n 1 03ADA78C0FEF79F4 0.0411\n 2 040A12AB5EAA444C 0.0411\n 3 043AAAE41C9A37B7 0.123 \n 4 0D540BEBC45ADCA7 0.164 \n 5 0E0A0107787B32AA 0.123 \n 6 109D67D114778917 0.123 \n 7 10D75CDD5B4AD3B0 0.123 \n 8 1183CA4884F8A0A8 0.0274\n 9 11C45CF0DDD03ACE 0.0822\n10 1226AD9944384B64 0.0411\n# ... with 124 more rows\n\nhyp_adhere_cma %>% \n  summarize(\n    median_mpr = median(CMA),\n    mean_mpr = mean(CMA)\n  )\n\n# A tibble: 1 x 2\n  median_mpr mean_mpr\n       <dbl>    <dbl>\n1     0.0411   0.0848"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#data-limitations",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#data-limitations",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Data Limitations",
    "text": "Data Limitations\nWhile data included in claimsdb is useful for many types of analyses, it does include a few notable limitations. - As mentioned earlier, the data is a small sample (500 beneficiaries) and is not intended to be representative of the Medicare population. In addition, the data is synthetic and should not be used for drawing inferences on the Medicare population. - Since the data is more than 10 years old, it doesn’t capture newer medications or procedures. It also includes procedure codes that have been retired or replaced. This is a challenge when applying external code lists that are much newer. - The diagnosis fields in the data use the International Classification of Diseases, Ninth Revision (ICD-9), but the United States converted to ICD-10 in 2015. If you are interesting in a mapping between ICD-9 and ICD-10, CMS has resources to consider. - The Medicare population is mostly Americans aged 65 and over, so the data will not have claims on certain specialties such as pediatrics or maternity care."
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "",
    "text": "In the fields of health informatics and health services research, health insurance claims data are a valuable resource to help answer questions about health care access and financing. However, claims data in the real world often contains both sensitive (protected health information) and proprietary (trade secrets) elements. For most students and educators seeking opportunities to learn how to use claims data, there are few available sources for practice.\nTo help with this problem, claimsdb 📦 provides easy access to a sample of health insurance enrollment and claims data from the Centers for Medicare and Medicaid Services (CMS) Data Entrepreneurs’ Synthetic Public Use File (DE-SynPUF), as a set of relational tables or as an in-memory database using DuckDB. All the data is contained within a single package, so users do not need to download any external data. This package is inspired by and based on the starwarsdb package.\nThe data are structured as actual Medicare claims data but are fully “synthetic,” after a process of alterations meant to reduce the risk of re-identification of real Medicare beneficiaries. The synthetic process that CMS adopted changes the co-variation across variables, so analysts should be cautious about drawing inferences about the actual Medicare population.\nThe data included in claimsdb comes from 500 randomly selected 2008 Medicare beneficiaries from Sample 2 of the DE-SynPUF, and it includes all the associated claims for these members for 2008-2009. CMS provides resources, including a codebook, FAQs, and other documents with more information about this data.\nTo introduce claimsdb, this post covers the following topics: - Installation of the claimsdb package - How to setup a remote database with the included data - Examples of how to use claimsdb for analysis - An overview of the limitations of the data included in package"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#installation-and-components-of-the-claimsdb-package",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#installation-and-components-of-the-claimsdb-package",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Installation and components of the claimsdb package",
    "text": "Installation and components of the claimsdb package\nYou can install the development version of claimsdb from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"jfangmeier/claimsdb\")\n\nYou can then load claimsdb alongside your other packages. We will be using the tidyverse packages in our later examples.\n\nlibrary(tidyverse)\nlibrary(claimsdb)\n\nThe tables are available after loading the claimsdb package. This includes a schema that describes each of the tables and the included variables from the CMS DE-SynPUF. You can see that claimsdb includes five tables. One of the tables, bene contains beneficiary records, while the others include specific types of claims data.\n\nschema\n\n# A tibble: 5 x 5\n  TABLE      TABLE_TITLE                                TABLE~1 UNIT_~2 PROPER~3\n  <chr>      <chr>                                      <chr>   <chr>   <list>  \n1 bene       CMS Beneficiary Summary DE-SynPUF          Synthe~ Benefi~ <tibble>\n2 carrier    CMS Carrier Claims DE-SynPUF               Synthe~ claim   <tibble>\n3 inpatient  CMS Inpatient Claims DE-SynPUF             Synthe~ claim   <tibble>\n4 outpatient CMS Outpatient Claims DE-SynPUF            Synthe~ claim   <tibble>\n5 pde        CMS Prescription Drug Events (PDE) DE-Syn~ Synthe~ claim   <tibble>\n# ... with abbreviated variable names 1: TABLE_DESCRIPTION, 2: UNIT_OF_RECORD,\n#   3: PROPERTIES\n\n\nYou can access details on the variables in each of the tables like in this example with the inpatient table. You can see that this table contains 35 fields, including a beneficiary code to identify members across tables as well as detailed information on each inpatient claim.\n\nschema %>% \n  filter(TABLE == \"inpatient\") %>% \n  pull(PROPERTIES)\n\n[[1]]\n# A tibble: 35 x 3\n   VARIABLE                 TYPE    DESCRIPTION                                 \n   <chr>                    <chr>   <chr>                                       \n 1 DESYNPUF_ID              string  Beneficiary Code                            \n 2 CLM_ID                   string  Claim ID                                    \n 3 SEGMENT                  numeric Claim Line Segment                          \n 4 CLM_FROM_DT              date    Claims start date                           \n 5 CLM_THRU_DT              date    Claims end date                             \n 6 PRVDR_NUM                string  Provider Institution                        \n 7 CLM_PMT_AMT              numeric Claim Payment Amount                        \n 8 NCH_PRMRY_PYR_CLM_PD_AMT numeric NCH Primary Payer Claim Paid Amount         \n 9 AT_PHYSN_NPI             string  Attending Physician National Provider Ident~\n10 OP_PHYSN_NPI             string  Operating Physician National Provider Ident~\n# ... with 25 more rows"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#access-claims-data-as-a-remote-database",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#access-claims-data-as-a-remote-database",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Access claims data as a remote database",
    "text": "Access claims data as a remote database\nMany organizations store claims data in a remote database, so claimsdb also includes all of the tables as an in-memory DuckDB database. This can be a great way to practice working with this type of data, including building queries with dplyr code using dbplyr.\nTo setup the in-memory database, you need to create a database connection using claims_connect() and create connections to each of the tables you want to use.\n\nlibrary(dbplyr)\n\n# Setup connection to duckDB database\ncon <- claims_connect()\n\n# Setup connections to each of the enrollment and claims tables in the database\nbene_rmt <- tbl(con, \"bene\")\ninpatient_rmt <- tbl(con, \"inpatient\")\noutpatient_rmt <- tbl(con, \"outpatient\")\ncarrier_rmt <- tbl(con, \"carrier\")\npde_rmt <- tbl(con, \"pde\")\n\nYou can then preview your connection to each of the remote tables.\n\n# Preview the prescription drug event table in the database\npde_rmt\n\n# Source:   table<pde> [?? x 8]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      PDE_ID    SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5\n   <chr>            <chr>     <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 00E040C6ECE8F878 83014463~ 2008-12-20 492880~      30      30       0      10\n 2 00E040C6ECE8F878 83594465~ 2009-04-25 529590~      20      30       0       0\n 3 00E040C6ECE8F878 83144465~ 2009-09-22 000834~      80      30      40      80\n 4 00E040C6ECE8F878 83614464~ 2009-10-03 634810~      60      10       0      10\n 5 00E040C6ECE8F878 83014461~ 2009-11-16 511294~      60      30       0      20\n 6 00E040C6ECE8F878 83234464~ 2009-12-11 580160~     270      30       0      10\n 7 014F2C07689C173B 83294462~ 2009-09-14 009045~      90      30       0      60\n 8 014F2C07689C173B 83874466~ 2009-10-11 596040~      40      20       0     570\n 9 014F2C07689C173B 83314462~ 2009-11-24 510790~      30      30       0     150\n10 014F2C07689C173B 83874467~ 2009-12-29 511293~      30      30       0      10\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#examples-of-using-claimsdb-for-analysis",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#examples-of-using-claimsdb-for-analysis",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Examples of using claimsdb for analysis",
    "text": "Examples of using claimsdb for analysis\nTo analyze and explore the claims and beneficiary data, you can execute your own SQL code on the database using the DBI package.\n\nDBI::dbGetQuery(\n  con, \n  paste0(\n    'SELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"',\n    'FROM \"bene\"',\n    'WHERE \"BENE_SEX_IDENT_CD\" = 1.0',\n    'LIMIT 10'\n  )\n)\n\n        DESYNPUF_ID BENE_BIRTH_DT\n1  001115EAB83B19BB    1939-12-01\n2  03ADA78C0FEF79F4    1923-02-01\n3  040A12AB5EAA444C    1943-10-01\n4  0507DE00BC6E6CD6    1932-07-01\n5  05672CCCED56BCAD    1937-07-01\n6  060CDE3A044F64BA    1943-02-01\n7  061B5B3D9A459675    1932-04-01\n8  08C8E0A0C6EAC884    1954-06-01\n9  09EEB5C4C4FAEF10    1935-04-01\n10 0A58C6D6B9BE67CF    1942-07-01\n\n\nHowever, in the following examples, we will use the dbplyr package which translates dplyr code into SQL that can be executed against the database. You can see that the results below with dplyr functions match the results above that used a SQL query.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT\n   <chr>            <date>       \n 1 001115EAB83B19BB 1939-12-01   \n 2 03ADA78C0FEF79F4 1923-02-01   \n 3 040A12AB5EAA444C 1943-10-01   \n 4 0507DE00BC6E6CD6 1932-07-01   \n 5 05672CCCED56BCAD 1937-07-01   \n 6 060CDE3A044F64BA 1943-02-01   \n 7 061B5B3D9A459675 1932-04-01   \n 8 08C8E0A0C6EAC884 1954-06-01   \n 9 09EEB5C4C4FAEF10 1935-04-01   \n10 0A58C6D6B9BE67CF 1942-07-01   \n# ... with more rows\n\n\nWe can use the show_query() function to see the SQL code that dbplyr created and that it closely matches the SQL code above.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT) %>% \n  show_query()\n\n<SQL>\nSELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"\nFROM \"bene\"\nWHERE (\"BENE_SEX_IDENT_CD\" = 1.0)\n\n\n\nFirst, a note about working with dates/times in databases\ndbplyr is an amazing tool for working with databases, especially if you want to use many functions from the dplyr and tidyr packages. However, it does not currently have SQL translations for all functions in the tidyverse family of packages. For example, the lubridate package’s date and time functions work on local dataframes but cannot be translated to work on remote tables at this time. In the example below, you can see that the lubridate function year() (for parsing the year from a date) works on the local dataframe but generates an error on the remote table with the same data.\n\nbene %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# A tibble: 998 x 3\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with 988 more rows\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\nFortunately, dbplyr allows you pass along SQL functions in your dplyr code, and it will include these functions in the generated query. For date/time functions, we need to consult the documentation from DuckDB on date functions. To parse a part of a date (e.g., the year), we need to use the date_part() function for DuckDB.\nThe function to do this task may vary across database backends, so if you are doing this with a different database (Oracle, SQL Server, etc.), you will need to read the documentation for that database management system.\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = date_part('year', BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\n\n\nExample 1: which members had the highest prescription drug costs for 2008?\nFor this first example, we are going to identify the beneficiaries with the highest total prescription drug costs in 2008. We need to use the pde table that has claims on prescription drug events and the bene table that has beneficiary records. We create an object that is the aggregated costs for prescription drugs at the beneficiary level in 2008. Note that we had to use date_part() to parse the year from the service date.\n\n# Calculate rx costs for utilizing members in 2008\nrx_costs_rmt <- pde_rmt %>% \n  mutate(BENE_YEAR = date_part('year', SRVC_DT)) %>% \n  filter(BENE_YEAR == 2008) %>% \n  group_by(BENE_YEAR, DESYNPUF_ID) %>% \n  summarize(TOTAL_RX_COST = sum(TOT_RX_CST_AMT, na.rm = T), .groups = \"drop\") %>% \n  ungroup()\n\nrx_costs_rmt\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 00E040C6ECE8F878            10\n 2      2008 03ADA78C0FEF79F4          5020\n 3      2008 040A12AB5EAA444C           120\n 4      2008 043AAAE41C9A37B7           810\n 5      2008 05672CCCED56BCAD             0\n 6      2008 061B5B3D9A459675           270\n 7      2008 08BB74BA9DFD5C06            20\n 8      2008 08C8E0A0C6EAC884          5860\n 9      2008 09EEB5C4C4FAEF10          1590\n10      2008 0A58C6D6B9BE67CF           930\n# ... with more rows\n\n\nThen we join the aggregated cost data to the beneficiary table. This is necessary because the pde table does not include beneficiaries who didn’t use any prescription drugs. After joining the table we reassign missing cost data to zero for those beneficiaries with no utilization. We can then use collect() to retrieve the results as a local dataframe.\n\n# Join the rx costs data to the beneficiary file and include members with no costs\nrx_bene_rmt <- bene_rmt %>% \n  filter(BENE_YEAR == 2008) %>% \n  select(\n    BENE_YEAR,\n    DESYNPUF_ID\n  ) %>% \n  left_join(\n    rx_costs_rmt, by = c(\"BENE_YEAR\", \"DESYNPUF_ID\")\n  ) %>% \n  mutate(TOTAL_RX_COST = ifelse(is.na(TOTAL_RX_COST), 0, TOTAL_RX_COST)) %>% \n  arrange(desc(TOTAL_RX_COST))\n\nrx_bene_rmt\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n# Ordered by: desc(TOTAL_RX_COST)\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with more rows\n\n# You can use collect() to bring the results into a local dataframe\nrx_bene_df <- rx_bene_rmt %>% \n  collect()\n\nrx_bene_df\n\n# A tibble: 500 x 3\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with 490 more rows\n\n\n\n\nExample 2: what percent of beneficiaries received an office visit within 30 days of discharge from a hospital?\nIn the next example, we are identifying which beneficiaries had an office visit within 30 days of being discharged. We will start with the inpatient table that contains records for all inpatient stays, including when a beneficiary was discharged. We create an object that includes the beneficiary ID, the discharge date, and the date 30 days after discharge. Note that for DuckDB we need to coerce “30” to an integer to calculate the new date.\n\n# Identify all member discharge dates and the dates 30 days after discharge from the inpatient table\nip_discharges <- inpatient_rmt %>% \n  transmute(\n    DESYNPUF_ID, \n    DSCHRG_DT = NCH_BENE_DSCHRG_DT,\n    DSCHRG_DT_30 = NCH_BENE_DSCHRG_DT + as.integer(30)) %>% \n  distinct()\n\nip_discharges\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      DSCHRG_DT  DSCHRG_DT_30\n   <chr>            <date>     <date>      \n 1 014F2C07689C173B 2009-09-20 2009-10-20  \n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24  \n 3 060CDE3A044F64BA 2008-10-18 2008-11-17  \n 4 08BB74BA9DFD5C06 2009-03-07 2009-04-06  \n 5 08BB74BA9DFD5C06 2009-03-08 2009-04-07  \n 6 08C8E0A0C6EAC884 2008-02-22 2008-03-23  \n 7 08C8E0A0C6EAC884 2009-05-28 2009-06-27  \n 8 08C8E0A0C6EAC884 2009-06-14 2009-07-14  \n 9 08C8E0A0C6EAC884 2009-07-07 2009-08-06  \n10 08C8E0A0C6EAC884 2009-08-23 2009-09-22  \n# ... with more rows\n\n\nNext, we need to identify office visits from the carrier table. I created a vector of five office visit codes for this example. Since these codes must match the values in the “HCPCS” columns, we reshape the table with pivot_longer() then filter for the office visit codes.\n\n# Create vector of office visit codes\noff_vis_cds <- as.character(99211:99215)\n\n# Identify members who had an office visit from the carrier table\noff_visit <- carrier_rmt %>% \n  select(DESYNPUF_ID, CLM_ID, CLM_FROM_DT, matches(\"HCPCS\")) %>% \n  pivot_longer(cols = matches(\"HCPCS\"), names_to = \"LINE\", values_to = \"HCPCS\") %>% \n  filter(HCPCS %in% off_vis_cds) %>% \n  distinct(DESYNPUF_ID, CLM_FROM_DT) %>% \n  mutate(OFFICE_VISIT = 1)\n\noff_visit\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      CLM_FROM_DT OFFICE_VISIT\n   <chr>            <date>             <dbl>\n 1 00E040C6ECE8F878 2009-01-17             1\n 2 00E040C6ECE8F878 2009-07-20             1\n 3 00E040C6ECE8F878 2009-07-26             1\n 4 029A22E4A3AAEE15 2008-01-25             1\n 5 029A22E4A3AAEE15 2008-01-30             1\n 6 029A22E4A3AAEE15 2008-08-28             1\n 7 029A22E4A3AAEE15 2008-11-19             1\n 8 029A22E4A3AAEE15 2009-04-01             1\n 9 029A22E4A3AAEE15 2009-04-12             1\n10 029A22E4A3AAEE15 2009-05-13             1\n# ... with more rows\n\n\nFinally, we join the office visits object to the discharges object. We can use the sql_on option in left_join() to inject some custom SQL to join when the office visit date is within 30 days of the discharge date.\n\n# Join discharges data and office visit data\ndischarge_offvis <- \n  ip_discharges %>% \n  left_join(\n    off_visit,\n    sql_on = paste0(\n      \"(LHS.DESYNPUF_ID = RHS.DESYNPUF_ID) AND\",\n      \"(RHS.CLM_FROM_DT >= LHS.DSCHRG_DT) AND\",\n      \"(RHS.CLM_FROM_DT <= LHS.DSCHRG_DT_30)\"\n      )\n  )\n\ndischarge_offvis\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID.x    DSCHRG_DT  DSCHRG_DT_30 DESYNPUF_ID.y    CLM_FROM_DT OFFIC~1\n   <chr>            <date>     <date>       <chr>            <date>        <dbl>\n 1 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-25        1\n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-30        1\n 3 060CDE3A044F64BA 2008-10-18 2008-11-17   060CDE3A044F64BA 2008-11-01        1\n 4 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-07        1\n 5 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-27        1\n 6 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-05-04        1\n 7 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-19        1\n 8 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-24        1\n 9 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-06-22        1\n10 202F481BD60B4F1C 2009-04-28 2009-05-28   202F481BD60B4F1C 2009-05-03        1\n# ... with more rows, and abbreviated variable name 1: OFFICE_VISIT\n\n\nAfter the join is complete, we can calculate the share of discharges with a timely office visit.\n\n# Find the percent of members with a office visit within 30 days of discharge\ndischarge_offvis %>% \n  distinct(\n    DESYNPUF_ID.x,\n    DSCHRG_DT,\n    OFFICE_VISIT\n  ) %>% \n  mutate(OFFICE_VISIT = ifelse(is.na(OFFICE_VISIT), 0, OFFICE_VISIT)) %>% \n  summarize(\n    OFV_RATE = mean(OFFICE_VISIT, na.rm = T)\n  )\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n  OFV_RATE\n     <dbl>\n1    0.540\n\n\n\n\nExample 3: how well are beneficiaries filling their hypertension drug prescriptions?\nFor this final example, we need to identify hypertension medications from the pde table and calculate medication adherence rates for each beneficiary. To isolate the hypertension medications, we can borrow from the HEDIS medication list for ACE inhibitors and ARB medications (which are commonly used to treat hypertension). This medication list is for 2018/2019, so it likely includes new drugs that did not exist in 2008/2009 and may not include older drugs that are no longer used (ideally it’s best to use external lists that match the time frame of the claims data).\n\nhedis_wb <- tempfile()\n\ndownload.file(\n  url = \"https://www.ncqa.org/hedis-2019-ndc-mld-directory-complete-workbook-final-11-1-2018-3/\",\n  destfile = hedis_wb,\n  mode = \"wb\"\n)\n\nhedis_df <- readxl::read_excel(\n  path = hedis_wb,\n  sheet = \"Medications List to NDC Codes\"\n)\n\nhyp_ndc_df <- \n  hedis_df %>% \n  filter(`Medication List` == \"ACE Inhibitor/ARB Medications\") %>% \n  select(\n    LIST = `Medication List`,\n    PRODUCTID = `NDC Code`\n  )\n\nhyp_ndc_df\n\n# A tibble: 3,230 x 2\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with 3,220 more rows\n\n\nOne of the cool features of dbplyr is that we can copy this local dataframe to the database as a temporary table using the copy_to() function.\n\nhyp_ndc_rmt <- copy_to(con, hyp_ndc_df, overwrite = T)\n\nhyp_ndc_rmt\n\n# Source:   table<hyp_ndc_df> [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with more rows\n\n\nWith the hypertension codes in the database, we can use inner_join() to find the matching drugs from the pde table.\n\npde_hyp_rmt <- \n  pde_rmt %>% \n  inner_join(\n    hyp_ndc_rmt, by = c(\"PROD_SRVC_ID\" = \"PRODUCTID\")\n  )\n\npde_hyp_rmt\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID   PDE_ID SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5 LIST \n   <chr>         <chr>  <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>\n 1 03ADA78C0FEF~ 83554~ 2008-02-01 510790~      30      30       0      10 ACE ~\n 2 040A12AB5EAA~ 83974~ 2009-11-27 638740~      90      30       0      10 ACE ~\n 3 043AAAE41C9A~ 83024~ 2008-08-11 002472~     200      90      10      80 ACE ~\n 4 0D540BEBC45A~ 83454~ 2009-02-18 604290~      60      90      10      10 ACE ~\n 5 0D540BEBC45A~ 83874~ 2009-06-15 661050~      30      30       0      10 ACE ~\n 6 0E0A0107787B~ 83714~ 2008-06-30 661050~     100      90      70      60 ACE ~\n 7 109D67D11477~ 83064~ 2008-11-21 134110~      30      60       0      70 ACE ~\n 8 109D67D11477~ 83034~ 2009-07-03 666850~      30      30       0      10 ACE ~\n 9 10D75CDD5B4A~ 83804~ 2008-07-01 002472~      90      90       0      10 ACE ~\n10 1183CA4884F8~ 83934~ 2009-01-25 001850~      90      20       0     110 ACE ~\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT\n\n\nWe can now use collect() to retrieve the data as a local dataframe. As a dataframe, we can now use the AdhereR package to calculate the medication possession ratio (MPR) for members who filled a hypertension medication. MPR is a commonly used metric of medication adherence, measuring if beneficiaries have gaps between their prescription fills.\nWe can see the MPR for each member who filled one of the medications, and we can calculate the median and mean MPRs across these beneficiaries.\n\n# install.packages(\"AdhereR\")\nlibrary(AdhereR)\n\npde_hyp_df <- pde_hyp_rmt %>% \n  collect()\n\nhyp_adhere <- CMA4(\n  data = pde_hyp_df,\n  ID.colname = 'DESYNPUF_ID',\n  event.date.colname = 'SRVC_DT',\n  event.duration.colname = 'DAYS_SUPLY_NUM',\n  date.format = \"%Y-%m-%d\"\n)\n\nhyp_adhere_cma <- hyp_adhere$CMA %>% tibble()\n\nhyp_adhere_cma\n\n# A tibble: 134 x 2\n   DESYNPUF_ID         CMA\n   <chr>             <dbl>\n 1 03ADA78C0FEF79F4 0.0411\n 2 040A12AB5EAA444C 0.0411\n 3 043AAAE41C9A37B7 0.123 \n 4 0D540BEBC45ADCA7 0.164 \n 5 0E0A0107787B32AA 0.123 \n 6 109D67D114778917 0.123 \n 7 10D75CDD5B4AD3B0 0.123 \n 8 1183CA4884F8A0A8 0.0274\n 9 11C45CF0DDD03ACE 0.0822\n10 1226AD9944384B64 0.0411\n# ... with 124 more rows\n\nhyp_adhere_cma %>% \n  summarize(\n    median_mpr = median(CMA),\n    mean_mpr = mean(CMA)\n  )\n\n# A tibble: 1 x 2\n  median_mpr mean_mpr\n       <dbl>    <dbl>\n1     0.0411   0.0848"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#data-limitations",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#data-limitations",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Data Limitations",
    "text": "Data Limitations\nWhile data included in claimsdb is useful for many types of analyses, it does include a few notable limitations. - As mentioned earlier, the data is a small sample (500 beneficiaries) and is not intended to be representative of the Medicare population. In addition, the data is synthetic and should not be used for drawing inferences on the Medicare population. - Since the data is more than 10 years old, it doesn’t capture newer medications or procedures. It also includes procedure codes that have been retired or replaced. This is a challenge when applying external code lists that are much newer. - The diagnosis fields in the data use the International Classification of Diseases, Ninth Revision (ICD-9), but the United States converted to ICD-10 in 2015. If you are interesting in a mapping between ICD-9 and ICD-10, CMS has resources to consider. - The Medicare population is mostly Americans aged 65 and over, so the data will not have claims on certain specialties such as pediatrics or maternity care."
  },
  {
    "objectID": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html",
    "href": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "",
    "text": "In the fields of health informatics and health services research, health insurance claims data are a valuable resource to help answer questions about health care access and financing. However, claims data in the real world often contains both sensitive (protected health information) and proprietary (trade secrets) elements. For most students and educators seeking opportunities to learn how to use claims data, there are few available sources for practice.\nTo help with this problem, claimsdb 📦 provides easy access to a sample of health insurance enrollment and claims data from the Centers for Medicare and Medicaid Services (CMS) Data Entrepreneurs’ Synthetic Public Use File (DE-SynPUF), as a set of relational tables or as an in-memory database using DuckDB. All the data is contained within a single package, so users do not need to download any external data. This package is inspired by and based on the starwarsdb package.\nThe data are structured as actual Medicare claims data but are fully “synthetic,” after a process of alterations meant to reduce the risk of re-identification of real Medicare beneficiaries. The synthetic process that CMS adopted changes the co-variation across variables, so analysts should be cautious about drawing inferences about the actual Medicare population.\nThe data included in claimsdb comes from 500 randomly selected 2008 Medicare beneficiaries from Sample 2 of the DE-SynPUF, and it includes all the associated claims for these members for 2008-2009. CMS provides resources, including a codebook, FAQs, and other documents with more information about this data.\nTo introduce claimsdb, this post covers the following topics: - Installation of the claimsdb package - How to setup a remote database with the included data - Examples of how to use claimsdb for analysis - An overview of the limitations of the data included in package"
  },
  {
    "objectID": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#installation-and-components-of-the-claimsdb-package",
    "href": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#installation-and-components-of-the-claimsdb-package",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Installation and components of the claimsdb package",
    "text": "Installation and components of the claimsdb package\nYou can install the development version of claimsdb from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"jfangmeier/claimsdb\")\n\nYou can then load claimsdb alongside your other packages. We will be using the tidyverse packages in our later examples.\n\nlibrary(tidyverse)\nlibrary(claimsdb)\n\nThe tables are available after loading the claimsdb package. This includes a schema that describes each of the tables and the included variables from the CMS DE-SynPUF. You can see that claimsdb includes five tables. One of the tables, bene contains beneficiary records, while the others include specific types of claims data.\n\nschema\n\n# A tibble: 5 x 5\n  TABLE      TABLE_TITLE                                TABLE~1 UNIT_~2 PROPER~3\n  <chr>      <chr>                                      <chr>   <chr>   <list>  \n1 bene       CMS Beneficiary Summary DE-SynPUF          Synthe~ Benefi~ <tibble>\n2 carrier    CMS Carrier Claims DE-SynPUF               Synthe~ claim   <tibble>\n3 inpatient  CMS Inpatient Claims DE-SynPUF             Synthe~ claim   <tibble>\n4 outpatient CMS Outpatient Claims DE-SynPUF            Synthe~ claim   <tibble>\n5 pde        CMS Prescription Drug Events (PDE) DE-Syn~ Synthe~ claim   <tibble>\n# ... with abbreviated variable names 1: TABLE_DESCRIPTION, 2: UNIT_OF_RECORD,\n#   3: PROPERTIES\n\n\nYou can access details on the variables in each of the tables like in this example with the inpatient table. You can see that this table contains 35 fields, including a beneficiary code to identify members across tables as well as detailed information on each inpatient claim.\n\nschema %>% \n  filter(TABLE == \"inpatient\") %>% \n  pull(PROPERTIES)\n\n[[1]]\n# A tibble: 35 x 3\n   VARIABLE                 TYPE    DESCRIPTION                                 \n   <chr>                    <chr>   <chr>                                       \n 1 DESYNPUF_ID              string  Beneficiary Code                            \n 2 CLM_ID                   string  Claim ID                                    \n 3 SEGMENT                  numeric Claim Line Segment                          \n 4 CLM_FROM_DT              date    Claims start date                           \n 5 CLM_THRU_DT              date    Claims end date                             \n 6 PRVDR_NUM                string  Provider Institution                        \n 7 CLM_PMT_AMT              numeric Claim Payment Amount                        \n 8 NCH_PRMRY_PYR_CLM_PD_AMT numeric NCH Primary Payer Claim Paid Amount         \n 9 AT_PHYSN_NPI             string  Attending Physician National Provider Ident~\n10 OP_PHYSN_NPI             string  Operating Physician National Provider Ident~\n# ... with 25 more rows"
  },
  {
    "objectID": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#access-claims-data-as-a-remote-database",
    "href": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#access-claims-data-as-a-remote-database",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Access claims data as a remote database",
    "text": "Access claims data as a remote database\nMany organizations store claims data in a remote database, so claimsdb also includes all of the tables as an in-memory DuckDB database. This can be a great way to practice working with this type of data, including building queries with dplyr code using dbplyr.\nTo setup the in-memory database, you need to create a database connection using claims_connect() and create connections to each of the tables you want to use.\n\nlibrary(dbplyr)\n\n# Setup connection to duckDB database\ncon <- claims_connect()\n\n# Setup connections to each of the enrollment and claims tables in the database\nbene_rmt <- tbl(con, \"bene\")\ninpatient_rmt <- tbl(con, \"inpatient\")\noutpatient_rmt <- tbl(con, \"outpatient\")\ncarrier_rmt <- tbl(con, \"carrier\")\npde_rmt <- tbl(con, \"pde\")\n\nYou can then preview your connection to each of the remote tables.\n\n# Preview the prescription drug event table in the database\npde_rmt\n\n# Source:   table<pde> [?? x 8]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      PDE_ID    SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5\n   <chr>            <chr>     <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 00E040C6ECE8F878 83014463~ 2008-12-20 492880~      30      30       0      10\n 2 00E040C6ECE8F878 83594465~ 2009-04-25 529590~      20      30       0       0\n 3 00E040C6ECE8F878 83144465~ 2009-09-22 000834~      80      30      40      80\n 4 00E040C6ECE8F878 83614464~ 2009-10-03 634810~      60      10       0      10\n 5 00E040C6ECE8F878 83014461~ 2009-11-16 511294~      60      30       0      20\n 6 00E040C6ECE8F878 83234464~ 2009-12-11 580160~     270      30       0      10\n 7 014F2C07689C173B 83294462~ 2009-09-14 009045~      90      30       0      60\n 8 014F2C07689C173B 83874466~ 2009-10-11 596040~      40      20       0     570\n 9 014F2C07689C173B 83314462~ 2009-11-24 510790~      30      30       0     150\n10 014F2C07689C173B 83874467~ 2009-12-29 511293~      30      30       0      10\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT"
  },
  {
    "objectID": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#examples-of-using-claimsdb-for-analysis",
    "href": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#examples-of-using-claimsdb-for-analysis",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Examples of using claimsdb for analysis",
    "text": "Examples of using claimsdb for analysis\nTo analyze and explore the claims and beneficiary data, you can execute your own SQL code on the database using the DBI package.\n\nDBI::dbGetQuery(\n  con, \n  paste0(\n    'SELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"',\n    'FROM \"bene\"',\n    'WHERE \"BENE_SEX_IDENT_CD\" = 1.0',\n    'LIMIT 10'\n  )\n)\n\n        DESYNPUF_ID BENE_BIRTH_DT\n1  001115EAB83B19BB    1939-12-01\n2  03ADA78C0FEF79F4    1923-02-01\n3  040A12AB5EAA444C    1943-10-01\n4  0507DE00BC6E6CD6    1932-07-01\n5  05672CCCED56BCAD    1937-07-01\n6  060CDE3A044F64BA    1943-02-01\n7  061B5B3D9A459675    1932-04-01\n8  08C8E0A0C6EAC884    1954-06-01\n9  09EEB5C4C4FAEF10    1935-04-01\n10 0A58C6D6B9BE67CF    1942-07-01\n\n\nHowever, in the following examples, we will use the dbplyr package which translates dplyr code into SQL that can be executed against the database. You can see that the results below with dplyr functions match the results above that used a SQL query.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT\n   <chr>            <date>       \n 1 001115EAB83B19BB 1939-12-01   \n 2 03ADA78C0FEF79F4 1923-02-01   \n 3 040A12AB5EAA444C 1943-10-01   \n 4 0507DE00BC6E6CD6 1932-07-01   \n 5 05672CCCED56BCAD 1937-07-01   \n 6 060CDE3A044F64BA 1943-02-01   \n 7 061B5B3D9A459675 1932-04-01   \n 8 08C8E0A0C6EAC884 1954-06-01   \n 9 09EEB5C4C4FAEF10 1935-04-01   \n10 0A58C6D6B9BE67CF 1942-07-01   \n# ... with more rows\n\n\nWe can use the show_query() function to see the SQL code that dbplyr created and that it closely matches the SQL code above.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT) %>% \n  show_query()\n\n<SQL>\nSELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"\nFROM \"bene\"\nWHERE (\"BENE_SEX_IDENT_CD\" = 1.0)\n\n\n\nFirst, a note about working with dates/times in databases\ndbplyr is an amazing tool for working with databases, especially if you want to use many functions from the dplyr and tidyr packages. However, it does not currently have SQL translations for all functions in the tidyverse family of packages. For example, the lubridate package’s date and time functions work on local dataframes but cannot be translated to work on remote tables at this time. In the example below, you can see that the lubridate function year() (for parsing the year from a date) works on the local dataframe but generates an error on the remote table with the same data.\n\nbene %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# A tibble: 998 x 3\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with 988 more rows\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\nFortunately, dbplyr allows you pass along SQL functions in your dplyr code, and it will include these functions in the generated query. For date/time functions, we need to consult the documentation from DuckDB on date functions. To parse a part of a date (e.g., the year), we need to use the date_part() function for DuckDB.\nThe function to do this task may vary across database backends, so if you are doing this with a different database (Oracle, SQL Server, etc.), you will need to read the documentation for that database management system.\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = date_part('year', BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\n\n\nExample 1: which members had the highest prescription drug costs for 2008?\nFor this first example, we are going to identify the beneficiaries with the highest total prescription drug costs in 2008. We need to use the pde table that has claims on prescription drug events and the bene table that has beneficiary records. We create an object that is the aggregated costs for prescription drugs at the beneficiary level in 2008. Note that we had to use date_part() to parse the year from the service date.\n\n# Calculate rx costs for utilizing members in 2008\nrx_costs_rmt <- pde_rmt %>% \n  mutate(BENE_YEAR = date_part('year', SRVC_DT)) %>% \n  filter(BENE_YEAR == 2008) %>% \n  group_by(BENE_YEAR, DESYNPUF_ID) %>% \n  summarize(TOTAL_RX_COST = sum(TOT_RX_CST_AMT, na.rm = T), .groups = \"drop\") %>% \n  ungroup()\n\nrx_costs_rmt\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 00E040C6ECE8F878            10\n 2      2008 03ADA78C0FEF79F4          5020\n 3      2008 040A12AB5EAA444C           120\n 4      2008 043AAAE41C9A37B7           810\n 5      2008 05672CCCED56BCAD             0\n 6      2008 061B5B3D9A459675           270\n 7      2008 08BB74BA9DFD5C06            20\n 8      2008 08C8E0A0C6EAC884          5860\n 9      2008 09EEB5C4C4FAEF10          1590\n10      2008 0A58C6D6B9BE67CF           930\n# ... with more rows\n\n\nThen we join the aggregated cost data to the beneficiary table. This is necessary because the pde table does not include beneficiaries who didn’t use any prescription drugs. After joining the table we reassign missing cost data to zero for those beneficiaries with no utilization. We can then use collect() to retrieve the results as a local dataframe.\n\n# Join the rx costs data to the beneficiary file and include members with no costs\nrx_bene_rmt <- bene_rmt %>% \n  filter(BENE_YEAR == 2008) %>% \n  select(\n    BENE_YEAR,\n    DESYNPUF_ID\n  ) %>% \n  left_join(\n    rx_costs_rmt, by = c(\"BENE_YEAR\", \"DESYNPUF_ID\")\n  ) %>% \n  mutate(TOTAL_RX_COST = ifelse(is.na(TOTAL_RX_COST), 0, TOTAL_RX_COST)) %>% \n  arrange(desc(TOTAL_RX_COST))\n\nrx_bene_rmt\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n# Ordered by: desc(TOTAL_RX_COST)\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with more rows\n\n# You can use collect() to bring the results into a local dataframe\nrx_bene_df <- rx_bene_rmt %>% \n  collect()\n\nrx_bene_df\n\n# A tibble: 500 x 3\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with 490 more rows\n\n\n\n\nExample 2: what percent of beneficiaries received an office visit within 30 days of discharge from a hospital?\nIn the next example, we are identifying which beneficiaries had an office visit within 30 days of being discharged. We will start with the inpatient table that contains records for all inpatient stays, including when a beneficiary was discharged. We create an object that includes the beneficiary ID, the discharge date, and the date 30 days after discharge. Note that for DuckDB we need to coerce “30” to an integer to calculate the new date.\n\n# Identify all member discharge dates and the dates 30 days after discharge from the inpatient table\nip_discharges <- inpatient_rmt %>% \n  transmute(\n    DESYNPUF_ID, \n    DSCHRG_DT = NCH_BENE_DSCHRG_DT,\n    DSCHRG_DT_30 = NCH_BENE_DSCHRG_DT + as.integer(30)) %>% \n  distinct()\n\nip_discharges\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      DSCHRG_DT  DSCHRG_DT_30\n   <chr>            <date>     <date>      \n 1 014F2C07689C173B 2009-09-20 2009-10-20  \n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24  \n 3 060CDE3A044F64BA 2008-10-18 2008-11-17  \n 4 08BB74BA9DFD5C06 2009-03-07 2009-04-06  \n 5 08BB74BA9DFD5C06 2009-03-08 2009-04-07  \n 6 08C8E0A0C6EAC884 2008-02-22 2008-03-23  \n 7 08C8E0A0C6EAC884 2009-05-28 2009-06-27  \n 8 08C8E0A0C6EAC884 2009-06-14 2009-07-14  \n 9 08C8E0A0C6EAC884 2009-07-07 2009-08-06  \n10 08C8E0A0C6EAC884 2009-08-23 2009-09-22  \n# ... with more rows\n\n\nNext, we need to identify office visits from the carrier table. I created a vector of five office visit codes for this example. Since these codes must match the values in the “HCPCS” columns, we reshape the table with pivot_longer() then filter for the office visit codes.\n\n# Create vector of office visit codes\noff_vis_cds <- as.character(99211:99215)\n\n# Identify members who had an office visit from the carrier table\noff_visit <- carrier_rmt %>% \n  select(DESYNPUF_ID, CLM_ID, CLM_FROM_DT, matches(\"HCPCS\")) %>% \n  pivot_longer(cols = matches(\"HCPCS\"), names_to = \"LINE\", values_to = \"HCPCS\") %>% \n  filter(HCPCS %in% off_vis_cds) %>% \n  distinct(DESYNPUF_ID, CLM_FROM_DT) %>% \n  mutate(OFFICE_VISIT = 1)\n\noff_visit\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      CLM_FROM_DT OFFICE_VISIT\n   <chr>            <date>             <dbl>\n 1 00E040C6ECE8F878 2009-01-17             1\n 2 00E040C6ECE8F878 2009-07-20             1\n 3 00E040C6ECE8F878 2009-07-26             1\n 4 029A22E4A3AAEE15 2008-01-25             1\n 5 029A22E4A3AAEE15 2008-01-30             1\n 6 029A22E4A3AAEE15 2008-08-28             1\n 7 029A22E4A3AAEE15 2008-11-19             1\n 8 029A22E4A3AAEE15 2009-04-01             1\n 9 029A22E4A3AAEE15 2009-04-12             1\n10 029A22E4A3AAEE15 2009-05-13             1\n# ... with more rows\n\n\nFinally, we join the office visits object to the discharges object. We can use the sql_on option in left_join() to inject some custom SQL to join when the office visit date is within 30 days of the discharge date.\n\n# Join discharges data and office visit data\ndischarge_offvis <- \n  ip_discharges %>% \n  left_join(\n    off_visit,\n    sql_on = paste0(\n      \"(LHS.DESYNPUF_ID = RHS.DESYNPUF_ID) AND\",\n      \"(RHS.CLM_FROM_DT >= LHS.DSCHRG_DT) AND\",\n      \"(RHS.CLM_FROM_DT <= LHS.DSCHRG_DT_30)\"\n      )\n  )\n\ndischarge_offvis\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID.x    DSCHRG_DT  DSCHRG_DT_30 DESYNPUF_ID.y    CLM_FROM_DT OFFIC~1\n   <chr>            <date>     <date>       <chr>            <date>        <dbl>\n 1 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-25        1\n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-30        1\n 3 060CDE3A044F64BA 2008-10-18 2008-11-17   060CDE3A044F64BA 2008-11-01        1\n 4 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-07        1\n 5 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-27        1\n 6 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-05-04        1\n 7 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-19        1\n 8 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-24        1\n 9 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-06-22        1\n10 202F481BD60B4F1C 2009-04-28 2009-05-28   202F481BD60B4F1C 2009-05-03        1\n# ... with more rows, and abbreviated variable name 1: OFFICE_VISIT\n\n\nAfter the join is complete, we can calculate the share of discharges with a timely office visit.\n\n# Find the percent of members with a office visit within 30 days of discharge\ndischarge_offvis %>% \n  distinct(\n    DESYNPUF_ID.x,\n    DSCHRG_DT,\n    OFFICE_VISIT\n  ) %>% \n  mutate(OFFICE_VISIT = ifelse(is.na(OFFICE_VISIT), 0, OFFICE_VISIT)) %>% \n  summarize(\n    OFV_RATE = mean(OFFICE_VISIT, na.rm = T)\n  )\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n  OFV_RATE\n     <dbl>\n1    0.540\n\n\n\n\nExample 3: how well are beneficiaries filling their hypertension drug prescriptions?\nFor this final example, we need to identify hypertension medications from the pde table and calculate medication adherence rates for each beneficiary. To isolate the hypertension medications, we can borrow from the HEDIS medication list for ACE inhibitors and ARB medications (which are commonly used to treat hypertension). This medication list is for 2018/2019, so it likely includes new drugs that did not exist in 2008/2009 and may not include older drugs that are no longer used (ideally it’s best to use external lists that match the time frame of the claims data).\n\nhedis_wb <- tempfile()\n\ndownload.file(\n  url = \"https://www.ncqa.org/hedis-2019-ndc-mld-directory-complete-workbook-final-11-1-2018-3/\",\n  destfile = hedis_wb,\n  mode = \"wb\"\n)\n\nhedis_df <- readxl::read_excel(\n  path = hedis_wb,\n  sheet = \"Medications List to NDC Codes\"\n)\n\nhyp_ndc_df <- \n  hedis_df %>% \n  filter(`Medication List` == \"ACE Inhibitor/ARB Medications\") %>% \n  select(\n    LIST = `Medication List`,\n    PRODUCTID = `NDC Code`\n  )\n\nhyp_ndc_df\n\n# A tibble: 3,230 x 2\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with 3,220 more rows\n\n\nOne of the cool features of dbplyr is that we can copy this local dataframe to the database as a temporary table using the copy_to() function.\n\nhyp_ndc_rmt <- copy_to(con, hyp_ndc_df, overwrite = T)\n\nhyp_ndc_rmt\n\n# Source:   table<hyp_ndc_df> [?? x 2]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with more rows\n\n\nWith the hypertension codes in the database, we can use inner_join() to find the matching drugs from the pde table.\n\npde_hyp_rmt <- \n  pde_rmt %>% \n  inner_join(\n    hyp_ndc_rmt, by = c(\"PROD_SRVC_ID\" = \"PRODUCTID\")\n  )\n\npde_hyp_rmt\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB 0.3.5-dev1410 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID   PDE_ID SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5 LIST \n   <chr>         <chr>  <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>\n 1 03ADA78C0FEF~ 83554~ 2008-02-01 510790~      30      30       0      10 ACE ~\n 2 040A12AB5EAA~ 83974~ 2009-11-27 638740~      90      30       0      10 ACE ~\n 3 043AAAE41C9A~ 83024~ 2008-08-11 002472~     200      90      10      80 ACE ~\n 4 0D540BEBC45A~ 83454~ 2009-02-18 604290~      60      90      10      10 ACE ~\n 5 0D540BEBC45A~ 83874~ 2009-06-15 661050~      30      30       0      10 ACE ~\n 6 0E0A0107787B~ 83714~ 2008-06-30 661050~     100      90      70      60 ACE ~\n 7 109D67D11477~ 83064~ 2008-11-21 134110~      30      60       0      70 ACE ~\n 8 109D67D11477~ 83034~ 2009-07-03 666850~      30      30       0      10 ACE ~\n 9 10D75CDD5B4A~ 83804~ 2008-07-01 002472~      90      90       0      10 ACE ~\n10 1183CA4884F8~ 83934~ 2009-01-25 001850~      90      20       0     110 ACE ~\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT\n\n\nWe can now use collect() to retrieve the data as a local dataframe. As a dataframe, we can now use the AdhereR package to calculate the medication possession ratio (MPR) for members who filled a hypertension medication. MPR is a commonly used metric of medication adherence, measuring if beneficiaries have gaps between their prescription fills.\nWe can see the MPR for each member who filled one of the medications, and we can calculate the median and mean MPRs across these beneficiaries.\n\n# install.packages(\"AdhereR\")\nlibrary(AdhereR)\n\npde_hyp_df <- pde_hyp_rmt %>% \n  collect()\n\nhyp_adhere <- CMA4(\n  data = pde_hyp_df,\n  ID.colname = 'DESYNPUF_ID',\n  event.date.colname = 'SRVC_DT',\n  event.duration.colname = 'DAYS_SUPLY_NUM',\n  date.format = \"%Y-%m-%d\"\n)\n\nhyp_adhere_cma <- hyp_adhere$CMA %>% tibble()\n\nhyp_adhere_cma\n\n# A tibble: 134 x 2\n   DESYNPUF_ID         CMA\n   <chr>             <dbl>\n 1 03ADA78C0FEF79F4 0.0411\n 2 040A12AB5EAA444C 0.0411\n 3 043AAAE41C9A37B7 0.123 \n 4 0D540BEBC45ADCA7 0.164 \n 5 0E0A0107787B32AA 0.123 \n 6 109D67D114778917 0.123 \n 7 10D75CDD5B4AD3B0 0.123 \n 8 1183CA4884F8A0A8 0.0274\n 9 11C45CF0DDD03ACE 0.0822\n10 1226AD9944384B64 0.0411\n# ... with 124 more rows\n\nhyp_adhere_cma %>% \n  summarize(\n    median_mpr = median(CMA),\n    mean_mpr = mean(CMA)\n  )\n\n# A tibble: 1 x 2\n  median_mpr mean_mpr\n       <dbl>    <dbl>\n1     0.0411   0.0848"
  },
  {
    "objectID": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#data-limitations",
    "href": "posts/2022-02-12-introducing-the-claimsdb-package-for-teaching-and-learning/index.html#data-limitations",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Data Limitations",
    "text": "Data Limitations\nWhile data included in claimsdb is useful for many types of analyses, it does include a few notable limitations. - As mentioned earlier, the data is a small sample (500 beneficiaries) and is not intended to be representative of the Medicare population. In addition, the data is synthetic and should not be used for drawing inferences on the Medicare population. - Since the data is more than 10 years old, it doesn’t capture newer medications or procedures. It also includes procedure codes that have been retired or replaced. This is a challenge when applying external code lists that are much newer. - The diagnosis fields in the data use the International Classification of Diseases, Ninth Revision (ICD-9), but the United States converted to ICD-10 in 2015. If you are interesting in a mapping between ICD-9 and ICD-10, CMS has resources to consider. - The Medicare population is mostly Americans aged 65 and over, so the data will not have claims on certain specialties such as pediatrics or maternity care."
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#bar-chart-races",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#bar-chart-races",
    "title": "College Football Bar Chart Races",
    "section": "Bar Chart Races",
    "text": "Bar Chart Races\nThis project is inspired by the Athletic’s fantastic series by Matt Brown and Michael Weinreb that looks back on 150 years of college football by summarizing each decade. One of my favorite parts of the series is how it describes the rise and fall of various schools over time, such as the early dominance of the Ivy League prior to World War I, Oklahoma in the 1950s, and Miami in the 1980s.\nThe series also got me to think about how all these trends in program success could be visualized, and a bar chart race seemed like the obvious option to try out. I heard about bar chart races from John Burn-Murdoch’s interview on the PolicyViz podcast where he discussed his innovative bar chart race of city populations dating back to 1500 CE. Burn-Murdoch’s chart was created in D3, but I’ve been looking to get some experience with the gganimate R package."
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#data-source-and-wrangling",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#data-source-and-wrangling",
    "title": "College Football Bar Chart Races",
    "section": "Data source and wrangling",
    "text": "Data source and wrangling\nIn order to show win totals for college football programs over time, I needed team record data for all teams for more than a century. Fortunately, Sports Reference kindly provides detailed data on each season for all teams that played major college football. I patiently scraped the win-loss records of each college football program from their team pages.\nA quick note on “major schools:” Sports Reference has a somewhat subjective definition of major schools that they adopted from James Howell that looks at whether a school played 50% or more of its games against football bowl subdivision (FBS) level opponents during a season. Some schools gain or lose major status over time. For example, Yale hasn’t been a major school in football since 1981 by this definition, even though they still play football today. For this project, I only included seasons of major schools.\nWith that out of the way, let’s load the R packages.\n\nlibrary(tidyverse)\nlibrary(zoo)\nlibrary(RcppRoll)\nlibrary(gganimate)\nlibrary(shadowtext)\n\nNow let’s look at the variables in the data from Sports Reference.\n\nglimpse(team_record_raw)\n\nRows: 13,616\nColumns: 17\n$ school   <chr> \"Air Force\", \"Air Force\", \"Air Force\", \"Air Force\", \"Air Forc~\n$ year     <chr> \"2019\", \"2018\", \"2017\", \"2016\", \"2015\", \"2014\", \"2013\", \"2012~\n$ conf     <chr> \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\"~\n$ w        <chr> \"\", \"5\", \"5\", \"10\", \"8\", \"10\", \"2\", \"6\", \"7\", \"9\", \"8\", \"8\", ~\n$ l        <chr> \"\", \"7\", \"7\", \"3\", \"6\", \"3\", \"10\", \"7\", \"6\", \"4\", \"5\", \"5\", \"~\n$ t        <chr> \"\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0~\n$ pct      <chr> \"\", \".417\", \".417\", \".769\", \".571\", \".769\", \".167\", \".462\", \"~\n$ srs      <chr> \"\", \"-1.72\", \"-4.77\", \"4.09\", \"0.36\", \"2.20\", \"-14.88\", \"-7.3~\n$ sos      <chr> \"\", \"-3.80\", \"-1.02\", \"-4.53\", \"-3.14\", \"-4.88\", \"-3.22\", \"-6~\n$ ap_pre   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"~\n$ ap_high  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"23\", \"\", \"\", \"\", \"\", \"\",~\n$ ap_post  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"~\n$ coach_es <chr> \"Troy Calhoun (0-0)\", \"Troy Calhoun (5-7)\", \"Troy Calhoun (5-~\n$ bowl     <chr> \"\", \"\", \"\", \"Arizona Bowl-W\", \"Armed Forces Bowl-L\", \"Famous ~\n$ notes    <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"~\n$ from     <chr> \"1957\", \"1957\", \"1957\", \"1957\", \"1957\", \"1957\", \"1957\", \"1957~\n$ to       <chr> \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019~\n\n\nThe primary variables I used for this project are school, year, conf (conference membership), w (wins), l (losses), and notes. Within the notes variable is information about whether the school had its record adjusted by the NCAA. For example, USC had to forgo all 12 of its wins from the 2005 season. To line up with the official NCAA records, I parsed the notes field and adjusted team record data accordingly.\nSome schools had gaps in their data for when they did not play as a major school or didn’t play at all, so I used tidyr::complete and zoo::na.locf to fill in those years with records of 0 wins and 0 losses.\nFinally, I used RcppRoll::roll_sumr to generate 10-year rolling win and loss totals, along with cumulative win and loss totals.\n\nteam_record_prep <- team_record_raw %>%\n  mutate_at(vars(year, w, l, t, pct, srs, sos, ap_pre, ap_high, ap_post),\n            list(~ as.numeric(.))) %>%\n  mutate(adjustment = ifelse(str_detect(notes, \"^(record adjusted to )\"), str_squish((\n    substr(notes, 20, 25)\n  )), NA)) %>%\n  separate(\n    col = adjustment,\n    sep = \"\\\\-\",\n    into = c(\"w_adj\", \"l_adj\", \"t_adj\")\n  ) %>%\n  #record if adjusted by NCAA\n  mutate_at(vars(w_adj, l_adj, t_adj), list(~ as.numeric(.))) %>%\n  mutate(\n    w = ifelse(is.na(w_adj), w, w_adj),\n    l = ifelse(is.na(l_adj), l, l_adj),\n    t = ifelse(is.na(t_adj), t, t_adj)\n  ) %>%\n  filter(year <= 2018) %>%\n  complete(year, school) %>% #provide all school/year combinations to account for years that a school takes off\n  group_by(school) %>%\n  arrange(desc(from)) %>%\n  mutate_at(vars(to, from), list(~ na.locf(.))) %>%\n  ungroup() %>%\n  filter(year >= from) %>% #remove school/year combinations before the school began playing\n  mutate(played_season = ifelse(is.na(w), 0, 1)) %>%\n  mutate_at(vars(w, l, t), list(~ ifelse(is.na(.), 0, .))) %>% #assign 0-0-0 for seasons not played\n  arrange(school, year) %>%\n  group_by(school) %>%\n  mutate(\n    cumulative_wins = cumsum(w),\n    cumulative_losses = cumsum(l),\n    rolling_wins_10yr =  roll_sumr(w, n = 10),\n    rolling_losses_10yr = roll_sumr(l, n = 10),\n    conf = na.locf(conf)\n  ) %>%\n  ungroup() %>%\n  mutate_at(vars(rolling_wins_10yr, rolling_losses_10yr), list(~ ifelse(is.na(.), 0, .)))\n\nIn the last step of data preparation, I ranked each school on their rolling 10-year and cumulative totals for wins and losses. Since I am only going to show the “race” among the leaders for each year, I chose to keep only the top 10 schools for each measure. I also trimmed Washington & Jefferson’s name down a bit. Apologies to the Presidents!\n\nteam_record_final <- team_record_prep %>%\n  group_by(year) %>%\n  mutate(\n    rank_cm_wins = rank(desc(cumulative_wins), ties.method = \"first\"),\n    rank_cm_losses = rank(desc(cumulative_losses), ties.method = \"first\"),\n    rank_r10_wins = rank(desc(rolling_wins_10yr), ties.method = \"first\"),\n    rank_r10_losses = rank(desc(rolling_losses_10yr), ties.method = \"first\"),\n    school = ifelse(school == \"Washington & Jefferson\", \"Wash & Jefferson\", school)\n  ) %>%\n  ungroup() %>%\n  filter_at(\n    vars(rank_cm_wins, rank_cm_losses, rank_r10_wins, rank_r10_losses),\n    any_vars(. %in% 1:10)\n  ) %>%\n  select(\n    year,\n    school,\n    conf,\n    cumulative_wins,\n    cumulative_losses,\n    rolling_wins_10yr,\n    rolling_losses_10yr,\n    rank_cm_wins,\n    rank_cm_losses,\n    rank_r10_wins,\n    rank_r10_losses\n  ) %>% \n  arrange(school, year)\n\nHere’s how the final dataset looks:\n\nglimpse(team_record_final)\n\nRows: 4,274\nColumns: 11\n$ year                <dbl> 2013, 2014, 2016, 2017, 2018, 1924, 1925, 1926, 19~\n$ school              <chr> \"Akron\", \"Akron\", \"Akron\", \"Akron\", \"Akron\", \"Alab~\n$ conf                <chr> \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"Southern\", \"So~\n$ cumulative_wins     <dbl> 121, 126, 139, 146, 150, 127, 137, 146, 151, 157, ~\n$ cumulative_losses   <dbl> 196, 203, 215, 222, 230, 54, 54, 54, 58, 61, 64, 6~\n$ rolling_wins_10yr   <dbl> 38, 37, 38, 41, 40, 61, 65, 68, 68, 74, 72, 72, 76~\n$ rolling_losses_10yr <dbl> 82, 84, 83, 82, 83, 19, 17, 14, 16, 19, 21, 20, 17~\n$ rank_cm_wins        <int> 148, 147, 143, 142, 143, 31, 29, 25, 27, 27, 28, 2~\n$ rank_cm_losses      <int> 129, 126, 127, 127, 125, 61, 63, 66, 63, 64, 61, 6~\n$ rank_r10_wins       <int> 106, 108, 111, 109, 112, 8, 7, 4, 4, 3, 3, 3, 3, 3~\n$ rank_r10_losses     <int> 9, 9, 8, 9, 7, 69, 82, 93, 90, 85, 75, 79, 91, 91,~"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#setting-up-the-animation-theme",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#setting-up-the-animation-theme",
    "title": "College Football Bar Chart Races",
    "section": "Setting up the animation theme",
    "text": "Setting up the animation theme\nTo make the graphic look a little bit like a football field, I set the background to be “Astroturf Green” and much of the text and lines to be white. Emily Kuehler’s post was a huge help with setting up a theme and getting gganimate to work properly. Go check out her post for some awesome Grand Slam tennis and NBA scoring bar chart races!\n\nmy_background <- '#196F0C' #Astroturf Green\nmy_theme <- theme(\n  rect = element_rect(fill = my_background),\n  plot.background = element_rect(fill = my_background, color = NA),\n  panel.background = element_rect(fill = my_background, color = NA),\n  panel.border = element_blank(),\n  plot.title = element_text(face = 'bold', size = 20, color = 'white'),\n  plot.subtitle = element_text(size = 14, color = 'white'),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.major.x = element_line(color = 'white'),\n  panel.grid.minor.x = element_line(color = 'white'),\n  legend.position = 'none',\n  plot.caption = element_text(size = 10, color = 'white'),\n  axis.ticks = element_blank(),\n  axis.text.y =  element_blank(),\n  axis.text = element_text(color = 'white')\n)\n\ntheme_set(theme_light() + my_theme)"
  },
  {
    "objectID": "posts/2019-07-17-college-football-bar-chart-races/index.html#creating-some-plots",
    "href": "posts/2019-07-17-college-football-bar-chart-races/index.html#creating-some-plots",
    "title": "College Football Bar Chart Races",
    "section": "Creating some plots!",
    "text": "Creating some plots!\nIn each of the plots, I used the shadowtext::geom_shadowtext function to add a black drop shadow behind the year to increase the contrast against the axis lines in the background. I also had to do a lot of testing with nudge_y to get the school labels in a position that worked both for schools with long names and those with short ones.\n\nRolling 10-year winners\n\nrolling_wins_chart <- team_record_final %>%\n  filter(rank_r10_wins %in% 1:10 & rolling_wins_10yr > 0) %>%\n  ggplot(aes(rank_r10_wins * -1, group = school)) +\n  geom_tile(aes(\n    y = rolling_wins_10yr / 2,\n    height = rolling_wins_10yr,\n    width = 0.9,\n    fill = conf\n  ),\n  alpha = 0.9) +\n  geom_text(\n    aes(y = rolling_wins_10yr, label = school),\n    nudge_y = -20,\n    nudge_x = .2,\n    size = 4\n  ) +\n  geom_text(\n    aes(y = rolling_wins_10yr, label = conf),\n    nudge_y = -20,\n    nudge_x = -.2,\n    size = 2.5\n  ) +\n  geom_text(aes(y = rolling_wins_10yr, label = as.character(rolling_wins_10yr)), nudge_y = 5) +\n  geom_shadowtext(aes(\n    x = -10,\n    y = 118,\n    label = paste0(year)\n  ),\n  size = 8,\n  color = 'white') +\n  coord_cartesian(clip = \"off\", expand = FALSE) +\n  coord_flip() +\n  labs(\n    title = 'Most College Football Wins',\n    subtitle = 'Ten Year Rolling Total of Major Program Games',\n    caption = 'bar colors represent conferences\\ndata source: Sports Reference | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nThere is a lot of noise from year to year among the top 10 by rolling win total (as you would expect!), but the plot really does show the rise of certain programs (Notre Dame, Oklahoma, Miami, Alabama). Also, it’s fascinating to see the variation over time in win totals necessary to be at the top.\nI’m also pleased with how the conference membership is displayed and how it clearly shows when a school changes conferences, like when Nebraska moved from the Big 8 to the Big 12 in 1990s.\n\n\n\n… and rolling 10-year losers\n\nrolling_losses_chart <- team_record_final %>%\n  filter(rank_r10_losses %in% 1:10 & rolling_losses_10yr > 0) %>%\n  ggplot(aes(rank_r10_losses * -1, group = school)) +\n  geom_tile(\n    aes(\n      y = rolling_losses_10yr / 2,\n      height = rolling_losses_10yr,\n      width = 0.9,\n      fill = conf\n    ),\n    alpha = 0.9\n  ) +\n  geom_text(\n    aes(y = rolling_losses_10yr, label = school),\n    nudge_y = -15,\n    nudge_x = .2,\n    size = 4\n  ) +\n  geom_text(\n    aes(y = rolling_losses_10yr, label = conf),\n    nudge_y = -15,\n    nudge_x = -.2,\n    size = 2.5\n  ) +\n  geom_text(aes(y = rolling_losses_10yr, label = as.character(rolling_losses_10yr)), nudge_y = 5) +\n  geom_shadowtext(aes(\n    x = -10,\n    y = 105,\n    label = paste0(year)\n  ),\n  size = 8,\n  color = 'white') +\n  coord_cartesian(clip = \"off\", expand = FALSE) +\n  coord_flip() +\n  labs(\n    title = 'Most College Football Losses',\n    subtitle = 'Ten Year Rolling Total of Major Program Games',\n    caption = 'bar colors represent conferences\\ndata source: Sports Reference | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nK-State fans might want to look away.\n\n\n\nAll time winners\n\nalltime_wins_chart <- team_record_final %>%\n  filter(rank_cm_wins %in% 1:10 & cumulative_wins > 0) %>%\n  ggplot(aes(rank_cm_wins * -1, group = school)) +\n  geom_tile(aes(\n    y = cumulative_wins / 2,\n    height = cumulative_wins,\n    width = 0.9,\n    fill = conf\n  ),\n  alpha = 0.9) +\n  geom_text(\n    aes(y = cumulative_wins, label = school),\n    nudge_y = -90,\n    nudge_x = .2,\n    size = 4\n  ) +\n  geom_text(\n    aes(y = cumulative_wins, label = conf),\n    nudge_y = -90,\n    nudge_x = -.2,\n    size = 2.5\n  ) +\n  geom_text(aes(y = cumulative_wins, label = as.character(cumulative_wins)), nudge_y = 25) +\n  geom_shadowtext(aes(\n    x = -10,\n    y = 925,\n    label = paste0(year)\n  ),\n  size = 8,\n  color = 'white') +\n  coord_cartesian(clip = \"off\", expand = FALSE) +\n  coord_flip() +\n  labs(\n    title = 'Most College Football Losses',\n    subtitle = 'Cumulative Total of Major Program Games',\n    caption = 'bar colors represent conferences\\ndata source: Sports Reference | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nTo wrap this up, I also generated a plot of cumulative wins. This plot clearly shows how much of a lead the Ivy League developed from their early start and how they all dropped out of the top 10 once they stopped playing major football in 1981."
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#visualizing-marathon-races",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#visualizing-marathon-races",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Visualizing marathon races",
    "text": "Visualizing marathon races\nWith marathons and other large gatherings canceled or delayed by the COVID-19 pandemic, I’ve been feeling extra nostalgic for past running experiences in some of my favorite places. At the top of the list is the Big Sur Marathon that runs along California State Route 1 from Big Sur Station to Carmel. The course starts near the southern edge of the Redwood forest, rises to the top of Hurricane Point overlooking the Pacific, crosses iconic Bixby Creek Bridge, and passes over numerous hills and turns before reaching the finish. Big Sur is a challenging spring marathon, and while I finished in just over 4 hours in 2019, it was definitely the most difficult race I’ve run. Having a live piano performance at Bixby helped though.\n\nLike other runners, I tracked my progress with GPS, using the RunKeeper app on my phone. This project will use the GPS location data from RunKeeper and visualize the course using the rayshader R package, which can provide a 3-dimensional view of a landscape or other surface. Tyler Morgan-Wall recently posted an excellent guide for combining elevation data with satellite imagery to create 3D renderings of landscapes. Examples from Sebastian Engel-Wolf, Simon Coulombe, and Francois Keck of using animation with rayshader also provided plenty of guidance and inspiration."
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#data-sources-and-prep",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#data-sources-and-prep",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Data sources and prep",
    "text": "Data sources and prep\nThis project will need the GPS data from RunKeeper, satellite imagery data, and elevation data. Morgan-Wall’s post includes the step-by-step process for downloading the SRTM elevation data and imagery data from USGS. I won’t repeat those steps here, but I’ve pre-downloaded the data for the Big Sur region of California.\nLet’s get to it and load the R packages.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rayshader)\nlibrary(raster)\nlibrary(sf)\nlibrary(mapview)\nlibrary(magick)\n\nFirst, let’s load the elevation data. I downloaded data for two bordering areas, so I’ll merge them together.\n\nelevation1 <- raster::raster(file.path(path_to_data, \"N36W122.hgt\"))\nelevation2 <- raster::raster(file.path(path_to_data, \"N36W123.hgt\"))\n\nbigsur_elevation <- raster::merge(elevation1,elevation2)\nplot(bigsur_elevation)\n\n\n\n\nNext, let’s load the satellite data, which comes in three files that are separate layers for the image. After loading the layers, we’ll combine them and apply a color adjustment. It will need more refinement, but the Monterey peninsula is visible.\n\nbigsur_r <- raster::raster(file.path(path_to_data, \"LC08_L1TP_044035_20191026_20191030_01_T1_B4.TIF\"))\nbigsur_g <- raster::raster(file.path(path_to_data, \"LC08_L1TP_044035_20191026_20191030_01_T1_B3.TIF\"))\nbigsur_b <- raster::raster(file.path(path_to_data, \"LC08_L1TP_044035_20191026_20191030_01_T1_B2.TIF\"))\n\nbigsur_rbg_corrected <- sqrt(raster::stack(bigsur_r, bigsur_g, bigsur_b))\nraster::plotRGB(bigsur_rbg_corrected)\n\n\n\n\nFinally, let’s load the GPS data from RunKeeper, which comes as a GPX file. With a quick view using mapView, we can see the tracked points from the GPS file are in the right locations.\n\nbigsur_gpx <-\n  st_read(\n    file.path(path_to_data, \"RK_gpx_2019-04-28_0643.gpx\"),\n    layer = \"track_points\",\n    quiet = TRUE\n  ) %>%\n  dplyr::select(track_seg_point_id, ele, time, geometry)\n\nmapView(bigsur_gpx)\n\n\n\n\n\n\nNow that each of the three files are loaded, we need to get them all on the same projection system to get them to play nicely together. In this case, I applied the CRS from the imagery files to the other two files. We will also use the boundaries of the GPS file to crop the dimensions of the other files.\n\nbigsur_extent <- \n  bigsur_gpx %>%\n  st_transform(., raster::crs(bigsur_r)) %>%\n  as_Spatial() %>%\n  extent() + 1e4\n\nbigsur_extent\n\nclass      : Extent \nxmin       : 590082.3 \nxmax       : 614524.6 \nymin       : 4007102 \nymax       : 4049264 \n\nrasterOptions(chunksize=1e+06, maxmemory=1e+08)\nbigsur_elevation_utm <- raster::projectRaster(bigsur_elevation, crs = crs(bigsur_r), method = \"bilinear\")\ncrs(bigsur_elevation_utm)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=10 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"unknown\",\n    BASEGEOGCRS[\"unknown\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8901]]],\n    CONVERSION[\"UTM zone 10N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-123,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]],\n        ID[\"EPSG\",16010]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]] \n\nbigsur_rgb_cropped <- raster::crop(bigsur_rbg_corrected, bigsur_extent)\nelevation_cropped <- raster::crop(bigsur_elevation_utm, bigsur_extent)\n\nFollowing Morgan-Wall’s guide, we make a few more adjustments to the imagery data. We now have a nicely cropped image of the marathon course with good contrast and color balance.\n\nnames(bigsur_rgb_cropped) <- c(\"r\",\"g\",\"b\")\n\nbigsur_r_cropped <- rayshader::raster_to_matrix(bigsur_rgb_cropped$r)\nbigsur_g_cropped <- rayshader::raster_to_matrix(bigsur_rgb_cropped$g)\nbigsur_b_cropped <- rayshader::raster_to_matrix(bigsur_rgb_cropped$b)\n\nbigsurel_matrix <- rayshader::raster_to_matrix(elevation_cropped)\n\nbigsur_rgb_array <- array(0,dim=c(nrow(bigsur_r_cropped),ncol(bigsur_r_cropped),3))\n\nbigsur_rgb_array[,,1] <- bigsur_r_cropped/255 #Red layer\nbigsur_rgb_array[,,2] <- bigsur_g_cropped/255 #Blue layer\nbigsur_rgb_array[,,3] <- bigsur_b_cropped/255 #Green layer\n\nbigsur_rgb_array <- aperm(bigsur_rgb_array, c(2,1,3))\n\nbigsur_rgb_contrast <- scales::rescale(bigsur_rgb_array,to=c(0,1))\nplot_map(bigsur_rgb_contrast)"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#enhancing-and-plotting-the-gps-data",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#enhancing-and-plotting-the-gps-data",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Enhancing and plotting the GPS data",
    "text": "Enhancing and plotting the GPS data\nThe GPS data only includes elevation, time, and coordinate information right now…\n\nbigsur_gpx %>% \n  head(n = 5) %>% \n  as_tibble()\n\n# A tibble: 5 x 4\n  track_seg_point_id   ele time                            geometry\n               <int> <dbl> <dttm>                       <POINT [°]>\n1                  0  92.7 2019-04-28 08:45:22  (-121.781 36.24761)\n2                  1  92.7 2019-04-28 08:45:22  (-121.781 36.24761)\n3                  2  92.9 2019-04-28 08:45:33 (-121.7811 36.24765)\n4                  3  93   2019-04-28 08:45:49 (-121.7812 36.24767)\n5                  4  93.1 2019-04-28 08:45:53 (-121.7813 36.24771)\n\n\nBut we can enhance it to calculate distance between each of the points, along with cumulative distance and time elapsed.\n\nbigsur_gpx_enr <-\n  bigsur_gpx %>% \n  st_transform(., raster::crs(bigsur_r)) %>%\n  mutate(time = as_datetime(time),\n         elapsed_time = ifelse(dplyr::row_number() == 1, 0,\n                               time - lag(time)),\n         distance_from_prior = sf::st_distance(\n           geometry,\n           lag(geometry),\n           by_element = TRUE),\n         distance_from_prior = ifelse(is.na(distance_from_prior), 0, distance_from_prior),\n         cumul_time = cumsum(elapsed_time),\n         cumul_dist = cumsum(distance_from_prior)\n  ) %>% \n  mutate(row = row_number()) %>% \n  as_Spatial() %>%\n  data.frame() %>%\n  mutate(lon = coords.x1,\n         lat = coords.x2,\n         dep = ele + 25)\n\nWe can also plot the enhanced data to show Hurricane Point near the midpoint of the race (the orange vertical bar), all the times I stopped to take photos, and how my pace variation increased a lot in the last few miles as I started to wear out. The final elapsed time and distance values turned out a little larger than the actual race stats, since I started the GPS before the starting line and after the finish line.\n\nbigsur_gpx_enr %>% \n  transmute(cumul_miles = measurements::conv_unit(cumul_dist, \"m\", \"mi\"),\n         Elevation = measurements::conv_unit(ele, \"m\", \"ft\"),\n         Pace = elapsed_time / 60 / measurements::conv_unit(distance_from_prior, \"m\", \"mi\")) %>% \n  pivot_longer(Elevation:Pace, names_to = \"measure\", values_to = \"value\") %>% \n  ggplot(aes(cumul_miles, value, color = measure)) +\n    facet_wrap(~measure, ncol = 1, scale = \"free_y\") +\n  geom_line(size = 1.5) +\n  geom_vline(xintercept = 12.2, color = \"orange\", size = 4, alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Big Sur Marathon GPX\",\n       x = \"Distance (miles)\",\n       y = NULL,\n       color = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#creating-a-video-animation-of-the-marathon",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#creating-a-video-animation-of-the-marathon",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Creating a video animation of the marathon",
    "text": "Creating a video animation of the marathon\nTo put together a video animation of the course, we will need to generate a large number of frames that will later be rendered into a video clip. Each frame will have a specific shot angle and displayed progress on the course.\nFor generating all the values of the angles, I used the transition_values function from Will Bishop.\n\ntransition_values <- function(from, to, steps = 10, \n                              one_way = FALSE, type = \"cos\") {\n  if (!(type %in% c(\"cos\", \"lin\")))\n    stop(\"type must be one of: 'cos', 'lin'\")\n  \n  range <- c(from, to)\n  middle <- mean(range)\n  half_width <- diff(range)/2\n  \n  # define scaling vector starting at 1 (between 1 to -1)\n  if (type == \"cos\") {\n    scaling <- cos(seq(0, 2*pi / ifelse(one_way, 2, 1), length.out = steps))\n  } else if (type == \"lin\") {\n    if (one_way) {\n      xout <- seq(1, -1, length.out = steps)\n    } else {\n      xout <- c(seq(1, -1, length.out = floor(steps/2)), \n                seq(-1, 1, length.out = ceiling(steps/2)))\n    }\n    scaling <- approx(x = c(-1, 1), y = c(-1, 1), xout = xout)$y \n  }\n  \n  middle - half_width * scaling\n}\n\nThe video will be 24 seconds long at 60 frames per second, so 1,440 frames will need to be created. Here are the values used to create all the angles:\n\nn_frames <- 60 * 24\ngpx_rows <- seq(1, nrow(bigsur_gpx_enr), length.out = n_frames) %>% round()\nzscale <- 15\nthetavalues <- transition_values(from = 280, \n                                 to = 130,\n                                 steps = n_frames,        \n                                 one_way = TRUE, \n                                 type = \"lin\")\nphivalues <- transition_values(from = 70, \n                               to = 20, \n                               steps = n_frames,\n                               one_way = FALSE, \n                               type = \"cos\")\nzoomvalues <- transition_values(from = 0.8, \n                                to = 0.3, \n                                steps = n_frames,\n                                one_way = FALSE, \n                                type = \"cos\")\n\nAnother challenge is plotting the GPS coordinates as a 3D line that progresses over time. After trying out a few options, I generated a new add_3d_line function based on Vinay Udyawer’s KUD3D package. This function is essentially a wrapper that takes the coordinate data, calculates point distances, and creates a line using rgl::lines3d.\n\nadd_3d_line <-\n  function(ras,\n           det,\n           zscale,\n           lonlat = FALSE,\n           col = \"red\",\n           alpha = 0.8,\n           size = 2,\n           ...) {\n    e <- extent(ras)\n    cell_size_x <-\n      raster::pointDistance(c(e@xmin, e@ymin), c(e@xmax, e@ymin), lonlat = lonlat) / ncol(ras)\n    cell_size_y <-\n      raster::pointDistance(c(e@xmin, e@ymin), c(e@xmin, e@ymax), lonlat = lonlat) / nrow(ras)\n    distances_x <-\n      raster::pointDistance(c(e@xmin, e@ymin), cbind(det$lon, rep(e@ymin, nrow(det))), lonlat = lonlat) / cell_size_x\n    distances_y <-\n      raster::pointDistance(c(e@xmin, e@ymin), cbind(rep(e@xmin, nrow(det)), det$lat), lonlat = lonlat) / cell_size_y\n    \n      rgl::lines3d(\n        x = distances_y - (nrow(ras)/2),\n        y = det$dep / zscale,\n        z = abs(distances_x) - (ncol(ras)/2),\n        color = col,\n        alpha = alpha,\n        lwd = size,\n        ...\n      )\n  }\n\nTo bring all the elevation, imagery, and GPS data together, I needed to rotate the imagery and elevation data 90 degrees, so I created a new version of the imagery file and rotated the elevation data using the matlab::rot90 function. I then added labels for the start and finish points of the race by manually finding the coordinates with the best fit.\nWithin the loop function, I calculated the cumulative distance and elapsed time for each frame and adjusted the camera for the shot angles. Using the new add_3d_line function, I then overlay the 3D path of the race progress over the landscape. After each snapshot is captured, I then remove the path using rgl::rgl.pop to reduce the amount of memory consumed by the creating all the frames.\n\nimage_write(image_rotate(image_read(bigsur_rgb_contrast), 90), file.path(path_to_data, \"rotated.png\"))\noverlay_img <- png::readPNG(file.path(path_to_data, \"rotated.png\"), info = TRUE)\n\nplot_3d(overlay_img, \n        matlab::rot90(bigsurel_matrix, k = 1),\n        zscale = zscale, \n        fov = 0,\n        theta = thetavalues[1],\n        phi = phivalues[1],\n        windowsize = c(1000,800),\n        zoom = zoomvalues[1],\n        water=FALSE,          \n        background = \"#F2E1D0\", \n        shadowcolor = \"#523E2B\")\n\nrender_label(\n  elevation_cropped,\n  x = 155,\n  y = 785,\n  z = 1000,\n  zscale = zscale,\n  text = \"Start\",\n  freetype = FALSE,\n  textcolor = \"#EF9D3C\",\n  linecolor = \"#EF9D3C\",\n  dashed = TRUE\n)\nrender_label(\n  elevation_cropped,\n  x = 1200,\n  y = 300,\n  z = 1000,\n  zscale = zscale,\n  text = \"Finish\",\n  freetype = FALSE,\n  textcolor = \"#EF9D3C\",\n  linecolor = \"#EF9D3C\",\n  dashed = TRUE\n)\n\nfor (i in seq_len(n_frames)) {\n\n  gpx_frame <- bigsur_gpx_enr %>% filter(row <= gpx_rows[i])\n  \n  elapsed_time <-\n    gpx_frame %>% pull(cumul_time) %>% max(.) %>% hms::as_hms(.) %>% as.character(.)\n  elapsed_dist <-\n    gpx_frame %>% pull(cumul_dist) %>% max(.) %>% measurements::conv_unit(., \"m\", \"mi\") %>% round(., 1) %>% as.character(.)\n  \n  render_camera(theta = thetavalues[i],\n                phi = phivalues[i],\n                zoom = zoomvalues[i])\n  \n  gpx_frame %>%\n    add_3d_line(\n      ras = elevation_cropped,\n      det = .,\n      zscale = 15,\n      latlon = FALSE,\n      alpha = 0.6,\n      size = 4,\n      col = \"#FFCC00\"\n    )  \n  \n  Sys.sleep(2.5)\n  \n  render_snapshot(filename = file.path(path_to_frames, paste0(\"bigsur\", i, \".png\")), \n                  title_text = glue::glue(\"Big Sur Marathon | April 28, 2019 | time: {elapsed_time} | distance: {elapsed_dist} miles\"),\n                  title_bar_color = \"#022533\", title_color = \"white\", title_bar_alpha = 1)\n  \n  rgl::rgl.pop(type = \"shapes\")\n  gc()\n}\nrgl::rgl.close()\n\nFinally, after all the frames are captured, I rendered the PNG files using ffmpeg within a system() call to create a new mp4 file.\n\nsetwd(file.path(path_to_frames))\nsystem(\"ffmpeg -framerate 60 -i bigsur%d.png -pix_fmt yuv420p bigsur_marathon.mp4\")\n\nVideo"
  },
  {
    "objectID": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#wrapping-up",
    "href": "posts/2020-05-24-rayshading-the-big-sur-marathon/index.html#wrapping-up",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Wrapping up",
    "text": "Wrapping up\nOverall, this video clip turned out much better than I expected. The suite of R packages to work with geographic data is really impressive, and my learning curve was lowered thanks to a number of excellent step-by-step guides. Rayshader is also a great way to take your geographic data to the next level. While I’m still really new to rayshader, I look forward to using it in future projects."
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#data-prep",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#data-prep",
    "title": "FiveThirtyEight Riddler Challenge: Can You Find The Fish In State Names?",
    "section": "Data prep",
    "text": "Data prep\n\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(rvest)\n\nwords_raw <- \"https://norvig.com/ngrams/word.list\" %>% \n  GET() %>% \n  read_html() %>% \n  html_text()\n\nwords <- str_split(words_raw, \"\\n\")[[1]]\n\nstates <- state.name %>% \n  str_to_lower(.) %>% \n  str_remove_all(., \" \")"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#calculating-string-intersections-letters-in-common",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#calculating-string-intersections-letters-in-common",
    "title": "FiveThirtyEight Riddler Challenge: Can You Find The Fish In State Names?",
    "section": "Calculating string intersections (letters in common)",
    "text": "Calculating string intersections (letters in common)\n\nstate_word_intersect <- \n  tidyr::expand_grid(words, states) %>% \n  mutate(intersect = map2(words, states, ~Reduce(intersect, strsplit(c(.x, .y), split = \"\")))) %>% \n  mutate(intersect_length = map_dbl(intersect, length))"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#solution",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#solution",
    "title": "FiveThirtyEight Riddler Challenge: Can You Find The Fish In State Names?",
    "section": "Solution",
    "text": "Solution\n\nword_single_state <- \n  state_word_intersect %>% \n  filter(intersect_length == 0) %>% \n  group_by(words) %>% \n  mutate(state_matches = n()) %>% \n  ungroup() %>% \n  filter(state_matches == 1)\n\nword_single_state %>% \n  mutate(word_length = str_length(words),\n         max_length = max(word_length, na.rm = TRUE)) %>% \n  filter(word_length == max_length) %>% \n  select(words, word_length, states)\n\n# A tibble: 2 x 3\n  words                   word_length states     \n  <chr>                         <int> <chr>      \n1 counterproductivenesses          23 alabama    \n2 hydrochlorofluorocarbon          23 mississippi"
  },
  {
    "objectID": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#extra-credit",
    "href": "posts/2020-05-25-fivethirtyeight-riddler-challenge-can-you-find-the-fish-in-state-names/index.html#extra-credit",
    "title": "FiveThirtyEight Riddler Challenge: Can You Find The Fish In State Names?",
    "section": "Extra credit",
    "text": "Extra credit\n\nword_single_state %>% \n  group_by(states) %>% \n  mutate(word_count = n()) %>% \n  ungroup() %>% \n  mutate(max_word_count = max(word_count)) %>% \n  filter(word_count == max_word_count) %>% \n  distinct(states, word_count)\n\n# A tibble: 1 x 2\n  states word_count\n  <chr>       <int>\n1 ohio        11342"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#health-insurance-benchmark-data",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#health-insurance-benchmark-data",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Health insurance benchmark data",
    "text": "Health insurance benchmark data\nIn the United States, about half of the population has health insurance coverage through an employer. With employer-sponsored health insurance having such a large role in the American health care system, it’s important to understand trends and variation over time and how that affects affordability for employers, workers, and their family members.\nOne of the longest running surveys of employer-sponsored coverage is the Medical Expenditure Panel Survey - Insurance Component (MEPS-IC), administered by the federal Agency for Healthcare Research and Quality. Since the late 1990s, the MEPS-IC has asked employees and their employers about the health benefits offered to employees and what benefits employees enroll into, making it a valuable source of benchmark and trend data.\nA significant challenge with MEPS-IC data is that it is stored in separate web pages for each measure and year, with separate point estimate and standard error tables on each page. For this project, I am going to scrape the data from a subset of MEPS-IC tables using appropriate techniques and then visualize the trend data.\nLet’s get to it and load the R packages.\n\nlibrary(tidyverse)\nlibrary(rvest)\n#devtools::install_github(\"dmi3kno/polite\")\nlibrary(polite)\nlibrary(httr)\nlibrary(gganimate)\nlibrary(sf)\nlibrary(viridis)\nlibrary(gt)\n#devtools::install_github(\"hadley/emo\")\nlibrary(emo)"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#web-scraping-and-the-polite-package",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#web-scraping-and-the-polite-package",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Web scraping and the polite package",
    "text": "Web scraping and the polite package\nTo scrape the data from the MEPS-IC website, I’m going to use the polite package, developed by Dmytro Perepolkin and co-authors. This package uses three principles of polite webscraping: seeking permission, taking slowly and never asking twice. Specifically, it manages the http session, declares the user agent string and checks the site policies, and uses rate-limiting and response caching to minimize the impact on the webserver.\nApplying the three principles of polite scraping, polite creates a session with bow, requests a page with nod, and pulls the contents of the page with scrape. Here is a brief example:\n\nsession <- bow(\"https://meps.ahrq.gov/\", force = TRUE)\nsession\n\n<polite session> https://meps.ahrq.gov/\n    User-agent: polite R package\n    robots.txt: 13 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\npage <- \n    nod(bow = session, \n        path = paste0(\"data_stats/summ_tables/insr/national/\", \n                      \"series_1/2019/tia1.htm\"))\npage\n\n<polite session> https://meps.ahrq.gov/data_stats/summ_tables/insr/national/series_1/2019/tia1.htm\n    User-agent: polite R package\n    robots.txt: 13 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\nscrape_page <- \n  page %>% \n    scrape(verbose=TRUE)\nscrape_page\n\n{html_document}\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n[1] <head>\\n<meta name=\"description\" content=\"2019 Insurance Component Summar ...\n[2] <body>\\r\\n<center>\\r\\n<table cellspacing=\"0\" border=\"0\" cellpadding=\"3\" c ..."
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#creating-a-table-of-contents-of-the-insurance-data",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#creating-a-table-of-contents-of-the-insurance-data",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Creating a table of contents of the insurance data",
    "text": "Creating a table of contents of the insurance data\nThe first step for scraping the MEP-IC pages is to generate a table of content of all of the webpages. To do this, I use polite to go through each of the pages in the MEPS-IC table of contents and append the results of each page together, using a while loop. I also use the rvest package to pull specific nodes from each page, including the table and a link to the next page.\n\n## Table of contents\ntoc_df <- tibble()\nsession <- bow(\"https://meps.ahrq.gov/\", force = TRUE)\n\n## State and metro tables\npath <- paste0(\"quick_tables_results.jsp?component=2&subcomponent=2\", \n               \"&year=-1&tableSeries=-1&tableSubSeries=&searchText=&\",\n               \"searchMethod=1&Action=Search\")\nindex <- 0\n\nwhile(!is.na(path)){\n  # make it verbose\n  index <- 1 + index\n  message(\"Scraping state/metro page: \", path)\n  # nod and scrape\n  current_page <- \n    nod(bow = session, \n        path = paste0(\"data_stats/\", path)) %>% \n    scrape(verbose=TRUE)\n  # extract post titles\n  toc_df <- current_page %>%\n    html_nodes(xpath = paste0('//*[contains(concat( \" \", @class, \" \" ),',\n                              ' concat( \" \", \"description\", \" \" ))]',\n                              '//table')) %>%\n    html_table(fill = TRUE) %>%\n    purrr::pluck(5) %>% \n    as_tibble() %>% \n    janitor::clean_names() %>% \n    bind_rows(toc_df)\n  # see if a \"Next\" link is available\n  if (index == 1){\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder div a\") %>% \n      html_attr(\"href\")\n  } else {\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder a+ a\") %>% \n      html_attr(\"href\")\n  }\n} # end while loop\n\n## National tables\npath <- paste0(\"quick_tables_results.jsp?component=2&subcomponent=1&\",\n               \"year=-1&tableSeries=-1&tableSubSeries=&searchText=&\", \n               \"searchMethod=1&Action=Search\")\nindex <- 0\n\nwhile(!is.na(path)){\n  # update index\n  index <- 1 + index\n  message(\"Scraping national page: \", path)\n  # nod and scrape\n  current_page <- \n    nod(bow = session, \n        path = paste0(\"data_stats/\", path)) %>% \n    scrape(verbose=TRUE)\n  # extract TOC tables\n  toc_df <- current_page %>%\n    html_nodes(xpath = paste0('//*[contains(concat( \" \", @class, \" \" ),',\n                              'concat( \" \", \"description\", \" \" ))]//table')) %>%\n    html_table(fill = TRUE) %>%\n    purrr::pluck(5) %>% \n    as_tibble() %>% \n    janitor::clean_names() %>% \n    bind_rows(toc_df)\n  # see if a \"Next\" link is available\n  if (index == 1){\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder div a\") %>% \n      html_attr(\"href\")\n  } else {\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder a+ a\") %>% \n      html_attr(\"href\")\n  }\n  }# end while loop\n\nAfter scraping the table of contents data, I prep it further and generate the set of URL links I will need to pull the specific data tables I plan to use for this project.\n\ntoc <- \n  toc_df %>% \n  select(-update) %>% \n  separate(title,\n           into = c(\"title\", \"year\"),\n           sep = \": United States,\",\n           convert = TRUE) %>% \n  mutate(year = as.numeric(substr(str_squish(year), 1, 4)),\n         number = str_remove(table_no, \"Table \")) %>% \n  separate(number,\n           into = c(\"series\", \"number\"),\n           sep = '\\\\.',\n           convert = TRUE,\n           extra = \"merge\") %>% \n  filter(!is.na(year)) %>% \n  mutate(number = str_remove_all(number, \"[:punct:]\"),\n         number = str_to_lower(paste0(\"t\", series, number)),\n         series = as.numeric(as.roman(series)),\n         url = case_when(\n           series %in% c(1, 3, 4, 11) ~ paste0(\"https://meps.ahrq.gov/data_stats/\",\n                                               \"summ_tables/insr/national/series_\",\n                                               series, \"/\", \n                                               year, \"/\", \n                                               number, \".htm\"),\n           TRUE ~ paste0(\"https://meps.ahrq.gov/data_stats/\", \n                         \"summ_tables/insr/state/series_\", \n                         series, \"/\", \n                         year, \"/\", \n                         number, \".htm\")\n         )) %>% \n  filter(!is.na(year) & !is.na(series) & !is.na(number))"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#scraping-data-on-family-premium-costs",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#scraping-data-on-family-premium-costs",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Scraping data on family premium costs",
    "text": "Scraping data on family premium costs\nAs part of the project, I’m interested in the trends in the total premium costs of family health insurance through an employer (the sum of employer and employee contributions). To pull this data, I need to scrape the data from Table IID1, which has this data by state. First, I pull a vector of URLs from the table of contents, and then I use the politely wrapper function to apply polite principles with purrr::map and the httr::GET function.\n\n## Family premiums by state\nfamily_prem_url_vctr <- \n  toc %>% \n  filter(number == \"tiid1\" & year >= 2000) %>% \n  pull(url)\n\n## Politely GET pages\npolite_GET <- politely(GET, verbose = FALSE)\n\nfamily_prem_res <- map(family_prem_url_vctr, polite_GET)"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#tidying-the-scraped-data-with-the-tidyverse",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#tidying-the-scraped-data-with-the-tidyverse",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Tidying the scraped data with the tidyverse",
    "text": "Tidying the scraped data with the tidyverse\nThe result is a list object with a length of the number of pages scraped.\n\nclass(family_prem_res)\n\n[1] \"list\"\n\nlength(family_prem_res)\n\n[1] 19\n\n\nWith a little bit of rvest and purrr you can see a preview of the table in the webpage, but the results are very messy, with extra columns, blank rows, and offset values.\n\nfamily_prem_res %>% \n  purrr::pluck(1) %>% \n  read_html() %>% \n  html_nodes(xpath = '/html/body/table[1]') %>% \n  html_table(fill = TRUE, header = FALSE) %>%\n  as.data.frame() %>% \n  tibble()\n\n# A tibble: 59 x 18\n   X1    X2    X3    X4    X5    X6    X7    X8    X9    X10   X11   X12   X13  \n   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~\n 2 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 3 \"Div~ \"Tot~ \"\"    \"Les~ \"\"    \"10 ~ \"\"    \"25 ~ \"\"    \"100~ \"\"    \"100~ \"\"   \n 4 \"Div~ \"Tot~ \"\"    \"Les~ \"\"    \"10 ~ \"\"    \"25 ~ \"\"    \"100~ \"\"    \"100~ \"\"   \n 5 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 6 \"Uni~ \"6,7~ \"\"    \"6,9~ \"\"    \"6,8~ \"\"    \"6,6~ \"\"    \"6,6~ \"\"    \"6,8~ \"\"   \n 7 \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~\n 8 \"Mas~ \"7,3~ \"\"    \"8,4~ \"\"    \"8,2~ \"\"    \"7,4~ \"\"    \"7,0~ \"\"    \"7,0~ \"\"   \n 9 \"New~ \"7,5~ \"\"    \"8,2~ \"\"    \"7,3~ \"\"    \"7,7~ \"\"    \"6,9~ \"\"    \"7,6~ \"\"   \n10 \"Con~ \"7,2~ \"\"    \"7,5~ \"\"    \"7,6~ \"\"    \"7,1~ \"\"    \"7,9~ \"\"    \"7,0~ \"\"   \n# ... with 49 more rows, and 5 more variables: X14 <chr>, X15 <chr>, X16 <chr>,\n#   X17 <chr>, X18 <chr>\n\n\nTo fix the table and transform it to a shape that is better for visualization, I created a function that processes each scraped result and puts it into a tidier format (long, not wide). Some highlights of this function include:\n\nTesting different xpaths to find the data table in the page. Over time, the MEPS-IC tables changed in their format and styling, so the pages do not have a consistent structure.\nApply a hierarchy of group categories, groups, and segments.\nPull the point estimate and standard error data tables separately and combine them at the end.\nUse the janitor package to remove empty rows and columns and rename variables from row values.\nUse the zoo package to fill in blank values from above rows.\nConvert all percentages to decimal values.\n\n\nseriesScraper <- function(resp) {\n\n  if (resp %>%\n      purrr::pluck(1) %>% \n      read_html() %>% #test to find the right node\n      html_nodes(xpath = '/html/body/center/table[1]') %>%\n      html_table(fill = TRUE, header = FALSE) %>%\n      as.data.frame() %>%\n      ncol() > 0) {\n    node <- \"/html/body/center/table\"\n  } else if (resp %>%\n             purrr::pluck(1) %>% \n             read_html() %>% #test to find the right node\n             html_nodes(xpath = '/html/body/table[1]') %>%\n             html_table(fill = TRUE, header = FALSE) %>%\n             as.data.frame() %>%\n             ncol() > 0) {\n    node <- \"/html/body/table\"\n  } else {\n    node <- \"/html/body/div/table\"\n  }\n  \n  string_filter <- paste(\n    \"\\\\_\\\\_\",\n    \"Source:\",\n    \"Note:\",\n    \"Table I\",\n    \"Table II\",\n    \"Table 1\",\n    \"Table 2\",\n    \"\\\\* Figure\",\n    \"\\\\*\\\\* Definitions\",\n    \"Totals may\",\n    \"Dollar amounts\",\n    \"\\\\*Figure\",\n    \"Definitions\",\n    \"No estimate\",\n    \"These cell\",\n    \"States not\",\n    sep = \"|\"\n  )\n  \n  url <- \n    resp %>% \n    purrr::pluck(1) %>% \n    purrr::pluck(\"url\")\n  \n  if (is.null(url)){\n    url <- \n      resp %>% \n      purrr::pluck(1)\n  }\n  \n  pt_ests <-\n    resp %>%\n    purrr::pluck(1) %>% \n    read_html() %>%\n    html_nodes(xpath = paste0(node, \"[1]\")) %>%\n    html_table(fill = TRUE, header = FALSE) %>%\n    as.data.frame() %>%\n    na_if(\"\") %>%\n    distinct() %>%\n    filter(X1 != \"District of Columbia\" &\n             X2 != \"District of Columbia\") %>%\n    mutate_at(vars(-X1), ~ ifelse(!is.na(X1) &\n                                    is.na(.), X1, .)) %>%\n    mutate(X1 = ifelse(is.na(X1) & !is.na(X2), \"group\", X1)) %>%\n    janitor::remove_empty(\"rows\") %>%\n    mutate_all( ~ str_replace_all(., \"[\\r\\n]\" , \"\")) %>%\n    mutate_all( ~ str_squish(str_remove(., \"\\\\*\\\\*\"))) %>%\n    mutate_all( ~ replace(., str_detect(., \"These cell\"), NA)) %>%\n    filter(!str_detect(.[[1]], string_filter)) %>%\n    janitor::remove_empty(c(\"rows\", \"cols\")) %>%\n    distinct() %>%\n    janitor::row_to_names(\n      row_number = 1,\n      remove_row = TRUE,\n      remove_rows_above = TRUE\n    ) %>%\n    subset(select = which(!duplicated(names(.)))) %>%\n    subset(select = which(!is.na(names(.)))) %>%\n    mutate(\n      group_category = ifelse(.[[1]] == .[[2]], .[[1]], NA),\n      group_category = zoo::na.locf(group_category, na.rm = FALSE),\n      group_category = ifelse(is.na(group_category), \"United States\", group_category)\n    ) %>%\n    filter(!.[[1]] == .[[2]]) %>%\n    rename(group = 1) %>%\n    pivot_longer(\n      cols = -starts_with(\"group\"),\n      names_to = \"segment\",\n      values_to = \"pt_est\"\n    ) %>%\n    mutate_if(is.factor, as.character)\n  \n  std_errs <- \n    resp %>%\n    purrr::pluck(1) %>% \n    read_html() %>%\n    html_nodes(xpath = paste0(node, \"[2]\")) %>%\n    html_table(fill = TRUE, header = FALSE) %>%\n    as.data.frame() %>%\n    na_if(\"\") %>%\n    distinct() %>%\n    filter(X1 != \"District of Columbia\" &\n             X2 != \"District of Columbia\") %>%\n    mutate_at(vars(-X1), ~ ifelse(!is.na(X1) &\n                                    is.na(.), X1, .)) %>%\n    mutate(X1 = ifelse(is.na(X1) & !is.na(X2), \"group\", X1)) %>%\n    janitor::remove_empty(\"rows\") %>%\n    mutate_all( ~ str_replace_all(., \"[\\r\\n]\" , \"\")) %>%\n    mutate_all( ~ str_squish(str_remove(., \"\\\\*\\\\*\"))) %>%\n    mutate_all( ~ replace(., str_detect(., \"These cell\"), NA)) %>%\n    filter(!str_detect(.[[1]], string_filter)) %>%\n    janitor::remove_empty(c(\"rows\", \"cols\")) %>%\n    distinct() %>%\n    janitor::row_to_names(\n      row_number = 1,\n      remove_row = TRUE,\n      remove_rows_above = TRUE\n    ) %>%\n    subset(select = which(!duplicated(names(.)))) %>%\n    subset(select = which(!is.na(names(.)))) %>%\n    mutate(\n      group_category = ifelse(.[[1]] == .[[2]], .[[1]], NA),\n      group_category = zoo::na.locf(group_category, na.rm = FALSE),\n      group_category = ifelse(is.na(group_category), \"United States\", group_category)\n    ) %>%\n    filter(!.[[1]] == .[[2]]) %>%\n    rename(group = 1) %>%\n    pivot_longer(\n      cols = -starts_with(\"group\"),\n      names_to = \"segment\",\n      values_to = \"std_err\"\n    ) %>%\n    mutate_if(is.factor, as.character) %>% \n    select(-group_category)\n  \n  inner_join(pt_ests,\n             std_errs,\n             by = c(\"group\", \"group_category\", \"segment\")) %>%\n    mutate(url = url) %>%\n    mutate(numeric = case_when(\n      str_detect(pt_est, \"[A-Za-z]\") ~ 0,\n      str_detect(std_err, \"[A-Za-z]\") ~ 0,\n      TRUE ~ 1\n    )) %>%\n    filter(numeric == 1) %>%\n    mutate(\n      pt_est = str_trim(str_remove_all(pt_est, \"\\\\*\"), side = \"both\"),\n      std_err = str_trim(str_remove_all(std_err, \"\\\\*\"), side = \"both\"),\n      pt_est = if_else(\n        str_detect(pt_est, \"%\"),\n        as.numeric(gsub(\"%\", \"\", pt_est)) / 100,\n        as.numeric(gsub(\",\", \"\", pt_est))\n      ),\n      std_err = if_else(\n        str_detect(std_err, \"%\"),\n        as.numeric(gsub(\"%\", \"\", std_err)) / 100,\n        as.numeric(gsub(\",\", \"\", std_err))\n      )\n    ) %>%\n    select(-numeric) %>%\n    janitor::remove_empty(c(\"cols\", \"rows\")) %>% \n    mutate(\n      series = parse_number(str_extract(url, \"series_[:digit:]{1,2}\")),\n      year = parse_number(str_extract(url, \"[:digit:]{4}\")),\n      table = str_remove(str_extract(url, \"[:alnum:]{2,12}.htm\"), \".htm\")\n    ) %>% \n    filter(!is.na(pt_est) & !is.na(std_err)) %>%\n    mutate_if(is.character, ~ str_remove(., \"\\\\:$\"))\n}\n\nWith this new function, I used purrr::map_dfr to apply it to each scraped page in the list object and append all the resulting tidy data frames together.\n\nfamily_prem_df <- map_dfr(family_prem_res, seriesScraper)\n\n\nhead(family_prem_df)\n\n# A tibble: 6 x 9\n  group         group_category segment   pt_est std_err url   series  year table\n  <chr>         <chr>          <chr>      <dbl>   <dbl> <chr>  <dbl> <dbl> <chr>\n1 United States United States  Total      6772.    19.6 http~      2  2000 tiid1\n2 United States United States  Less tha~  6994.   149   http~      2  2000 tiid1\n3 United States United States  10 - 24 ~  6860.   143.  http~      2  2000 tiid1\n4 United States United States  25 - 99 ~  6628.    78.4 http~      2  2000 tiid1\n5 United States United States  100-999 ~  6606.    52.3 http~      2  2000 tiid1\n6 United States United States  1000 or ~  6817.    37.7 http~      2  2000 tiid1"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#visualizing-family-premium-costs",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#visualizing-family-premium-costs",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Visualizing family premium costs",
    "text": "Visualizing family premium costs\nFor the family premium cost data, I want to generate a map to show how costs increase across states over time. For the state map, I’m going with a hexmap of states from Carto. To make all the hexagons the same size, I transformed the data to a Mercator projection. I also created a crosswalk of state names and abbreviations to join the data together.\n\nhex_map <- st_read(file.path(path_to_data, \"us_states_hexgrid.geojson\"))\n\nReading layer `us_states_hexgrid' from data source \n  `C:\\Users\\Josh\\Dropbox\\Projects\\meps-ic\\polite\\us_states_hexgrid.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 51 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -137.9747 ymin: 26.39343 xmax: -69.90286 ymax: 55.3132\nGeodetic CRS:  WGS 84\n\nstate_abb <- \n  tibble(\n    name = state.name,\n    abb = state.abb\n  ) %>% \n  bind_rows(\n    tibble(\n      name = \"District of Columbia\",\n      abb = \"DC\"\n    )\n  )\n\n#transform to mercator projection\nhex_map_merc <- st_transform(hex_map, crs = 3785)\n\nhex_map_merc %>% \n  ggplot() +\n  geom_sf()\n\n\n\n\nThe next step is to prep the family premium data with the map. I used the tidyr::complete function to add missing values for any states that did not have data for a particular year. This is most common in the earlier years of the survey. I then used gganimate to created an animated gif showing the changes in premium costs across states.\n\nfamily_prem_map_df <- \n  family_prem_df %>% \n  filter(segment == \"Total\") %>% \n  select(year, group, pt_est, std_err) %>% \n  tidyr::complete(year, group) %>% \n  inner_join(\n    state_abb, by = c(\"group\" = \"name\")\n  ) %>% \n  inner_join(\n    hex_map_merc, by = c(\"abb\" = \"iso3166_2\")\n  ) %>% \n  select(\n    abb, \n    year, \n    pt_est,\n    std_err,\n    geometry\n  )\n\nfamily_prem_anim <-\n  family_prem_map_df %>% \n  ggplot() +\n  geom_sf(aes(fill = pt_est, geometry = geometry)) +\n  scale_fill_viridis_c(\n    option = \"magma\", \n    na.value = \"grey50\",\n    aesthetics = \"fill\",\n    labels = scales::dollar\n  ) +\n  geom_sf_label(aes(label = abb, geometry = geometry), \n                color = \"black\", \n                size = 3, \n                label.padding = unit(0.15, \"lines\")) +\n  geom_text(aes(x = -8400000, y = 3200000, label = paste0(year)),\n            size = 8,\n            color = 'black') +\n  theme_bw() + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0, size = 24),\n        plot.subtitle = element_text(size = 14),\n        panel.border = element_blank(),\n        panel.spacing = unit(3, \"lines\"),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        legend.key.width = unit(4, \"lines\"),\n        legend.position = c(0.50, 0.85),\n        legend.direction = \"horizontal\",\n        legend.title = element_blank()) +\n  labs(\n    title = 'Family Health Insurance Premium Costs',\n    subtitle = 'For enrolled workers with employer-based coverage',\n    caption = 'data source: MEPS-IC | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nYou can see which states have had higher and lower premium costs. Alaska 🤯."
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#scraping-data-on-insurance-coverage-by-industry",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#scraping-data-on-insurance-coverage-by-industry",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Scraping data on insurance coverage by industry",
    "text": "Scraping data on insurance coverage by industry\nThe next part of this project is to compare health insurance trends by industry. To do this, I scraped and tidied data from five sets of tables on the number of establishments, the number of workers, enrollment rates, and deductible levels for single (employee-only) health insurance.\n\n## Coverage Rates by Industry\nenrollment_url_vctr <- \n  toc %>% \n  filter(number %in% c(\"tia1\", \"tib1\", \"tib2\", \"tib2b\", \"tif2\"),\n         year >= 2000) %>% \n  pull(url)\n\nenrollment_res <- map(enrollment_url_vctr, polite_GET)\n\nenrollment_df <- map_dfr(enrollment_res, seriesScraper)"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#comparing-variation-in-deductibles-across-industries",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#comparing-variation-in-deductibles-across-industries",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Comparing variation in deductibles across industries",
    "text": "Comparing variation in deductibles across industries\nI then prepped the data a little further and calculated the number of enrolled employees for each industrial sector, using other values from the survey. I also plotted the trends in employee deductibles from 2000 to 2019 for each of the sectors.\n\nenrollment_ind_df <- \n  enrollment_df %>% \n  filter(group_category == \"Industry group\" & segment == \"Total\") %>% \n  mutate(group = case_when(\n    group == \"Fin. svs. and real est.\" ~ \"Fin. svs. and real estate\",\n    group == \"Other Services\" ~ \"Other services\",\n    TRUE ~ group)) %>% \n  filter(group != \"Unknown\") %>% \n  select(year, group, pt_est, table, year) %>% \n  pivot_wider(names_from = table, values_from = pt_est) %>% \n  mutate(enrolled_employees = tib1 * tib2 * tib2b) %>% \n  select(year,\n         industry = group,\n         establishments = tia1,\n         employees = tib1,\n         enrolled_employees,\n         average_employee_deductible = tif2) %>% \n  filter(!is.na(average_employee_deductible))\n\n\nenrollment_ind_df %>% \n  ggplot(aes(x = year, y = average_employee_deductible)) +\n  geom_line() +\n  geom_hline(yintercept = 0.5) +\n  facet_wrap(~industry)"
  },
  {
    "objectID": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#creating-a-table-to-display-industry-variation",
    "href": "posts/2020-11-28-politely-scraping-health-insurance-data/index.html#creating-a-table-to-display-industry-variation",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Creating a table to display industry variation",
    "text": "Creating a table to display industry variation\nInstead of creating another animated gif, I decided to use the gt package to generate a table that contained the information I wanted to display. Thomas Mock has an excellent guide for creating high quality tables with gt that I highly recommend. He includes a section on how to implement plots in your table, so you can have numbers and visuals in your table together. In this case, I am going to create a line graph or spark plot to display the deductible trends, by creating a function to generate the plots.\n\nplot_spark <- function(data){\n  data %>% \n    mutate(color = \"blue\") %>% \n    ggplot(aes(x = year, y = average_employee_deductible, color = color)) +\n    geom_line(size = 15) +\n    theme_void() +\n    scale_color_identity() +\n    theme(legend.position = \"none\")\n}\n\ndeductible_plots <- enrollment_ind_df %>% \n  select(industry, year, average_employee_deductible) %>% \n  nest(deductibles = c(year, average_employee_deductible)) %>% \n  mutate(plot = map(deductibles, plot_spark))\n\nFor some added fun, I’m adding emojis that represent each of the industrial sectors. Hadley Wickham’s emo package makes it very easy to add emoji values.\n\nemoji <- tribble(\n  ~industry, ~emoji,\n  \"Agric., fish., forest.\", emo::ji(\"man_farmer\"),\n  \"Mining and manufacturing\", emo::ji(\"woman_factory_worker\"),\n  \"Construction\", emo::ji(\"construction_worker_man\"),\n  \"Utilities and transp.\", emo::ji(\"bus\"),\n  \"Wholesale trade\", emo::ji(\"ship\"),\n  \"Fin. svs. and real estate\", emo::ji(\"dollar\"),\n  \"Retail trade\", emo::ji(\"department_store\"),\n  \"Professional services\", emo::ji(\"woman_health_worker\"),\n  \"Other services\", emo::ji(\"man_artist\")\n)\n\nUsing Mock’s guide for gt tables, I then setup my new table with the embedded plots using the text_transform function and the data frame with the plots I created earlier. With gt, you can also easily add spanners, format the values in your columns, and change the font styling and background colors to create the look you want.\n\nenrollment_spark <-\n  enrollment_ind_df %>% \n  filter(year == 2019) %>% \n  inner_join(emoji, by = \"industry\") %>% \n  select(emoji, \n         industry, \n         establishments, \n         employees, \n         enrolled_employees, \n         average_employee_deductible) %>% \n  rename_all(~str_to_title(str_replace_all(., \"\\\\_\", \" \"))) %>% \n  mutate(ggplot = NA) %>% \n  gt() %>% \n  text_transform(\n    locations = cells_body(vars(ggplot)),\n    fn = function(x){\n      map(deductible_plots$plot, \n          ggplot_image, \n          height = px(20), \n          aspect_ratio = 4)\n    }\n  ) %>% \n  cols_width(vars(ggplot) ~ px(100)) %>% \n  cols_label(\n    ggplot = \"Employee Deductibles:\\n2000-2019\",\n    Emoji = \"\",\n  ) %>% \n  fmt_number(3:5, decimals = 0) %>% \n  fmt_currency(6, decimals = 0) %>% \n  tab_spanner(\n    label = \"2019 Private Sector Charactertics, USA\",\n    columns = c(3:6)\n  ) %>% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(everything()),\n      cells_column_labels(everything())\n    )\n  ) %>%  \n  tab_options(\n    row_group.border.top.width = px(3),\n    row_group.border.top.color = \"black\",\n    row_group.border.bottom.color = \"black\",\n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    column_labels.border.bottom.color = \"black\",\n    column_labels.border.bottom.width = px(2),\n  ) %>% \n  tab_source_note(md(\"**Source**: MEPS-IC | **Table by**: @joshfangmeier\"))\n\nenrollment_spark"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About Me",
    "section": "",
    "text": "On this personal site, I share some side projects where I apply my own data learnings to interesting questions in health care, sports, the outdoors, or whatever strikes my fancy. Opinions shared here reflect my own views and not those of my employer."
  },
  {
    "objectID": "posts/2022-10-01-exploring-school-districts/index.html",
    "href": "posts/2022-10-01-exploring-school-districts/index.html",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "",
    "text": "Several weeks ago, I came across the tweet below from Kyle Walker who shared an example of how to pull a large amount of geospatial data with a few lines of code using the tigris package that he developed. The school district data that he showed in his example stood out for the complex boundaries of some states (i.e., Nebraska) compared to bordering states. In this post, I’m going to go through a brief exploration of American public school district boundaries using the tigris and sf packages, along with a tidyverse-style workflow, and identify how some districts are drawn much more differently than others."
  },
  {
    "objectID": "posts/2022-10-01-exploring-school-districts/index.html#load-school-district-data",
    "href": "posts/2022-10-01-exploring-school-districts/index.html#load-school-district-data",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Load school district data",
    "text": "Load school district data\nTo retrieve the school district boundary data, we will use the tigris package, which downloads and loads TIGER/Line shapefiles from the US Census bureau. The school_districts function in tigris downloads school district data, which the Census bureau collects from state education officials. For our analysis, we will use data on unified and elementary districts as of 2021, which combined gives us very good coverage.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(mapview)\nlibrary(rmapshaper)\nlibrary(nngeo)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(measurements)\n\n\n\n\n\n\n\nNot all states have elementary districts, so we use the safely workflow from the purrr package to keep states that do.\n\n\n\n\n\n\n\n\n\n\nsch_uni_sf <- \n  map_dfr(\n    c(state.abb, \"DC\"),\n    ~school_districts(\n      state = .x,\n      type = \"unified\",\n      year = 2021,\n      progress_bar = F\n    )\n  )\n\nsafe_school_districts <- safely(school_districts)\n\nsch_ele_sf <- \n  map(\n    c(state.abb, \"DC\"),\n    ~safe_school_districts(\n      state = .x,\n      type = \"elementary\",\n      year = 2021,\n      progress_bar = F\n    )\n  ) %>% \n  map(\"result\") %>% \n  compact() %>% \n  bind_rows()\n\nfips <- \n  tigris::fips_codes %>% \n  select(\n    STATEFP = state_code,\n    STUSPS = state,\n    STATE_NAME = state_name) %>% \n  distinct()\n\nsch_sf <- \n  bind_rows(\n    sch_uni_sf,\n    sch_ele_sf\n    ) %>% \n  inner_join(\n    fips,\n    by = \"STATEFP\"\n  )\n\n\nglimpse(sch_sf)\n## Rows: 12,805\n## Columns: 18\n## $ STATEFP    <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",~\n## $ UNSDLEA    <chr> \"00001\", \"00003\", \"00005\", \"00006\", \"00007\", \"00008\", \"0001~\n## $ GEOID      <chr> \"0100001\", \"0100003\", \"0100005\", \"0100006\", \"0100007\", \"010~\n## $ NAME       <chr> \"Fort Rucker School District\", \"Maxwell AFB School District~\n## $ LSAD       <chr> \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\",~\n## $ LOGRADE    <chr> \"KG\", \"KG\", \"KG\", \"PK\", \"KG\", \"PK\", \"KG\", \"PK\", \"KG\", \"KG\",~\n## $ HIGRADE    <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\",~\n## $ MTFCC      <chr> \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G542~\n## $ SDTYP      <chr> \"B\", \"B\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n## $ FUNCSTAT   <chr> \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",~\n## $ ALAND      <dbl> 232948114, 8697481, 69770754, 1265301295, 124178343, 786179~\n## $ AWATER     <dbl> 2801707, 372428, 258708, 103562192, 2505910, 339435, 601265~\n## $ INTPTLAT   <chr> \"+31.4097368\", \"+32.3809438\", \"+34.2631303\", \"+34.3821918\",~\n## $ INTPTLON   <chr> \"-085.7458071\", \"-086.3637490\", \"-086.2106600\", \"-086.30505~\n## $ ELSDLEA    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n## $ STUSPS     <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",~\n## $ STATE_NAME <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala~\n## $ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-85.86572 3..., MULTIPOLYGON (~\n\nWe can see that resulting data frame includes a geometry column of the ‘MULTIPOLYGON’ class. This means that each row (school district) may contains more than one land parcel.\nTo preview the school district boundaries, we can create a simplified map with ms_simplify from the rmapshaper package and view it with the mapView function.\n\nsch_simp <- \n  ms_simplify(\n    sch_sf, \n    sys = T, \n    sys_mem = 4)\n\n\nm <- mapview(sch_simp)\nm\n\n\nm@map %>% \n  leaflet::setView(-96, 39, zoom = 4)"
  },
  {
    "objectID": "posts/2022-10-01-exploring-school-districts/index.html#split-districts-into-separate-polygons",
    "href": "posts/2022-10-01-exploring-school-districts/index.html#split-districts-into-separate-polygons",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Split districts into separate polygons",
    "text": "Split districts into separate polygons\nSince I’m interested in understanding the boundaries of school districts and how many non-contiguous parcels some districts contains, we will split the districts into separate polygons. We can use the ms_explode function from the rmapshaper package to convert the data frame from a ‘MULTIPOLYGON’ geometry type to ‘POLYGON’. We will also add a row identifier to help keep track of each polygon.\n\nsch_poly_sf <- \n  ms_explode(\n    sch_sf, \n    sys = T, \n    sys_mem = 4) %>% \n  group_by(NAME, STUSPS) %>% \n  mutate(ID = row_number()) %>% \n  ungroup()\n\n\nglimpse(sch_poly_sf)\n## Rows: 23,458\n## Columns: 19\n## $ STATEFP    <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",~\n## $ UNSDLEA    <chr> \"00001\", \"00003\", \"00005\", \"00005\", \"00005\", \"00005\", \"0000~\n## $ GEOID      <chr> \"0100001\", \"0100003\", \"0100005\", \"0100005\", \"0100005\", \"010~\n## $ NAME       <chr> \"Fort Rucker School District\", \"Maxwell AFB School District~\n## $ LSAD       <chr> \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\",~\n## $ LOGRADE    <chr> \"KG\", \"KG\", \"KG\", \"KG\", \"KG\", \"KG\", \"PK\", \"PK\", \"PK\", \"PK\",~\n## $ HIGRADE    <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\",~\n## $ MTFCC      <chr> \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G542~\n## $ SDTYP      <chr> \"B\", \"B\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n## $ FUNCSTAT   <chr> \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",~\n## $ ALAND      <dbl> 232948114, 8697481, 69770754, 69770754, 69770754, 69770754,~\n## $ AWATER     <dbl> 2801707, 372428, 258708, 258708, 258708, 258708, 103562192,~\n## $ INTPTLAT   <chr> \"+31.4097368\", \"+32.3809438\", \"+34.2631303\", \"+34.2631303\",~\n## $ INTPTLON   <chr> \"-085.7458071\", \"-086.3637490\", \"-086.2106600\", \"-086.21066~\n## $ ELSDLEA    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n## $ STUSPS     <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",~\n## $ STATE_NAME <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala~\n## $ geometry   <POLYGON [°]> POLYGON ((-85.78931 31.4909..., POLYGON ((-86.37655~\n## $ ID         <int> 1, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13~\n\nWe can see above that the geometry column is now a ‘POLYGON’ geometry type, and we can now quickly calculate the median number of non-contiguous land parcels in each district. We can see below that Hawaii has one district for the whole state with 17 parcels (islands separated by water in this case). However, Nebraska (a very landlocked state) has a median number of 16 parcels per district, much higher than neighboring South Dakota with 3 parcels per district.\n\nsch_poly_count <- \n  sch_poly_sf %>% \n  st_drop_geometry() %>% \n  count(STUSPS, NAME)\n\nsch_poly_count %>% \n  group_by(STUSPS) %>% \n  summarize(\n    SCH_DISTRICTS = n(),\n    MEDIAN_PARCELS = median(n)) %>% \n  arrange(desc(MEDIAN_PARCELS)) %>% \n  head(10)\n## # A tibble: 10 x 3\n##    STUSPS SCH_DISTRICTS MEDIAN_PARCELS\n##    <chr>          <int>          <dbl>\n##  1 HI                 1             17\n##  2 NE               244             16\n##  3 SD               149              3\n##  4 AL               140              2\n##  5 MN               331              2\n##  6 AK                53              1\n##  7 AR               233              1\n##  8 AZ               202              1\n##  9 CA               855              1\n## 10 CO               178              1"
  },
  {
    "objectID": "posts/2022-10-01-exploring-school-districts/index.html#measure-the-compactness-of-each-district",
    "href": "posts/2022-10-01-exploring-school-districts/index.html#measure-the-compactness-of-each-district",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Measure the ‘compactness’ of each district",
    "text": "Measure the ‘compactness’ of each district\nAlong with calculating the number of land parcels, we can also measure the degree of ‘compactness’ that a school district has. Compactness measures are often used to determine the amount of gerrymandering of political districts. One measure is the convex hull score, which is the ratio of the area of the district to the area of the smallest convex polygon that could envelop the district.\nHere is a quick illustration of a convex hull drawn around a district (in red). Convex hull scores range from 0 to 1, with scores closer to 1 indicating more compactness.\n\nggplot() +\n  geom_sf(data = sch_sf %>% slice(1), fill = \"red\") +\n  geom_sf(data = sch_sf %>% slice(1) %>% st_convex_hull(), fill = NA, size = 1) +\n  theme_void()\n\n\n\n\nTo calculate convex hull scores for all the districts, we can write a function that calculates the score for a district after the areas of the district and convex hull are calculated.\n\nch_compactness <- function(geo_column) {\n  dist_area <- st_area(geo_column)\n  ch_area <- st_area(st_convex_hull(geo_column))\n  \n  ch_ratio <- as.numeric(dist_area / ch_area)\n  \n  return(ch_ratio)\n}\n\nThis ch_compactness function is then applied to all the districts using the map_dbl function from purrr, since we are expecting numeric values for our scores. We can see again that Hawaii, Nebraska, and South Dakota are the states with the least amount of district compactness, since their median scores are the lowest.\n\nsch_compactness <- \n  sch_sf %>% \n  mutate(CH_COMPACT = map_dbl(geometry, ch_compactness))\n\n\nsch_compactness %>% \n  st_drop_geometry() %>% \n  group_by(STUSPS) %>% \n  summarize(\n    SCH_DISTRICTS = n(),\n    MED_CH_COMPACT = median(CH_COMPACT)) %>% \n  arrange(MED_CH_COMPACT) %>% \n  head(10)\n## # A tibble: 10 x 3\n##    STUSPS SCH_DISTRICTS MED_CH_COMPACT\n##    <chr>          <int>          <dbl>\n##  1 HI                 1         0.0670\n##  2 NE               244         0.662 \n##  3 SD               149         0.754 \n##  4 AL               140         0.754 \n##  5 OK               510         0.779 \n##  6 MN               331         0.779 \n##  7 AR               235         0.779 \n##  8 OR               197         0.781 \n##  9 MO               516         0.784 \n## 10 ND               174         0.784"
  },
  {
    "objectID": "posts/2022-10-01-exploring-school-districts/index.html#identify-district-enclaves",
    "href": "posts/2022-10-01-exploring-school-districts/index.html#identify-district-enclaves",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Identify district enclaves",
    "text": "Identify district enclaves\nThe final metric that we will explore is how many enclaves each school district has. Wikipedia has a helpful explainer of what an enclave is, but in short we are looking for parcels of a district or whole district that are completely surrounded by another school district. This could be explained by some land being carved out of a district for a nearby district or a city district that is surrounded by its county district neighbor. Here is a quick plot showing the enclave that is Tuscaloosa City, Alabama (surrounded by Tuscaloosa County).\n\nggplot() +\n  geom_sf(\n    data = sch_sf %>% filter(GEOID %in% c(\"0103390\", \"0103360\")),\n    aes(fill = NAME)) +\n  scale_fill_viridis_d() +\n  labs(fill = NULL) +\n  theme_void() +\n  theme(legend.position = 'bottom')\n\n\n\n\nTo identify enclaves, we process each of the polygons to remove any holes or empty spaces within their boundaries using the st_remove_holes function from the nngeo package.\n\nsch_rem_holes_sf <- \n  sch_poly_sf %>% \n  select(\n    ENCLAVED_BY_NAME = NAME, \n    ENCLAVED_BY_STUSPS = STUSPS\n  ) %>% \n  st_remove_holes()\n\nThe next step is to join the original polygon data frame with the new ‘hole-free’ data frame to see which polygons fit within the boundaries of other polygons. To do this, we can use the st_join function from the sf package with the st_within join option that looks for matches where the polygon fits within the polygons of the joined data frame.\n\nsch_within_sf <- \n  sch_poly_sf %>%\n  st_join(\n    sch_rem_holes_sf,\n    join = st_within\n  ) %>% \n  filter(\n    NAME != ENCLAVED_BY_NAME,\n    !is.na(ENCLAVED_BY_NAME)\n  ) %>% \n  distinct(\n    NAME,\n    STUSPS,\n    ID,\n    .keep_all = T\n  )\n\nsch_within_sf %>% \n  st_drop_geometry() %>% \n  count(STUSPS, sort = T)\n## # A tibble: 41 x 2\n##    STUSPS     n\n##    <chr>  <int>\n##  1 NE      3227\n##  2 AL      1743\n##  3 SD       560\n##  4 MI       500\n##  5 WI       448\n##  6 GA       435\n##  7 MN       363\n##  8 OK       162\n##  9 ND       142\n## 10 TN       138\n## # ... with 31 more rows\n\nWe can see that Nebraska has the most district enclaves and nearly twice as many enclaves as second place Alabama. We can also plot the enclaves to see the state-by-state variation.\n\nstates_sf <- \n  states(cb = TRUE, progress_bar = F) %>% \n  filter(STUSPS %in% c(state.abb, \"DC\")) %>% \n  shift_geometry()\n\nggplot() +\n  geom_sf(\n    data = states_sf,\n    fill = \"grey10\") +\n  geom_sf(\n    data = sch_within_sf %>% shift_geometry(), \n    fill = \"orange\",\n    color = NA) + \n  theme_void()\n\n\n\n\nWe can also zoom in on Nebraska and its bordering states to see the contrast more clearly.\n\nstates_of_interest <- c(\"NE\", \"KS\", \"CO\", \"WY\", \"SD\", \"IA\", \"MO\")\n\nggplot() +\n  geom_sf(\n    data = states_sf %>% \n      filter(STUSPS %in% states_of_interest),\n    fill = \"grey10\") +\n  geom_sf(\n    data = sch_within_sf %>% \n      shift_geometry() %>% \n      filter(STUSPS %in% states_of_interest), \n    fill = \"orange\",\n    color = NA) + \n  theme_void()"
  },
  {
    "objectID": "posts/2022-10-01-exploring-school-districts/index.html#wrapping-up-with-summary-table",
    "href": "posts/2022-10-01-exploring-school-districts/index.html#wrapping-up-with-summary-table",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Wrapping up with summary table",
    "text": "Wrapping up with summary table\nFinally, we can identify the districts with the most enclaves and bring together the other metrics we calculated together into one table. First, we can create a new data frame that brings the enclave count, district area, enclave area, parcel count, and compactness scores together.\n\nsch_enclaves <- \n  sch_within_sf %>% \n  st_drop_geometry() %>% \n  count(NAME, STUSPS, sort = T)\n\nsch_enclaves_area <- \n  sch_within_sf %>% \n  group_by(NAME, STUSPS) %>% \n  summarize() %>% \n  ungroup() %>% \n  mutate(ENCLAVE_AREA = st_area(geometry)) %>% \n  st_drop_geometry()\n\nsch_tbl_df <- \n  sch_sf %>% \n  inner_join(\n    sch_enclaves %>% \n      rename(ENCLAVES = n),\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  inner_join(\n    sch_compactness %>% \n      st_drop_geometry() %>% \n      select(\n        NAME,\n        STUSPS,\n        CH_COMPACT\n      ),\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  inner_join(\n    sch_poly_count %>% \n      rename(PARTS = n),\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  inner_join(\n    sch_enclaves_area,\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  mutate(TOTAL_AREA = st_area(geometry))\n\nThen we can create a formatted table using the gt package and apply additional styling using the gtExtras package. This table includes the 20 districts with the most enclaves. We can embed plots of each district on the same row as other values, and we can also include bar charts to visualize the data even further.\n\nsch_plot <- \n  sch_tbl_df %>% \n  arrange(desc(ENCLAVES)) %>% \n  head(n = 20) %>% \n  mutate(\n    ENCLAVE_RATIO = as.numeric(ENCLAVE_AREA) / as.numeric(TOTAL_AREA),\n    TOTAL_AREA_MI = conv_unit(as.numeric(TOTAL_AREA), from = \"m2\", to = \"mi2\"),\n    PLOT = map(\n      geometry,\n      ~(ggplot(data = .x) +\n        geom_sf(fill = \"purple\") +\n        theme_void()) %>% \n        ggplot_image(height = px(125))\n      )) %>% \n  st_drop_geometry() %>% \n  select(\n    PLOT,\n    NAME,\n    STATE_NAME,\n    TOTAL_AREA_MI,\n    ENCLAVES,\n    ENCLAVE_RATIO,\n    CH_COMPACT\n  )\n\nsch_plot %>% \n  gt() %>% \n  cols_label(\n    PLOT = \"District Boundaries\",\n    NAME = \"District Name\",\n    STATE_NAME = \"State\",\n    TOTAL_AREA_MI = \"Total Area (square miles)\",\n    ENCLAVES = \"Number of Enclaves\",\n    ENCLAVE_RATIO = \"Share of Area in Enclaves\",\n    CH_COMPACT = \"District Compactness\"\n  ) %>%\n  fmt_markdown(c(PLOT)) %>%\n  fmt_number(\n    c(ENCLAVES),\n    decimals = 0\n  ) %>% \n  fmt_number(\n    c(TOTAL_AREA_MI),\n    decimals = 1\n  ) %>% \n  fmt_percent(\n    c(ENCLAVE_RATIO)\n  ) %>% \n  gt_theme_pff(\n    divider = TOTAL_AREA_MI\n  ) %>% \n  gt_plt_bar(\n    column = CH_COMPACT,\n    target = 1,\n    width = 45,\n    palette = c(\"purple\", \"black\")\n  ) %>% \n  tab_header(\n    title = \"School Districts with Most Enclaves\",\n    subtitle = \"Area, enclave, and compactness stats\"\n  ) %>% \n  gt_highlight_cols(columns = ENCLAVES, fill = \"#e4e8ec\")\n\n\n\n\n\n  \n    \n      School Districts with Most Enclaves\n    \n    \n      Area, enclave, and compactness stats\n    \n  \n  \n    \n      District Boundaries\n      District Name\n      State\n      Total Area (square miles)\n      Number of Enclaves\n      Share of Area in Enclaves\n      District Compactness\n    \n  \n  \n    \n\nJefferson County School District\nAlabama\n806.6\n403\n1.29%\n          \n    \n\nSylacauga City School District\nAlabama\n20.4\n169\n100.00%\n          \n    \n\nMadison County School District\nAlabama\n610.7\n157\n2.53%\n          \n    \n\nNorth Huron School District\nMichigan\n182.7\n124\n0.80%\n          \n    \n\nShelby County School District\nAlabama\n717.1\n97\n1.21%\n          \n    \n\nLee County School District\nAlabama\n487.9\n96\n1.11%\n          \n    \n\nHumboldt Table Rock Steinauer Public Schools\nNebraska\n384.0\n93\n5.10%\n          \n    \n\nLimestone County School District\nAlabama\n515.3\n92\n2.67%\n          \n    \n\nGordon County School District\nGeorgia\n342.7\n91\n0.52%\n          \n    \n\nWalker County School District\nAlabama\n775.8\n84\n0.19%\n          \n    \n\nTuscaloosa County School District\nAlabama\n1,280.5\n70\n0.49%\n          \n    \n\nNorfolk Public Schools\nNebraska\n144.9\n62\n7.40%\n          \n    \n\nEast Butler Public Schools\nNebraska\n289.8\n53\n3.25%\n          \n    \n\nColumbus City School District\nOhio\n137.2\n51\n3.30%\n          \n    \n\nHall County School District\nGeorgia\n390.2\n50\n0.49%\n          \n    \n\nLangford School District 45-5\nSouth Dakota\n421.3\n50\n5.07%\n          \n    \n\nBartow County School District\nGeorgia\n440.8\n49\n0.17%\n          \n    \n\nWhitfield County School District\nGeorgia\n269.9\n49\n0.21%\n          \n    \n\nCozad Community Schools\nNebraska\n240.6\n47\n4.81%\n          \n    \n\nMontgomery County School District\nAlabama\n763.4\n46\n0.06%"
  },
  {
    "objectID": "posts/2022-10-01_exploring-school-districts/index.html",
    "href": "posts/2022-10-01_exploring-school-districts/index.html",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "",
    "text": "Several weeks ago, I came across the tweet below from Kyle Walker who shared an example of how to pull a large amount of geospatial data with a few lines of code using the tigris package that he developed. The school district data that he showed in his example stood out for the complex boundaries of some states (i.e., Nebraska) compared to bordering states. In this post, I’m going to go through a brief exploration of American public school district boundaries using the tigris and sf packages, along with a tidyverse-style workflow, and identify how some districts are drawn much more differently than others."
  },
  {
    "objectID": "posts/2022-10-01_exploring-school-districts/index.html#load-school-district-data",
    "href": "posts/2022-10-01_exploring-school-districts/index.html#load-school-district-data",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Load school district data",
    "text": "Load school district data\nTo retrieve the school district boundary data, we will use the tigris package, which downloads and loads TIGER/Line shapefiles from the US Census bureau. The school_districts function in tigris downloads school district data, which the Census bureau collects from state education officials. For our analysis, we will use data on unified and elementary districts as of 2021, which combined gives us very good coverage.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(mapview)\nlibrary(rmapshaper)\nlibrary(nngeo)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(measurements)\n\n\n\n\n\n\n\nNot all states have elementary districts, so we use the safely workflow from the purrr package to keep states that do.\n\n\n\n\n\n\n\n\n\n\nsch_uni_sf <- \n  map_dfr(\n    c(state.abb, \"DC\"),\n    ~school_districts(\n      state = .x,\n      type = \"unified\",\n      year = 2021,\n      progress_bar = F\n    )\n  )\n\nsafe_school_districts <- safely(school_districts)\n\nsch_ele_sf <- \n  map(\n    c(state.abb, \"DC\"),\n    ~safe_school_districts(\n      state = .x,\n      type = \"elementary\",\n      year = 2021,\n      progress_bar = F\n    )\n  ) %>% \n  map(\"result\") %>% \n  compact() %>% \n  bind_rows()\n\nfips <- \n  tigris::fips_codes %>% \n  select(\n    STATEFP = state_code,\n    STUSPS = state,\n    STATE_NAME = state_name) %>% \n  distinct()\n\nsch_sf <- \n  bind_rows(\n    sch_uni_sf,\n    sch_ele_sf\n    ) %>% \n  inner_join(\n    fips,\n    by = \"STATEFP\"\n  )\n\n\nglimpse(sch_sf)\n## Rows: 12,805\n## Columns: 18\n## $ STATEFP    <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",~\n## $ UNSDLEA    <chr> \"00001\", \"00003\", \"00005\", \"00006\", \"00007\", \"00008\", \"0001~\n## $ GEOID      <chr> \"0100001\", \"0100003\", \"0100005\", \"0100006\", \"0100007\", \"010~\n## $ NAME       <chr> \"Fort Rucker School District\", \"Maxwell AFB School District~\n## $ LSAD       <chr> \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\",~\n## $ LOGRADE    <chr> \"KG\", \"KG\", \"KG\", \"PK\", \"KG\", \"PK\", \"KG\", \"PK\", \"KG\", \"KG\",~\n## $ HIGRADE    <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\",~\n## $ MTFCC      <chr> \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G542~\n## $ SDTYP      <chr> \"B\", \"B\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n## $ FUNCSTAT   <chr> \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",~\n## $ ALAND      <dbl> 232948114, 8697481, 69770754, 1265301295, 124178343, 786179~\n## $ AWATER     <dbl> 2801707, 372428, 258708, 103562192, 2505910, 339435, 601265~\n## $ INTPTLAT   <chr> \"+31.4097368\", \"+32.3809438\", \"+34.2631303\", \"+34.3821918\",~\n## $ INTPTLON   <chr> \"-085.7458071\", \"-086.3637490\", \"-086.2106600\", \"-086.30505~\n## $ ELSDLEA    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n## $ STUSPS     <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",~\n## $ STATE_NAME <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala~\n## $ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-85.86572 3..., MULTIPOLYGON (~\n\nWe can see that resulting data frame includes a geometry column of the ‘MULTIPOLYGON’ class. This means that each row (school district) may contains more than one land parcel.\nTo preview the school district boundaries, we can create a simplified map with ms_simplify from the rmapshaper package and view it with the mapView function.\n\nsch_simp <- \n  ms_simplify(\n    sch_sf, \n    sys = T, \n    sys_mem = 4)\n\n\nm <- mapview(sch_simp)\nm\n\n\nm@map %>% \n  leaflet::setView(-96, 39, zoom = 4)"
  },
  {
    "objectID": "posts/2022-10-01_exploring-school-districts/index.html#split-districts-into-separate-polygons",
    "href": "posts/2022-10-01_exploring-school-districts/index.html#split-districts-into-separate-polygons",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Split districts into separate polygons",
    "text": "Split districts into separate polygons\nSince I’m interested in understanding the boundaries of school districts and how many non-contiguous parcels some districts contains, we will split the districts into separate polygons. We can use the ms_explode function from the rmapshaper package to convert the data frame from a ‘MULTIPOLYGON’ geometry type to ‘POLYGON’. We will also add a row identifier to help keep track of each polygon.\n\nsch_poly_sf <- \n  ms_explode(\n    sch_sf, \n    sys = T, \n    sys_mem = 4) %>% \n  group_by(NAME, STUSPS) %>% \n  mutate(ID = row_number()) %>% \n  ungroup()\n\n\nglimpse(sch_poly_sf)\n## Rows: 23,458\n## Columns: 19\n## $ STATEFP    <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",~\n## $ UNSDLEA    <chr> \"00001\", \"00003\", \"00005\", \"00005\", \"00005\", \"00005\", \"0000~\n## $ GEOID      <chr> \"0100001\", \"0100003\", \"0100005\", \"0100005\", \"0100005\", \"010~\n## $ NAME       <chr> \"Fort Rucker School District\", \"Maxwell AFB School District~\n## $ LSAD       <chr> \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\", \"00\",~\n## $ LOGRADE    <chr> \"KG\", \"KG\", \"KG\", \"KG\", \"KG\", \"KG\", \"PK\", \"PK\", \"PK\", \"PK\",~\n## $ HIGRADE    <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\",~\n## $ MTFCC      <chr> \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G5420\", \"G542~\n## $ SDTYP      <chr> \"B\", \"B\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n## $ FUNCSTAT   <chr> \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",~\n## $ ALAND      <dbl> 232948114, 8697481, 69770754, 69770754, 69770754, 69770754,~\n## $ AWATER     <dbl> 2801707, 372428, 258708, 258708, 258708, 258708, 103562192,~\n## $ INTPTLAT   <chr> \"+31.4097368\", \"+32.3809438\", \"+34.2631303\", \"+34.2631303\",~\n## $ INTPTLON   <chr> \"-085.7458071\", \"-086.3637490\", \"-086.2106600\", \"-086.21066~\n## $ ELSDLEA    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n## $ STUSPS     <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",~\n## $ STATE_NAME <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala~\n## $ geometry   <POLYGON [°]> POLYGON ((-85.78931 31.4909..., POLYGON ((-86.37655~\n## $ ID         <int> 1, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13~\n\nWe can see above that the geometry column is now a ‘POLYGON’ geometry type, and we can now quickly calculate the median number of non-contiguous land parcels in each district. We can see below that Hawaii has one district for the whole state with 17 parcels (islands separated by water in this case). However, Nebraska (a very landlocked state) has a median number of 16 parcels per district, much higher than neighboring South Dakota with 3 parcels per district.\n\nsch_poly_count <- \n  sch_poly_sf %>% \n  st_drop_geometry() %>% \n  count(STUSPS, NAME)\n\nsch_poly_count %>% \n  group_by(STUSPS) %>% \n  summarize(\n    SCH_DISTRICTS = n(),\n    MEDIAN_PARCELS = median(n)) %>% \n  arrange(desc(MEDIAN_PARCELS)) %>% \n  head(10)\n## # A tibble: 10 x 3\n##    STUSPS SCH_DISTRICTS MEDIAN_PARCELS\n##    <chr>          <int>          <dbl>\n##  1 HI                 1             17\n##  2 NE               244             16\n##  3 SD               149              3\n##  4 AL               140              2\n##  5 MN               331              2\n##  6 AK                53              1\n##  7 AR               233              1\n##  8 AZ               202              1\n##  9 CA               855              1\n## 10 CO               178              1"
  },
  {
    "objectID": "posts/2022-10-01_exploring-school-districts/index.html#measure-the-compactness-of-each-district",
    "href": "posts/2022-10-01_exploring-school-districts/index.html#measure-the-compactness-of-each-district",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Measure the ‘compactness’ of each district",
    "text": "Measure the ‘compactness’ of each district\nAlong with calculating the number of land parcels, we can also measure the degree of ‘compactness’ that a school district has. Compactness measures are often used to determine the amount of gerrymandering of political districts. One measure is the convex hull score, which is the ratio of the area of the district to the area of the smallest convex polygon that could envelop the district.\nHere is a quick illustration of a convex hull drawn around a district (in red). Convex hull scores range from 0 to 1, with scores closer to 1 indicating more compactness.\n\nggplot() +\n  geom_sf(data = sch_sf %>% slice(1), fill = \"red\") +\n  geom_sf(data = sch_sf %>% slice(1) %>% st_convex_hull(), fill = NA, size = 1) +\n  theme_void()\n\n\n\n\nTo calculate convex hull scores for all the districts, we can write a function that calculates the score for a district after the areas of the district and convex hull are calculated.\n\nch_compactness <- function(geo_column) {\n  dist_area <- st_area(geo_column)\n  ch_area <- st_area(st_convex_hull(geo_column))\n  \n  ch_ratio <- as.numeric(dist_area / ch_area)\n  \n  return(ch_ratio)\n}\n\nThis ch_compactness function is then applied to all the districts using the map_dbl function from purrr, since we are expecting numeric values for our scores. We can see again that Hawaii, Nebraska, and South Dakota are the states with the least amount of district compactness, since their median scores are the lowest.\n\nsch_compactness <- \n  sch_sf %>% \n  mutate(CH_COMPACT = map_dbl(geometry, ch_compactness))\n\n\nsch_compactness %>% \n  st_drop_geometry() %>% \n  group_by(STUSPS) %>% \n  summarize(\n    SCH_DISTRICTS = n(),\n    MED_CH_COMPACT = median(CH_COMPACT)) %>% \n  arrange(MED_CH_COMPACT) %>% \n  head(10)\n## # A tibble: 10 x 3\n##    STUSPS SCH_DISTRICTS MED_CH_COMPACT\n##    <chr>          <int>          <dbl>\n##  1 HI                 1         0.0670\n##  2 NE               244         0.662 \n##  3 SD               149         0.754 \n##  4 AL               140         0.754 \n##  5 OK               510         0.779 \n##  6 MN               331         0.779 \n##  7 AR               235         0.779 \n##  8 OR               197         0.781 \n##  9 MO               516         0.784 \n## 10 ND               174         0.784"
  },
  {
    "objectID": "posts/2022-10-01_exploring-school-districts/index.html#identify-district-enclaves",
    "href": "posts/2022-10-01_exploring-school-districts/index.html#identify-district-enclaves",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Identify district enclaves",
    "text": "Identify district enclaves\nThe final metric that we will explore is how many enclaves each school district has. Wikipedia has a helpful explainer of what an enclave is, but in short we are looking for parcels of a district or whole district that are completely surrounded by another school district. This could be explained by some land being carved out of a district for a nearby district or a city district that is surrounded by its county district neighbor. Here is a quick plot showing the enclave that is Tuscaloosa City, Alabama (surrounded by Tuscaloosa County).\n\nggplot() +\n  geom_sf(\n    data = sch_sf %>% filter(GEOID %in% c(\"0103390\", \"0103360\")),\n    aes(fill = NAME)) +\n  scale_fill_viridis_d() +\n  labs(fill = NULL) +\n  theme_void() +\n  theme(legend.position = 'bottom')\n\n\n\n\nTo identify enclaves, we process each of the polygons to remove any holes or empty spaces within their boundaries using the st_remove_holes function from the nngeo package.\n\nsch_rem_holes_sf <- \n  sch_poly_sf %>% \n  select(\n    ENCLAVED_BY_NAME = NAME, \n    ENCLAVED_BY_STUSPS = STUSPS\n  ) %>% \n  st_remove_holes()\n\nThe next step is to join the original polygon data frame with the new ‘hole-free’ data frame to see which polygons fit within the boundaries of other polygons. To do this, we can use the st_join function from the sf package with the st_within join option that looks for matches where the polygon fits within the polygons of the joined data frame.\n\nsch_within_sf <- \n  sch_poly_sf %>%\n  st_join(\n    sch_rem_holes_sf,\n    join = st_within\n  ) %>% \n  filter(\n    NAME != ENCLAVED_BY_NAME,\n    !is.na(ENCLAVED_BY_NAME)\n  ) %>% \n  distinct(\n    NAME,\n    STUSPS,\n    ID,\n    .keep_all = T\n  )\n\nsch_within_sf %>% \n  st_drop_geometry() %>% \n  count(STUSPS, sort = T)\n## # A tibble: 41 x 2\n##    STUSPS     n\n##    <chr>  <int>\n##  1 NE      3227\n##  2 AL      1743\n##  3 SD       560\n##  4 MI       500\n##  5 WI       448\n##  6 GA       435\n##  7 MN       363\n##  8 OK       162\n##  9 ND       142\n## 10 TN       138\n## # ... with 31 more rows\n\nWe can see that Nebraska has the most district enclaves and nearly twice as many enclaves as second place Alabama. We can also plot the enclaves to see the state-by-state variation.\n\nstates_sf <- \n  states(cb = TRUE, progress_bar = F) %>% \n  filter(STUSPS %in% c(state.abb, \"DC\")) %>% \n  shift_geometry()\n\nggplot() +\n  geom_sf(\n    data = states_sf,\n    fill = \"grey10\") +\n  geom_sf(\n    data = sch_within_sf %>% shift_geometry(), \n    fill = \"orange\",\n    color = NA) + \n  labs(\n    title = \"School District Enclaves\"\n  ) +\n  theme_void()\n\n\n\n\nWe can also zoom in on Nebraska and its bordering states to see the contrast more clearly.\n\nstates_of_interest <- c(\"NE\", \"KS\", \"CO\", \"WY\", \"SD\", \"IA\", \"MO\")\n\nggplot() +\n  geom_sf(\n    data = states_sf %>% \n      filter(STUSPS %in% states_of_interest),\n    fill = \"grey10\") +\n  geom_sf(\n    data = sch_within_sf %>% \n      shift_geometry() %>% \n      filter(STUSPS %in% states_of_interest), \n    fill = \"orange\",\n    color = NA) + \n  labs(\n    title = paste0(\n      \"School District Enclaves in \",\n      glue::glue_collapse(states_of_interest, sep = \", \", last = \", and \")\n    )\n  ) +\n  theme_void()"
  },
  {
    "objectID": "posts/2022-10-01_exploring-school-districts/index.html#wrapping-up-with-summary-table",
    "href": "posts/2022-10-01_exploring-school-districts/index.html#wrapping-up-with-summary-table",
    "title": "Exploring the Oddities of Public School Districts",
    "section": "Wrapping up with summary table",
    "text": "Wrapping up with summary table\nFinally, we can identify the districts with the most enclaves and bring together the other metrics we calculated together into one table. First, we can create a new data frame that brings the enclave count, district area, enclave area, parcel count, and compactness scores together.\n\nsch_enclaves <- \n  sch_within_sf %>% \n  st_drop_geometry() %>% \n  count(NAME, STUSPS, sort = T)\n\nsch_enclaves_area <- \n  sch_within_sf %>% \n  group_by(NAME, STUSPS) %>% \n  summarize() %>% \n  ungroup() %>% \n  mutate(ENCLAVE_AREA = st_area(geometry)) %>% \n  st_drop_geometry()\n\nsch_tbl_df <- \n  sch_sf %>% \n  inner_join(\n    sch_enclaves %>% \n      rename(ENCLAVES = n),\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  inner_join(\n    sch_compactness %>% \n      st_drop_geometry() %>% \n      select(\n        NAME,\n        STUSPS,\n        CH_COMPACT\n      ),\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  inner_join(\n    sch_poly_count %>% \n      rename(PARTS = n),\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  inner_join(\n    sch_enclaves_area,\n    by = c(\"NAME\", \"STUSPS\")\n  ) %>% \n  mutate(TOTAL_AREA = st_area(geometry))\n\nThen we can create a formatted table using the gt package and apply additional styling using the gtExtras package. This table includes the 20 districts with the most enclaves. We can embed plots of each district on the same row as other values, and we can also include bar charts to visualize the data even further.\n\nsch_plot <- \n  sch_tbl_df %>% \n  arrange(desc(ENCLAVES)) %>% \n  head(n = 20) %>% \n  mutate(\n    ENCLAVE_RATIO = as.numeric(ENCLAVE_AREA) / as.numeric(TOTAL_AREA),\n    TOTAL_AREA_MI = conv_unit(as.numeric(TOTAL_AREA), from = \"m2\", to = \"mi2\"),\n    PLOT = map(\n      geometry,\n      ~(ggplot(data = .x) +\n        geom_sf(fill = \"purple\") +\n        theme_void()) %>% \n        ggplot_image(height = px(125))\n      )) %>% \n  st_drop_geometry() %>% \n  select(\n    PLOT,\n    NAME,\n    STATE_NAME,\n    TOTAL_AREA_MI,\n    ENCLAVES,\n    ENCLAVE_RATIO,\n    CH_COMPACT\n  )\n\nsch_plot %>% \n  gt() %>% \n  cols_label(\n    PLOT = \"District Boundaries\",\n    NAME = \"District Name\",\n    STATE_NAME = \"State\",\n    TOTAL_AREA_MI = \"Total Area (square miles)\",\n    ENCLAVES = \"Number of Enclaves\",\n    ENCLAVE_RATIO = \"Share of Area in Enclaves\",\n    CH_COMPACT = \"District Compactness\"\n  ) %>%\n  fmt_markdown(c(PLOT)) %>%\n  fmt_number(\n    c(ENCLAVES),\n    decimals = 0\n  ) %>% \n  fmt_number(\n    c(TOTAL_AREA_MI),\n    decimals = 1\n  ) %>% \n  fmt_percent(\n    c(ENCLAVE_RATIO)\n  ) %>% \n  gt_theme_pff(\n    divider = TOTAL_AREA_MI\n  ) %>% \n  gt_plt_bar(\n    column = CH_COMPACT,\n    target = 1,\n    width = 45,\n    palette = c(\"purple\", \"black\")\n  ) %>% \n  tab_header(\n    title = \"School Districts with Most Enclaves\",\n    subtitle = \"Area, enclave, and compactness stats\"\n  ) %>% \n  gt_highlight_cols(columns = ENCLAVES, fill = \"#e4e8ec\")\n\n\n\n\n\n  \n    \n      School Districts with Most Enclaves\n    \n    \n      Area, enclave, and compactness stats\n    \n  \n  \n    \n      District Boundaries\n      District Name\n      State\n      Total Area (square miles)\n      Number of Enclaves\n      Share of Area in Enclaves\n      District Compactness\n    \n  \n  \n    \n\nJefferson County School District\nAlabama\n806.6\n403\n1.29%\n          \n    \n\nSylacauga City School District\nAlabama\n20.4\n169\n100.00%\n          \n    \n\nMadison County School District\nAlabama\n610.7\n157\n2.53%\n          \n    \n\nNorth Huron School District\nMichigan\n182.7\n124\n0.80%\n          \n    \n\nShelby County School District\nAlabama\n717.1\n97\n1.21%\n          \n    \n\nLee County School District\nAlabama\n487.9\n96\n1.11%\n          \n    \n\nHumboldt Table Rock Steinauer Public Schools\nNebraska\n384.0\n93\n5.10%\n          \n    \n\nLimestone County School District\nAlabama\n515.3\n92\n2.67%\n          \n    \n\nGordon County School District\nGeorgia\n342.7\n91\n0.52%\n          \n    \n\nWalker County School District\nAlabama\n775.8\n84\n0.19%\n          \n    \n\nTuscaloosa County School District\nAlabama\n1,280.5\n70\n0.49%\n          \n    \n\nNorfolk Public Schools\nNebraska\n144.9\n62\n7.40%\n          \n    \n\nEast Butler Public Schools\nNebraska\n289.8\n53\n3.25%\n          \n    \n\nColumbus City School District\nOhio\n137.2\n51\n3.30%\n          \n    \n\nHall County School District\nGeorgia\n390.2\n50\n0.49%\n          \n    \n\nLangford School District 45-5\nSouth Dakota\n421.3\n50\n5.07%\n          \n    \n\nBartow County School District\nGeorgia\n440.8\n49\n0.17%\n          \n    \n\nWhitfield County School District\nGeorgia\n269.9\n49\n0.21%\n          \n    \n\nCozad Community Schools\nNebraska\n240.6\n47\n4.81%\n          \n    \n\nMontgomery County School District\nAlabama\n763.4\n46\n0.06%"
  },
  {
    "objectID": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html",
    "href": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "",
    "text": "In the fields of health informatics and health services research, health insurance claims data are a valuable resource to help answer questions about health care access and financing. However, claims data in the real world often contains both sensitive (protected health information) and proprietary (trade secrets) elements. For most students and educators seeking opportunities to learn how to use claims data, there are few available sources for practice.\nTo help with this problem, claimsdb 📦 provides easy access to a sample of health insurance enrollment and claims data from the Centers for Medicare and Medicaid Services (CMS) Data Entrepreneurs’ Synthetic Public Use File (DE-SynPUF), as a set of relational tables or as an in-memory database using DuckDB. All the data is contained within a single package, so users do not need to download any external data. This package is inspired by and based on the starwarsdb package.\nThe data are structured as actual Medicare claims data but are fully “synthetic,” after a process of alterations meant to reduce the risk of re-identification of real Medicare beneficiaries. The synthetic process that CMS adopted changes the co-variation across variables, so analysts should be cautious about drawing inferences about the actual Medicare population.\nThe data included in claimsdb comes from 500 randomly selected 2008 Medicare beneficiaries from Sample 2 of the DE-SynPUF, and it includes all the associated claims for these members for 2008-2009. CMS provides resources, including a codebook, FAQs, and other documents with more information about this data.\nTo introduce claimsdb, this post covers the following topics: - Installation of the claimsdb package - How to setup a remote database with the included data - Examples of how to use claimsdb for analysis - An overview of the limitations of the data included in package"
  },
  {
    "objectID": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#installation-and-components-of-the-claimsdb-package",
    "href": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#installation-and-components-of-the-claimsdb-package",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Installation and components of the claimsdb package",
    "text": "Installation and components of the claimsdb package\nYou can install the development version of claimsdb from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"jfangmeier/claimsdb\")\n\nYou can then load claimsdb alongside your other packages. We will be using the tidyverse packages in our later examples.\n\nlibrary(tidyverse)\nlibrary(claimsdb)\n\nThe tables are available after loading the claimsdb package. This includes a schema that describes each of the tables and the included variables from the CMS DE-SynPUF. You can see that claimsdb includes five tables. One of the tables, bene contains beneficiary records, while the others include specific types of claims data.\n\nschema\n\n# A tibble: 5 x 5\n  TABLE      TABLE_TITLE                                TABLE~1 UNIT_~2 PROPER~3\n  <chr>      <chr>                                      <chr>   <chr>   <list>  \n1 bene       CMS Beneficiary Summary DE-SynPUF          Synthe~ Benefi~ <tibble>\n2 carrier    CMS Carrier Claims DE-SynPUF               Synthe~ claim   <tibble>\n3 inpatient  CMS Inpatient Claims DE-SynPUF             Synthe~ claim   <tibble>\n4 outpatient CMS Outpatient Claims DE-SynPUF            Synthe~ claim   <tibble>\n5 pde        CMS Prescription Drug Events (PDE) DE-Syn~ Synthe~ claim   <tibble>\n# ... with abbreviated variable names 1: TABLE_DESCRIPTION, 2: UNIT_OF_RECORD,\n#   3: PROPERTIES\n\n\nYou can access details on the variables in each of the tables like in this example with the inpatient table. You can see that this table contains 35 fields, including a beneficiary code to identify members across tables as well as detailed information on each inpatient claim.\n\nschema %>% \n  filter(TABLE == \"inpatient\") %>% \n  pull(PROPERTIES)\n\n[[1]]\n# A tibble: 35 x 3\n   VARIABLE                 TYPE    DESCRIPTION                                 \n   <chr>                    <chr>   <chr>                                       \n 1 DESYNPUF_ID              string  Beneficiary Code                            \n 2 CLM_ID                   string  Claim ID                                    \n 3 SEGMENT                  numeric Claim Line Segment                          \n 4 CLM_FROM_DT              date    Claims start date                           \n 5 CLM_THRU_DT              date    Claims end date                             \n 6 PRVDR_NUM                string  Provider Institution                        \n 7 CLM_PMT_AMT              numeric Claim Payment Amount                        \n 8 NCH_PRMRY_PYR_CLM_PD_AMT numeric NCH Primary Payer Claim Paid Amount         \n 9 AT_PHYSN_NPI             string  Attending Physician National Provider Ident~\n10 OP_PHYSN_NPI             string  Operating Physician National Provider Ident~\n# ... with 25 more rows"
  },
  {
    "objectID": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#access-claims-data-as-a-remote-database",
    "href": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#access-claims-data-as-a-remote-database",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Access claims data as a remote database",
    "text": "Access claims data as a remote database\nMany organizations store claims data in a remote database, so claimsdb also includes all of the tables as an in-memory DuckDB database. This can be a great way to practice working with this type of data, including building queries with dplyr code using dbplyr.\nTo setup the in-memory database, you need to create a database connection using claims_connect() and create connections to each of the tables you want to use.\n\nlibrary(dbplyr)\n\n# Setup connection to duckDB database\ncon <- claims_connect()\n\n# Setup connections to each of the enrollment and claims tables in the database\nbene_rmt <- tbl(con, \"bene\")\ninpatient_rmt <- tbl(con, \"inpatient\")\noutpatient_rmt <- tbl(con, \"outpatient\")\ncarrier_rmt <- tbl(con, \"carrier\")\npde_rmt <- tbl(con, \"pde\")\n\nYou can then preview your connection to each of the remote tables.\n\n# Preview the prescription drug event table in the database\npde_rmt\n\n# Source:   table<pde> [?? x 8]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      PDE_ID    SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5\n   <chr>            <chr>     <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 00E040C6ECE8F878 83014463~ 2008-12-20 492880~      30      30       0      10\n 2 00E040C6ECE8F878 83594465~ 2009-04-25 529590~      20      30       0       0\n 3 00E040C6ECE8F878 83144465~ 2009-09-22 000834~      80      30      40      80\n 4 00E040C6ECE8F878 83614464~ 2009-10-03 634810~      60      10       0      10\n 5 00E040C6ECE8F878 83014461~ 2009-11-16 511294~      60      30       0      20\n 6 00E040C6ECE8F878 83234464~ 2009-12-11 580160~     270      30       0      10\n 7 014F2C07689C173B 83294462~ 2009-09-14 009045~      90      30       0      60\n 8 014F2C07689C173B 83874466~ 2009-10-11 596040~      40      20       0     570\n 9 014F2C07689C173B 83314462~ 2009-11-24 510790~      30      30       0     150\n10 014F2C07689C173B 83874467~ 2009-12-29 511293~      30      30       0      10\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT"
  },
  {
    "objectID": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#examples-of-using-claimsdb-for-analysis",
    "href": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#examples-of-using-claimsdb-for-analysis",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Examples of using claimsdb for analysis",
    "text": "Examples of using claimsdb for analysis\nTo analyze and explore the claims and beneficiary data, you can execute your own SQL code on the database using the DBI package.\n\nDBI::dbGetQuery(\n  con, \n  paste0(\n    'SELECT \"DESYNPUF_ID\", \"BENE_BIRTH_DT\"',\n    'FROM \"bene\"',\n    'WHERE \"BENE_SEX_IDENT_CD\" = 1.0',\n    'LIMIT 10'\n  )\n)\n\n        DESYNPUF_ID BENE_BIRTH_DT\n1  001115EAB83B19BB    1939-12-01\n2  03ADA78C0FEF79F4    1923-02-01\n3  040A12AB5EAA444C    1943-10-01\n4  0507DE00BC6E6CD6    1932-07-01\n5  05672CCCED56BCAD    1937-07-01\n6  060CDE3A044F64BA    1943-02-01\n7  061B5B3D9A459675    1932-04-01\n8  08C8E0A0C6EAC884    1954-06-01\n9  09EEB5C4C4FAEF10    1935-04-01\n10 0A58C6D6B9BE67CF    1942-07-01\n\n\nHowever, in the following examples, we will use the dbplyr package which translates dplyr code into SQL that can be executed against the database. You can see that the results below with dplyr functions match the results above that used a SQL query.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT\n   <chr>            <date>       \n 1 001115EAB83B19BB 1939-12-01   \n 2 03ADA78C0FEF79F4 1923-02-01   \n 3 040A12AB5EAA444C 1943-10-01   \n 4 0507DE00BC6E6CD6 1932-07-01   \n 5 05672CCCED56BCAD 1937-07-01   \n 6 060CDE3A044F64BA 1943-02-01   \n 7 061B5B3D9A459675 1932-04-01   \n 8 08C8E0A0C6EAC884 1954-06-01   \n 9 09EEB5C4C4FAEF10 1935-04-01   \n10 0A58C6D6B9BE67CF 1942-07-01   \n# ... with more rows\n\n\nWe can use the show_query() function to see the SQL code that dbplyr created and that it closely matches the SQL code above.\n\nbene_rmt %>% \n  filter(BENE_SEX_IDENT_CD == 1) %>% \n  select(DESYNPUF_ID, BENE_BIRTH_DT) %>% \n  show_query()\n\n<SQL>\nSELECT DESYNPUF_ID, BENE_BIRTH_DT\nFROM bene\nWHERE (BENE_SEX_IDENT_CD = 1.0)\n\n\n\nFirst, a note about working with dates/times in databases\ndbplyr is an amazing tool for working with databases, especially if you want to use many functions from the dplyr and tidyr packages. However, it does not currently have SQL translations for all functions in the tidyverse family of packages. For example, the lubridate package’s date and time functions work on local dataframes but cannot be translated to work on remote tables at this time. In the example below, you can see that the lubridate function year() (for parsing the year from a date) works on the local dataframe but generates an error on the remote table with the same data.\n\nbene %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# A tibble: 998 x 3\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with 988 more rows\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = lubridate::year(BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\nFortunately, dbplyr allows you pass along SQL functions in your dplyr code, and it will include these functions in the generated query. For date/time functions, we need to consult the documentation from DuckDB on date functions. To parse a part of a date (e.g., the year), we need to use the date_part() function for DuckDB.\nThe function to do this task may vary across database backends, so if you are doing this with a different database (Oracle, SQL Server, etc.), you will need to read the documentation for that database management system.\n\nbene_rmt %>% \n  transmute(\n    DESYNPUF_ID,\n    BENE_BIRTH_DT,\n    BIRTH_YEAR = date_part('year', BENE_BIRTH_DT)\n  )\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      BENE_BIRTH_DT BIRTH_YEAR\n   <chr>            <date>             <dbl>\n 1 001115EAB83B19BB 1939-12-01          1939\n 2 0018A1975BC0EE4F 1935-11-01          1935\n 3 00E040C6ECE8F878 1932-11-01          1932\n 4 014F2C07689C173B 1935-07-01          1935\n 5 029A22E4A3AAEE15 1921-05-01          1921\n 6 03410742A11DDC52 1943-07-01          1943\n 7 03ADA78C0FEF79F4 1923-02-01          1923\n 8 040A12AB5EAA444C 1943-10-01          1943\n 9 043AAAE41C9A37B7 1924-03-01          1924\n10 0507DE00BC6E6CD6 1932-07-01          1932\n# ... with more rows\n\n\n\n\nExample 1: which members had the highest prescription drug costs for 2008?\nFor this first example, we are going to identify the beneficiaries with the highest total prescription drug costs in 2008. We need to use the pde table that has claims on prescription drug events and the bene table that has beneficiary records. We create an object that is the aggregated costs for prescription drugs at the beneficiary level in 2008. Note that we had to use date_part() to parse the year from the service date.\n\n# Calculate rx costs for utilizing members in 2008\nrx_costs_rmt <- pde_rmt %>% \n  mutate(BENE_YEAR = date_part('year', SRVC_DT)) %>% \n  filter(BENE_YEAR == 2008) %>% \n  group_by(BENE_YEAR, DESYNPUF_ID) %>% \n  summarize(TOTAL_RX_COST = sum(TOT_RX_CST_AMT, na.rm = T), .groups = \"drop\") %>% \n  ungroup()\n\nrx_costs_rmt\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 00E040C6ECE8F878            10\n 2      2008 03ADA78C0FEF79F4          5020\n 3      2008 040A12AB5EAA444C           120\n 4      2008 043AAAE41C9A37B7           810\n 5      2008 05672CCCED56BCAD             0\n 6      2008 061B5B3D9A459675           270\n 7      2008 08BB74BA9DFD5C06            20\n 8      2008 08C8E0A0C6EAC884          5860\n 9      2008 09EEB5C4C4FAEF10          1590\n10      2008 0A58C6D6B9BE67CF           930\n# ... with more rows\n\n\nThen we join the aggregated cost data to the beneficiary table. This is necessary because the pde table does not include beneficiaries who didn’t use any prescription drugs. After joining the table we reassign missing cost data to zero for those beneficiaries with no utilization. We can then use collect() to retrieve the results as a local dataframe.\n\n# Join the rx costs data to the beneficiary file and include members with no costs\nrx_bene_rmt <- bene_rmt %>% \n  filter(BENE_YEAR == 2008) %>% \n  select(\n    BENE_YEAR,\n    DESYNPUF_ID\n  ) %>% \n  left_join(\n    rx_costs_rmt, by = c(\"BENE_YEAR\", \"DESYNPUF_ID\")\n  ) %>% \n  mutate(TOTAL_RX_COST = ifelse(is.na(TOTAL_RX_COST), 0, TOTAL_RX_COST)) %>% \n  arrange(desc(TOTAL_RX_COST))\n\nrx_bene_rmt\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n# Ordered by: desc(TOTAL_RX_COST)\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with more rows\n\n# You can use collect() to bring the results into a local dataframe\nrx_bene_df <- rx_bene_rmt %>% \n  collect()\n\nrx_bene_df\n\n# A tibble: 500 x 3\n   BENE_YEAR DESYNPUF_ID      TOTAL_RX_COST\n       <dbl> <chr>                    <dbl>\n 1      2008 484FAAB99C7F90F2         10410\n 2      2008 1187122DCAD04B48          9300\n 3      2008 5199E41C3B2B36AD          9110\n 4      2008 801D1702FC0C105F          8210\n 5      2008 92158DA24D7A8799          7910\n 6      2008 A705B8508EB5747D          7380\n 7      2008 A9142A5D9895479C          6780\n 8      2008 D20C21DD9F8177D0          6590\n 9      2008 08C8E0A0C6EAC884          5860\n10      2008 EBE9CC1D92AA56C0          5690\n# ... with 490 more rows\n\n\n\n\nExample 2: what percent of beneficiaries received an office visit within 30 days of discharge from a hospital?\nIn the next example, we are identifying which beneficiaries had an office visit within 30 days of being discharged. We will start with the inpatient table that contains records for all inpatient stays, including when a beneficiary was discharged. We create an object that includes the beneficiary ID, the discharge date, and the date 30 days after discharge. Note that for DuckDB we need to coerce “30” to an integer to calculate the new date.\n\n# Identify all member discharge dates and the dates 30 days after discharge from the inpatient table\nip_discharges <- inpatient_rmt %>% \n  transmute(\n    DESYNPUF_ID, \n    DSCHRG_DT = NCH_BENE_DSCHRG_DT,\n    DSCHRG_DT_30 = NCH_BENE_DSCHRG_DT + as.integer(30)) %>% \n  distinct()\n\nip_discharges\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      DSCHRG_DT  DSCHRG_DT_30\n   <chr>            <date>     <date>      \n 1 014F2C07689C173B 2009-09-20 2009-10-20  \n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24  \n 3 060CDE3A044F64BA 2008-10-18 2008-11-17  \n 4 08BB74BA9DFD5C06 2009-03-07 2009-04-06  \n 5 08BB74BA9DFD5C06 2009-03-08 2009-04-07  \n 6 08C8E0A0C6EAC884 2008-02-22 2008-03-23  \n 7 08C8E0A0C6EAC884 2009-05-28 2009-06-27  \n 8 08C8E0A0C6EAC884 2009-06-14 2009-07-14  \n 9 08C8E0A0C6EAC884 2009-07-07 2009-08-06  \n10 08C8E0A0C6EAC884 2009-08-23 2009-09-22  \n# ... with more rows\n\n\nNext, we need to identify office visits from the carrier table. I created a vector of five office visit codes for this example. Since these codes must match the values in the “HCPCS” columns, we reshape the table with pivot_longer() then filter for the office visit codes.\n\n# Create vector of office visit codes\noff_vis_cds <- as.character(99211:99215)\n\n# Identify members who had an office visit from the carrier table\noff_visit <- carrier_rmt %>% \n  select(DESYNPUF_ID, CLM_ID, CLM_FROM_DT, matches(\"HCPCS\")) %>% \n  pivot_longer(cols = matches(\"HCPCS\"), names_to = \"LINE\", values_to = \"HCPCS\") %>% \n  filter(HCPCS %in% off_vis_cds) %>% \n  distinct(DESYNPUF_ID, CLM_FROM_DT) %>% \n  mutate(OFFICE_VISIT = 1)\n\noff_visit\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID      CLM_FROM_DT OFFICE_VISIT\n   <chr>            <date>             <dbl>\n 1 00E040C6ECE8F878 2009-01-17             1\n 2 00E040C6ECE8F878 2009-07-20             1\n 3 00E040C6ECE8F878 2009-07-26             1\n 4 029A22E4A3AAEE15 2008-01-25             1\n 5 029A22E4A3AAEE15 2008-01-30             1\n 6 029A22E4A3AAEE15 2008-08-28             1\n 7 029A22E4A3AAEE15 2008-11-19             1\n 8 029A22E4A3AAEE15 2009-04-01             1\n 9 029A22E4A3AAEE15 2009-04-12             1\n10 029A22E4A3AAEE15 2009-05-13             1\n# ... with more rows\n\n\nFinally, we join the office visits object to the discharges object. We can use the sql_on option in left_join() to inject some custom SQL to join when the office visit date is within 30 days of the discharge date.\n\n# Join discharges data and office visit data\ndischarge_offvis <- \n  ip_discharges %>% \n  left_join(\n    off_visit,\n    sql_on = paste0(\n      \"(LHS.DESYNPUF_ID = RHS.DESYNPUF_ID) AND\",\n      \"(RHS.CLM_FROM_DT >= LHS.DSCHRG_DT) AND\",\n      \"(RHS.CLM_FROM_DT <= LHS.DSCHRG_DT_30)\"\n      )\n  )\n\ndischarge_offvis\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID.x    DSCHRG_DT  DSCHRG_DT_30 DESYNPUF_ID.y    CLM_FROM_DT OFFIC~1\n   <chr>            <date>     <date>       <chr>            <date>        <dbl>\n 1 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-25        1\n 2 029A22E4A3AAEE15 2008-01-25 2008-02-24   029A22E4A3AAEE15 2008-01-30        1\n 3 060CDE3A044F64BA 2008-10-18 2008-11-17   060CDE3A044F64BA 2008-11-01        1\n 4 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-07        1\n 5 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-04-27        1\n 6 1187122DCAD04B48 2009-04-06 2009-05-06   1187122DCAD04B48 2009-05-04        1\n 7 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-19        1\n 8 12D6FF0C18764D0D 2009-05-08 2009-06-07   12D6FF0C18764D0D 2009-05-24        1\n 9 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-06-22        1\n10 144C187653FBBE83 2008-06-08 2008-07-08   144C187653FBBE83 2008-07-04        1\n# ... with more rows, and abbreviated variable name 1: OFFICE_VISIT\n\n\nAfter the join is complete, we can calculate the share of discharges with a timely office visit.\n\n# Find the percent of members with a office visit within 30 days of discharge\ndischarge_offvis %>% \n  distinct(\n    DESYNPUF_ID.x,\n    DSCHRG_DT,\n    OFFICE_VISIT\n  ) %>% \n  mutate(OFFICE_VISIT = ifelse(is.na(OFFICE_VISIT), 0, OFFICE_VISIT)) %>% \n  summarize(\n    OFV_RATE = mean(OFFICE_VISIT, na.rm = T)\n  )\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n  OFV_RATE\n     <dbl>\n1    0.540\n\n\n\n\nExample 3: how well are beneficiaries filling their hypertension drug prescriptions?\nFor this final example, we need to identify hypertension medications from the pde table and calculate medication adherence rates for each beneficiary. To isolate the hypertension medications, we can borrow from the HEDIS medication list for ACE inhibitors and ARB medications (which are commonly used to treat hypertension). This medication list is for 2018/2019, so it likely includes new drugs that did not exist in 2008/2009 and may not include older drugs that are no longer used (ideally it’s best to use external lists that match the time frame of the claims data).\n\nhedis_wb <- tempfile()\n\ndownload.file(\n  url = \"https://www.ncqa.org/hedis-2019-ndc-mld-directory-complete-workbook-final-11-1-2018-3/\",\n  destfile = hedis_wb,\n  mode = \"wb\"\n)\n\nhedis_df <- readxl::read_excel(\n  path = hedis_wb,\n  sheet = \"Medications List to NDC Codes\"\n)\n\nhyp_ndc_df <- \n  hedis_df %>% \n  filter(`Medication List` == \"ACE Inhibitor/ARB Medications\") %>% \n  select(\n    LIST = `Medication List`,\n    PRODUCTID = `NDC Code`\n  )\n\nhyp_ndc_df\n\n# A tibble: 3,230 x 2\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with 3,220 more rows\n\n\nOne of the cool features of dbplyr is that we can copy this local dataframe to the database as a temporary table using the copy_to() function.\n\nhyp_ndc_rmt <- copy_to(con, hyp_ndc_df, overwrite = T)\n\nhyp_ndc_rmt\n\n# Source:   table<hyp_ndc_df> [?? x 2]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   LIST                          PRODUCTID  \n   <chr>                         <chr>      \n 1 ACE Inhibitor/ARB Medications 00093512501\n 2 ACE Inhibitor/ARB Medications 00093512505\n 3 ACE Inhibitor/ARB Medications 00185005301\n 4 ACE Inhibitor/ARB Medications 00185005305\n 5 ACE Inhibitor/ARB Medications 00247213730\n 6 ACE Inhibitor/ARB Medications 00378044301\n 7 ACE Inhibitor/ARB Medications 00781189201\n 8 ACE Inhibitor/ARB Medications 13811062810\n 9 ACE Inhibitor/ARB Medications 13811062850\n10 ACE Inhibitor/ARB Medications 21695032630\n# ... with more rows\n\n\nWith the hypertension codes in the database, we can use inner_join() to find the matching drugs from the pde table.\n\npde_hyp_rmt <- \n  pde_rmt %>% \n  inner_join(\n    hyp_ndc_rmt, by = c(\"PROD_SRVC_ID\" = \"PRODUCTID\")\n  )\n\npde_hyp_rmt\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB 0.5.1 [Josh@Windows 10 x64:R 4.1.0/:memory:]\n   DESYNPUF_ID   PDE_ID SRVC_DT    PROD_~1 QTY_D~2 DAYS_~3 PTNT_~4 TOT_R~5 LIST \n   <chr>         <chr>  <date>     <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>\n 1 03ADA78C0FEF~ 83554~ 2008-02-01 510790~      30      30       0      10 ACE ~\n 2 040A12AB5EAA~ 83974~ 2009-11-27 638740~      90      30       0      10 ACE ~\n 3 043AAAE41C9A~ 83024~ 2008-08-11 002472~     200      90      10      80 ACE ~\n 4 0D540BEBC45A~ 83454~ 2009-02-18 604290~      60      90      10      10 ACE ~\n 5 0D540BEBC45A~ 83874~ 2009-06-15 661050~      30      30       0      10 ACE ~\n 6 0E0A0107787B~ 83714~ 2008-06-30 661050~     100      90      70      60 ACE ~\n 7 109D67D11477~ 83064~ 2008-11-21 134110~      30      60       0      70 ACE ~\n 8 109D67D11477~ 83034~ 2009-07-03 666850~      30      30       0      10 ACE ~\n 9 10D75CDD5B4A~ 83804~ 2008-07-01 002472~      90      90       0      10 ACE ~\n10 1183CA4884F8~ 83934~ 2009-01-25 001850~      90      20       0     110 ACE ~\n# ... with more rows, and abbreviated variable names 1: PROD_SRVC_ID,\n#   2: QTY_DSPNSD_NUM, 3: DAYS_SUPLY_NUM, 4: PTNT_PAY_AMT, 5: TOT_RX_CST_AMT\n\n\nWe can now use collect() to retrieve the data as a local dataframe. As a dataframe, we can now use the AdhereR package to calculate the medication possession ratio (MPR) for members who filled a hypertension medication. MPR is a commonly used metric of medication adherence, measuring if beneficiaries have gaps between their prescription fills.\nWe can see the MPR for each member who filled one of the medications, and we can calculate the median and mean MPRs across these beneficiaries.\n\n# install.packages(\"AdhereR\")\nlibrary(AdhereR)\n\npde_hyp_df <- pde_hyp_rmt %>% \n  collect()\n\nhyp_adhere <- CMA4(\n  data = pde_hyp_df,\n  ID.colname = 'DESYNPUF_ID',\n  event.date.colname = 'SRVC_DT',\n  event.duration.colname = 'DAYS_SUPLY_NUM',\n  date.format = \"%Y-%m-%d\"\n)\n\nhyp_adhere_cma <- hyp_adhere$CMA %>% tibble()\n\nhyp_adhere_cma\n\n# A tibble: 134 x 2\n   DESYNPUF_ID         CMA\n   <chr>             <dbl>\n 1 03ADA78C0FEF79F4 0.0411\n 2 040A12AB5EAA444C 0.0411\n 3 043AAAE41C9A37B7 0.123 \n 4 0D540BEBC45ADCA7 0.164 \n 5 0E0A0107787B32AA 0.123 \n 6 109D67D114778917 0.123 \n 7 10D75CDD5B4AD3B0 0.123 \n 8 1183CA4884F8A0A8 0.0274\n 9 11C45CF0DDD03ACE 0.0822\n10 1226AD9944384B64 0.0411\n# ... with 124 more rows\n\nhyp_adhere_cma %>% \n  summarize(\n    median_mpr = median(CMA),\n    mean_mpr = mean(CMA)\n  )\n\n# A tibble: 1 x 2\n  median_mpr mean_mpr\n       <dbl>    <dbl>\n1     0.0411   0.0848"
  },
  {
    "objectID": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#data-limitations",
    "href": "posts/2022-02-12_introducing-the-claimsdb-package-for-teaching-and-learning/index.html#data-limitations",
    "title": "Introducing the claimsdb package for teaching and learning",
    "section": "Data Limitations",
    "text": "Data Limitations\nWhile data included in claimsdb is useful for many types of analyses, it does include a few notable limitations. - As mentioned earlier, the data is a small sample (500 beneficiaries) and is not intended to be representative of the Medicare population. In addition, the data is synthetic and should not be used for drawing inferences on the Medicare population. - Since the data is more than 10 years old, it doesn’t capture newer medications or procedures. It also includes procedure codes that have been retired or replaced. This is a challenge when applying external code lists that are much newer. - The diagnosis fields in the data use the International Classification of Diseases, Ninth Revision (ICD-9), but the United States converted to ICD-10 in 2015. If you are interesting in a mapping between ICD-9 and ICD-10, CMS has resources to consider. - The Medicare population is mostly Americans aged 65 and over, so the data will not have claims on certain specialties such as pediatrics or maternity care."
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#health-insurance-benchmark-data",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#health-insurance-benchmark-data",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Health insurance benchmark data",
    "text": "Health insurance benchmark data\nIn the United States, about half of the population has health insurance coverage through an employer. With employer-sponsored health insurance having such a large role in the American health care system, it’s important to understand trends and variation over time and how that affects affordability for employers, workers, and their family members.\nOne of the longest running surveys of employer-sponsored coverage is the Medical Expenditure Panel Survey - Insurance Component (MEPS-IC), administered by the federal Agency for Healthcare Research and Quality. Since the late 1990s, the MEPS-IC has asked employees and their employers about the health benefits offered to employees and what benefits employees enroll into, making it a valuable source of benchmark and trend data.\nA significant challenge with MEPS-IC data is that it is stored in separate web pages for each measure and year, with separate point estimate and standard error tables on each page. For this project, I am going to scrape the data from a subset of MEPS-IC tables using appropriate techniques and then visualize the trend data.\nLet’s get to it and load the R packages.\n\nlibrary(tidyverse)\nlibrary(rvest)\n#devtools::install_github(\"dmi3kno/polite\")\nlibrary(polite)\nlibrary(httr)\nlibrary(gganimate)\nlibrary(sf)\nlibrary(viridis)\nlibrary(gt)\n#devtools::install_github(\"hadley/emo\")\nlibrary(emo)"
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#web-scraping-and-the-polite-package",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#web-scraping-and-the-polite-package",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Web scraping and the polite package",
    "text": "Web scraping and the polite package\nTo scrape the data from the MEPS-IC website, I’m going to use the polite package, developed by Dmytro Perepolkin and co-authors. This package uses three principles of polite webscraping: seeking permission, taking slowly and never asking twice. Specifically, it manages the http session, declares the user agent string and checks the site policies, and uses rate-limiting and response caching to minimize the impact on the webserver.\nApplying the three principles of polite scraping, polite creates a session with bow, requests a page with nod, and pulls the contents of the page with scrape. Here is a brief example:\n\nsession <- bow(\"https://meps.ahrq.gov/\", force = TRUE)\nsession\n\n<polite session> https://meps.ahrq.gov/\n    User-agent: polite R package\n    robots.txt: 13 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\npage <- \n    nod(bow = session, \n        path = paste0(\"data_stats/summ_tables/insr/national/\", \n                      \"series_1/2019/tia1.htm\"))\npage\n\n<polite session> https://meps.ahrq.gov/data_stats/summ_tables/insr/national/series_1/2019/tia1.htm\n    User-agent: polite R package\n    robots.txt: 13 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\nscrape_page <- \n  page %>% \n    scrape(verbose=TRUE)\nscrape_page\n\n{html_document}\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n[1] <head>\\n<meta name=\"description\" content=\"2019 Insurance Component Summar ...\n[2] <body>\\r\\n<center>\\r\\n<table cellspacing=\"0\" border=\"0\" cellpadding=\"3\" c ..."
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#creating-a-table-of-contents-of-the-insurance-data",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#creating-a-table-of-contents-of-the-insurance-data",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Creating a table of contents of the insurance data",
    "text": "Creating a table of contents of the insurance data\nThe first step for scraping the MEP-IC pages is to generate a table of content of all of the webpages. To do this, I use polite to go through each of the pages in the MEPS-IC table of contents and append the results of each page together, using a while loop. I also use the rvest package to pull specific nodes from each page, including the table and a link to the next page.\n\n## Table of contents\ntoc_df <- tibble()\nsession <- bow(\"https://meps.ahrq.gov/\", force = TRUE)\n\n## State and metro tables\npath <- paste0(\"quick_tables_results.jsp?component=2&subcomponent=2\", \n               \"&year=-1&tableSeries=-1&tableSubSeries=&searchText=&\",\n               \"searchMethod=1&Action=Search\")\nindex <- 0\n\nwhile(!is.na(path)){\n  # make it verbose\n  index <- 1 + index\n  message(\"Scraping state/metro page: \", path)\n  # nod and scrape\n  current_page <- \n    nod(bow = session, \n        path = paste0(\"data_stats/\", path)) %>% \n    scrape(verbose=TRUE)\n  # extract post titles\n  toc_df <- current_page %>%\n    html_nodes(xpath = paste0('//*[contains(concat( \" \", @class, \" \" ),',\n                              ' concat( \" \", \"description\", \" \" ))]',\n                              '//table')) %>%\n    html_table(fill = TRUE) %>%\n    purrr::pluck(5) %>% \n    as_tibble() %>% \n    janitor::clean_names() %>% \n    bind_rows(toc_df)\n  # see if a \"Next\" link is available\n  if (index == 1){\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder div a\") %>% \n      html_attr(\"href\")\n  } else {\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder a+ a\") %>% \n      html_attr(\"href\")\n  }\n} # end while loop\n\n## National tables\npath <- paste0(\"quick_tables_results.jsp?component=2&subcomponent=1&\",\n               \"year=-1&tableSeries=-1&tableSubSeries=&searchText=&\", \n               \"searchMethod=1&Action=Search\")\nindex <- 0\n\nwhile(!is.na(path)){\n  # update index\n  index <- 1 + index\n  message(\"Scraping national page: \", path)\n  # nod and scrape\n  current_page <- \n    nod(bow = session, \n        path = paste0(\"data_stats/\", path)) %>% \n    scrape(verbose=TRUE)\n  # extract TOC tables\n  toc_df <- current_page %>%\n    html_nodes(xpath = paste0('//*[contains(concat( \" \", @class, \" \" ),',\n                              'concat( \" \", \"description\", \" \" ))]//table')) %>%\n    html_table(fill = TRUE) %>%\n    purrr::pluck(5) %>% \n    as_tibble() %>% \n    janitor::clean_names() %>% \n    bind_rows(toc_df)\n  # see if a \"Next\" link is available\n  if (index == 1){\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder div a\") %>% \n      html_attr(\"href\")\n  } else {\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder a+ a\") %>% \n      html_attr(\"href\")\n  }\n  }# end while loop\n\nAfter scraping the table of contents data, I prep it further and generate the set of URL links I will need to pull the specific data tables I plan to use for this project.\n\ntoc <- \n  toc_df %>% \n  select(-update) %>% \n  separate(title,\n           into = c(\"title\", \"year\"),\n           sep = \": United States,\",\n           convert = TRUE) %>% \n  mutate(year = as.numeric(substr(str_squish(year), 1, 4)),\n         number = str_remove(table_no, \"Table \")) %>% \n  separate(number,\n           into = c(\"series\", \"number\"),\n           sep = '\\\\.',\n           convert = TRUE,\n           extra = \"merge\") %>% \n  filter(!is.na(year)) %>% \n  mutate(number = str_remove_all(number, \"[:punct:]\"),\n         number = str_to_lower(paste0(\"t\", series, number)),\n         series = as.numeric(as.roman(series)),\n         url = case_when(\n           series %in% c(1, 3, 4, 11) ~ paste0(\"https://meps.ahrq.gov/data_stats/\",\n                                               \"summ_tables/insr/national/series_\",\n                                               series, \"/\", \n                                               year, \"/\", \n                                               number, \".htm\"),\n           TRUE ~ paste0(\"https://meps.ahrq.gov/data_stats/\", \n                         \"summ_tables/insr/state/series_\", \n                         series, \"/\", \n                         year, \"/\", \n                         number, \".htm\")\n         )) %>% \n  filter(!is.na(year) & !is.na(series) & !is.na(number))"
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#scraping-data-on-family-premium-costs",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#scraping-data-on-family-premium-costs",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Scraping data on family premium costs",
    "text": "Scraping data on family premium costs\nAs part of the project, I’m interested in the trends in the total premium costs of family health insurance through an employer (the sum of employer and employee contributions). To pull this data, I need to scrape the data from Table IID1, which has this data by state. First, I pull a vector of URLs from the table of contents, and then I use the politely wrapper function to apply polite principles with purrr::map and the httr::GET function.\n\n## Family premiums by state\nfamily_prem_url_vctr <- \n  toc %>% \n  filter(number == \"tiid1\" & year >= 2000) %>% \n  pull(url)\n\n## Politely GET pages\npolite_GET <- politely(GET, verbose = FALSE)\n\nfamily_prem_res <- map(family_prem_url_vctr, polite_GET)"
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#tidying-the-scraped-data-with-the-tidyverse",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#tidying-the-scraped-data-with-the-tidyverse",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Tidying the scraped data with the tidyverse",
    "text": "Tidying the scraped data with the tidyverse\nThe result is a list object with a length of the number of pages scraped.\n\nclass(family_prem_res)\n\n[1] \"list\"\n\nlength(family_prem_res)\n\n[1] 19\n\n\nWith a little bit of rvest and purrr you can see a preview of the table in the webpage, but the results are very messy, with extra columns, blank rows, and offset values.\n\nfamily_prem_res %>% \n  purrr::pluck(1) %>% \n  read_html() %>% \n  html_nodes(xpath = '/html/body/table[1]') %>% \n  html_table(fill = TRUE, header = FALSE) %>%\n  as.data.frame() %>% \n  tibble()\n\n# A tibble: 59 x 18\n   X1    X2    X3    X4    X5    X6    X7    X8    X9    X10   X11   X12   X13  \n   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~\n 2 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 3 \"Div~ \"Tot~ \"\"    \"Les~ \"\"    \"10 ~ \"\"    \"25 ~ \"\"    \"100~ \"\"    \"100~ \"\"   \n 4 \"Div~ \"Tot~ \"\"    \"Les~ \"\"    \"10 ~ \"\"    \"25 ~ \"\"    \"100~ \"\"    \"100~ \"\"   \n 5 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 6 \"Uni~ \"6,7~ \"\"    \"6,9~ \"\"    \"6,8~ \"\"    \"6,6~ \"\"    \"6,6~ \"\"    \"6,8~ \"\"   \n 7 \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~\n 8 \"Mas~ \"7,3~ \"\"    \"8,4~ \"\"    \"8,2~ \"\"    \"7,4~ \"\"    \"7,0~ \"\"    \"7,0~ \"\"   \n 9 \"New~ \"7,5~ \"\"    \"8,2~ \"\"    \"7,3~ \"\"    \"7,7~ \"\"    \"6,9~ \"\"    \"7,6~ \"\"   \n10 \"Con~ \"7,2~ \"\"    \"7,5~ \"\"    \"7,6~ \"\"    \"7,1~ \"\"    \"7,9~ \"\"    \"7,0~ \"\"   \n# ... with 49 more rows, and 5 more variables: X14 <chr>, X15 <chr>, X16 <chr>,\n#   X17 <chr>, X18 <chr>\n\n\nTo fix the table and transform it to a shape that is better for visualization, I created a function that processes each scraped result and puts it into a tidier format (long, not wide). Some highlights of this function include:\n\nTesting different xpaths to find the data table in the page. Over time, the MEPS-IC tables changed in their format and styling, so the pages do not have a consistent structure.\nApply a hierarchy of group categories, groups, and segments.\nPull the point estimate and standard error data tables separately and combine them at the end.\nUse the janitor package to remove empty rows and columns and rename variables from row values.\nUse the zoo package to fill in blank values from above rows.\nConvert all percentages to decimal values.\n\n\nseriesScraper <- function(resp) {\n\n  if (resp %>%\n      purrr::pluck(1) %>% \n      read_html() %>% #test to find the right node\n      html_nodes(xpath = '/html/body/center/table[1]') %>%\n      html_table(fill = TRUE, header = FALSE) %>%\n      as.data.frame() %>%\n      ncol() > 0) {\n    node <- \"/html/body/center/table\"\n  } else if (resp %>%\n             purrr::pluck(1) %>% \n             read_html() %>% #test to find the right node\n             html_nodes(xpath = '/html/body/table[1]') %>%\n             html_table(fill = TRUE, header = FALSE) %>%\n             as.data.frame() %>%\n             ncol() > 0) {\n    node <- \"/html/body/table\"\n  } else {\n    node <- \"/html/body/div/table\"\n  }\n  \n  string_filter <- paste(\n    \"\\\\_\\\\_\",\n    \"Source:\",\n    \"Note:\",\n    \"Table I\",\n    \"Table II\",\n    \"Table 1\",\n    \"Table 2\",\n    \"\\\\* Figure\",\n    \"\\\\*\\\\* Definitions\",\n    \"Totals may\",\n    \"Dollar amounts\",\n    \"\\\\*Figure\",\n    \"Definitions\",\n    \"No estimate\",\n    \"These cell\",\n    \"States not\",\n    sep = \"|\"\n  )\n  \n  url <- \n    resp %>% \n    purrr::pluck(1) %>% \n    purrr::pluck(\"url\")\n  \n  if (is.null(url)){\n    url <- \n      resp %>% \n      purrr::pluck(1)\n  }\n  \n  pt_ests <-\n    resp %>%\n    purrr::pluck(1) %>% \n    read_html() %>%\n    html_nodes(xpath = paste0(node, \"[1]\")) %>%\n    html_table(fill = TRUE, header = FALSE) %>%\n    as.data.frame() %>%\n    na_if(\"\") %>%\n    distinct() %>%\n    filter(X1 != \"District of Columbia\" &\n             X2 != \"District of Columbia\") %>%\n    mutate_at(vars(-X1), ~ ifelse(!is.na(X1) &\n                                    is.na(.), X1, .)) %>%\n    mutate(X1 = ifelse(is.na(X1) & !is.na(X2), \"group\", X1)) %>%\n    janitor::remove_empty(\"rows\") %>%\n    mutate_all( ~ str_replace_all(., \"[\\r\\n]\" , \"\")) %>%\n    mutate_all( ~ str_squish(str_remove(., \"\\\\*\\\\*\"))) %>%\n    mutate_all( ~ replace(., str_detect(., \"These cell\"), NA)) %>%\n    filter(!str_detect(.[[1]], string_filter)) %>%\n    janitor::remove_empty(c(\"rows\", \"cols\")) %>%\n    distinct() %>%\n    janitor::row_to_names(\n      row_number = 1,\n      remove_row = TRUE,\n      remove_rows_above = TRUE\n    ) %>%\n    subset(select = which(!duplicated(names(.)))) %>%\n    subset(select = which(!is.na(names(.)))) %>%\n    mutate(\n      group_category = ifelse(.[[1]] == .[[2]], .[[1]], NA),\n      group_category = zoo::na.locf(group_category, na.rm = FALSE),\n      group_category = ifelse(is.na(group_category), \"United States\", group_category)\n    ) %>%\n    filter(!.[[1]] == .[[2]]) %>%\n    rename(group = 1) %>%\n    pivot_longer(\n      cols = -starts_with(\"group\"),\n      names_to = \"segment\",\n      values_to = \"pt_est\"\n    ) %>%\n    mutate_if(is.factor, as.character)\n  \n  std_errs <- \n    resp %>%\n    purrr::pluck(1) %>% \n    read_html() %>%\n    html_nodes(xpath = paste0(node, \"[2]\")) %>%\n    html_table(fill = TRUE, header = FALSE) %>%\n    as.data.frame() %>%\n    na_if(\"\") %>%\n    distinct() %>%\n    filter(X1 != \"District of Columbia\" &\n             X2 != \"District of Columbia\") %>%\n    mutate_at(vars(-X1), ~ ifelse(!is.na(X1) &\n                                    is.na(.), X1, .)) %>%\n    mutate(X1 = ifelse(is.na(X1) & !is.na(X2), \"group\", X1)) %>%\n    janitor::remove_empty(\"rows\") %>%\n    mutate_all( ~ str_replace_all(., \"[\\r\\n]\" , \"\")) %>%\n    mutate_all( ~ str_squish(str_remove(., \"\\\\*\\\\*\"))) %>%\n    mutate_all( ~ replace(., str_detect(., \"These cell\"), NA)) %>%\n    filter(!str_detect(.[[1]], string_filter)) %>%\n    janitor::remove_empty(c(\"rows\", \"cols\")) %>%\n    distinct() %>%\n    janitor::row_to_names(\n      row_number = 1,\n      remove_row = TRUE,\n      remove_rows_above = TRUE\n    ) %>%\n    subset(select = which(!duplicated(names(.)))) %>%\n    subset(select = which(!is.na(names(.)))) %>%\n    mutate(\n      group_category = ifelse(.[[1]] == .[[2]], .[[1]], NA),\n      group_category = zoo::na.locf(group_category, na.rm = FALSE),\n      group_category = ifelse(is.na(group_category), \"United States\", group_category)\n    ) %>%\n    filter(!.[[1]] == .[[2]]) %>%\n    rename(group = 1) %>%\n    pivot_longer(\n      cols = -starts_with(\"group\"),\n      names_to = \"segment\",\n      values_to = \"std_err\"\n    ) %>%\n    mutate_if(is.factor, as.character) %>% \n    select(-group_category)\n  \n  inner_join(pt_ests,\n             std_errs,\n             by = c(\"group\", \"group_category\", \"segment\")) %>%\n    mutate(url = url) %>%\n    mutate(numeric = case_when(\n      str_detect(pt_est, \"[A-Za-z]\") ~ 0,\n      str_detect(std_err, \"[A-Za-z]\") ~ 0,\n      TRUE ~ 1\n    )) %>%\n    filter(numeric == 1) %>%\n    mutate(\n      pt_est = str_trim(str_remove_all(pt_est, \"\\\\*\"), side = \"both\"),\n      std_err = str_trim(str_remove_all(std_err, \"\\\\*\"), side = \"both\"),\n      pt_est = if_else(\n        str_detect(pt_est, \"%\"),\n        as.numeric(gsub(\"%\", \"\", pt_est)) / 100,\n        as.numeric(gsub(\",\", \"\", pt_est))\n      ),\n      std_err = if_else(\n        str_detect(std_err, \"%\"),\n        as.numeric(gsub(\"%\", \"\", std_err)) / 100,\n        as.numeric(gsub(\",\", \"\", std_err))\n      )\n    ) %>%\n    select(-numeric) %>%\n    janitor::remove_empty(c(\"cols\", \"rows\")) %>% \n    mutate(\n      series = parse_number(str_extract(url, \"series_[:digit:]{1,2}\")),\n      year = parse_number(str_extract(url, \"[:digit:]{4}\")),\n      table = str_remove(str_extract(url, \"[:alnum:]{2,12}.htm\"), \".htm\")\n    ) %>% \n    filter(!is.na(pt_est) & !is.na(std_err)) %>%\n    mutate_if(is.character, ~ str_remove(., \"\\\\:$\"))\n}\n\nWith this new function, I used purrr::map_dfr to apply it to each scraped page in the list object and append all the resulting tidy data frames together.\n\nfamily_prem_df <- map_dfr(family_prem_res, seriesScraper)\n\n\nhead(family_prem_df)\n\n# A tibble: 6 x 9\n  group         group_category segment   pt_est std_err url   series  year table\n  <chr>         <chr>          <chr>      <dbl>   <dbl> <chr>  <dbl> <dbl> <chr>\n1 United States United States  Total      6772.    19.6 http~      2  2000 tiid1\n2 United States United States  Less tha~  6994.   149   http~      2  2000 tiid1\n3 United States United States  10 - 24 ~  6860.   143.  http~      2  2000 tiid1\n4 United States United States  25 - 99 ~  6628.    78.4 http~      2  2000 tiid1\n5 United States United States  100-999 ~  6606.    52.3 http~      2  2000 tiid1\n6 United States United States  1000 or ~  6817.    37.7 http~      2  2000 tiid1"
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#visualizing-family-premium-costs",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#visualizing-family-premium-costs",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Visualizing family premium costs",
    "text": "Visualizing family premium costs\nFor the family premium cost data, I want to generate a map to show how costs increase across states over time. For the state map, I’m going with a hexmap of states from Carto. To make all the hexagons the same size, I transformed the data to a Mercator projection. I also created a crosswalk of state names and abbreviations to join the data together.\n\nhex_map <- st_read(file.path(path_to_data, \"us_states_hexgrid.geojson\"))\n\nReading layer `us_states_hexgrid' from data source \n  `C:\\Users\\Josh\\Dropbox\\Projects\\meps-ic\\polite\\us_states_hexgrid.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 51 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -137.9747 ymin: 26.39343 xmax: -69.90286 ymax: 55.3132\nGeodetic CRS:  WGS 84\n\nstate_abb <- \n  tibble(\n    name = state.name,\n    abb = state.abb\n  ) %>% \n  bind_rows(\n    tibble(\n      name = \"District of Columbia\",\n      abb = \"DC\"\n    )\n  )\n\n#transform to mercator projection\nhex_map_merc <- st_transform(hex_map, crs = 3785)\n\nhex_map_merc %>% \n  ggplot() +\n  geom_sf()\n\n\n\n\nThe next step is to prep the family premium data with the map. I used the tidyr::complete function to add missing values for any states that did not have data for a particular year. This is most common in the earlier years of the survey. I then used gganimate to created an animated gif showing the changes in premium costs across states.\n\nfamily_prem_map_df <- \n  family_prem_df %>% \n  filter(segment == \"Total\") %>% \n  select(year, group, pt_est, std_err) %>% \n  tidyr::complete(year, group) %>% \n  inner_join(\n    state_abb, by = c(\"group\" = \"name\")\n  ) %>% \n  inner_join(\n    hex_map_merc, by = c(\"abb\" = \"iso3166_2\")\n  ) %>% \n  select(\n    abb, \n    year, \n    pt_est,\n    std_err,\n    geometry\n  )\n\nfamily_prem_anim <-\n  family_prem_map_df %>% \n  ggplot() +\n  geom_sf(aes(fill = pt_est, geometry = geometry)) +\n  scale_fill_viridis_c(\n    option = \"magma\", \n    na.value = \"grey50\",\n    aesthetics = \"fill\",\n    labels = scales::dollar\n  ) +\n  geom_sf_label(aes(label = abb, geometry = geometry), \n                color = \"black\", \n                size = 3, \n                label.padding = unit(0.15, \"lines\")) +\n  geom_text(aes(x = -8400000, y = 3200000, label = paste0(year)),\n            size = 8,\n            color = 'black') +\n  theme_bw() + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0, size = 24),\n        plot.subtitle = element_text(size = 14),\n        panel.border = element_blank(),\n        panel.spacing = unit(3, \"lines\"),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        legend.key.width = unit(4, \"lines\"),\n        legend.position = c(0.50, 0.85),\n        legend.direction = \"horizontal\",\n        legend.title = element_blank()) +\n  labs(\n    title = 'Family Health Insurance Premium Costs',\n    subtitle = 'For enrolled workers with employer-based coverage',\n    caption = 'data source: MEPS-IC | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nYou can see which states have had higher and lower premium costs. Alaska 🤯."
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#scraping-data-on-insurance-coverage-by-industry",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#scraping-data-on-insurance-coverage-by-industry",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Scraping data on insurance coverage by industry",
    "text": "Scraping data on insurance coverage by industry\nThe next part of this project is to compare health insurance trends by industry. To do this, I scraped and tidied data from five sets of tables on the number of establishments, the number of workers, enrollment rates, and deductible levels for single (employee-only) health insurance.\n\n## Coverage Rates by Industry\nenrollment_url_vctr <- \n  toc %>% \n  filter(number %in% c(\"tia1\", \"tib1\", \"tib2\", \"tib2b\", \"tif2\"),\n         year >= 2000) %>% \n  pull(url)\n\nenrollment_res <- map(enrollment_url_vctr, polite_GET)\n\nenrollment_df <- map_dfr(enrollment_res, seriesScraper)"
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#comparing-variation-in-deductibles-across-industries",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#comparing-variation-in-deductibles-across-industries",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Comparing variation in deductibles across industries",
    "text": "Comparing variation in deductibles across industries\nI then prepped the data a little further and calculated the number of enrolled employees for each industrial sector, using other values from the survey. I also plotted the trends in employee deductibles from 2000 to 2019 for each of the sectors.\n\nenrollment_ind_df <- \n  enrollment_df %>% \n  filter(group_category == \"Industry group\" & segment == \"Total\") %>% \n  mutate(group = case_when(\n    group == \"Fin. svs. and real est.\" ~ \"Fin. svs. and real estate\",\n    group == \"Other Services\" ~ \"Other services\",\n    TRUE ~ group)) %>% \n  filter(group != \"Unknown\") %>% \n  select(year, group, pt_est, table, year) %>% \n  pivot_wider(names_from = table, values_from = pt_est) %>% \n  mutate(enrolled_employees = tib1 * tib2 * tib2b) %>% \n  select(year,\n         industry = group,\n         establishments = tia1,\n         employees = tib1,\n         enrolled_employees,\n         average_employee_deductible = tif2) %>% \n  filter(!is.na(average_employee_deductible))\n\n\nenrollment_ind_df %>% \n  ggplot(aes(x = year, y = average_employee_deductible)) +\n  geom_line() +\n  geom_hline(yintercept = 0.5) +\n  facet_wrap(~industry)"
  },
  {
    "objectID": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#creating-a-table-to-display-industry-variation",
    "href": "posts/2020-11-28_politely-scraping-health-insurance-data/index.html#creating-a-table-to-display-industry-variation",
    "title": "Politely Scraping Health Insurance Data",
    "section": "Creating a table to display industry variation",
    "text": "Creating a table to display industry variation\nInstead of creating another animated gif, I decided to use the gt package to generate a table that contained the information I wanted to display. Thomas Mock has an excellent guide for creating high quality tables with gt that I highly recommend. He includes a section on how to implement plots in your table, so you can have numbers and visuals in your table together. In this case, I am going to create a line graph or spark plot to display the deductible trends, by creating a function to generate the plots.\n\nplot_spark <- function(data){\n  data %>% \n    mutate(color = \"blue\") %>% \n    ggplot(aes(x = year, y = average_employee_deductible, color = color)) +\n    geom_line(size = 15) +\n    theme_void() +\n    scale_color_identity() +\n    theme(legend.position = \"none\")\n}\n\ndeductible_plots <- enrollment_ind_df %>% \n  select(industry, year, average_employee_deductible) %>% \n  nest(deductibles = c(year, average_employee_deductible)) %>% \n  mutate(plot = map(deductibles, plot_spark))\n\nFor some added fun, I’m adding emojis that represent each of the industrial sectors. Hadley Wickham’s emo package makes it very easy to add emoji values.\n\nemoji <- tribble(\n  ~industry, ~emoji,\n  \"Agric., fish., forest.\", emo::ji(\"man_farmer\"),\n  \"Mining and manufacturing\", emo::ji(\"woman_factory_worker\"),\n  \"Construction\", emo::ji(\"construction_worker_man\"),\n  \"Utilities and transp.\", emo::ji(\"bus\"),\n  \"Wholesale trade\", emo::ji(\"ship\"),\n  \"Fin. svs. and real estate\", emo::ji(\"dollar\"),\n  \"Retail trade\", emo::ji(\"department_store\"),\n  \"Professional services\", emo::ji(\"woman_health_worker\"),\n  \"Other services\", emo::ji(\"man_artist\")\n)\n\nUsing Mock’s guide for gt tables, I then setup my new table with the embedded plots using the text_transform function and the data frame with the plots I created earlier. With gt, you can also easily add spanners, format the values in your columns, and change the font styling and background colors to create the look you want.\n\nenrollment_spark <-\n  enrollment_ind_df %>% \n  filter(year == 2019) %>% \n  inner_join(emoji, by = \"industry\") %>% \n  select(emoji, \n         industry, \n         establishments, \n         employees, \n         enrolled_employees, \n         average_employee_deductible) %>% \n  rename_all(~str_to_title(str_replace_all(., \"\\\\_\", \" \"))) %>% \n  mutate(ggplot = NA) %>% \n  gt() %>% \n  text_transform(\n    locations = cells_body(vars(ggplot)),\n    fn = function(x){\n      map(deductible_plots$plot, \n          ggplot_image, \n          height = px(20), \n          aspect_ratio = 4)\n    }\n  ) %>% \n  cols_width(vars(ggplot) ~ px(100)) %>% \n  cols_label(\n    ggplot = \"Employee Deductibles:\\n2000-2019\",\n    Emoji = \"\",\n  ) %>% \n  fmt_number(3:5, decimals = 0) %>% \n  fmt_currency(6, decimals = 0) %>% \n  tab_spanner(\n    label = \"2019 Private Sector Charactertics, USA\",\n    columns = c(3:6)\n  ) %>% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(everything()),\n      cells_column_labels(everything())\n    )\n  ) %>%  \n  tab_options(\n    row_group.border.top.width = px(3),\n    row_group.border.top.color = \"black\",\n    row_group.border.bottom.color = \"black\",\n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    column_labels.border.bottom.color = \"black\",\n    column_labels.border.bottom.width = px(2),\n  ) %>% \n  tab_source_note(md(\"**Source**: MEPS-IC | **Table by**: @joshfangmeier\"))\n\nenrollment_spark"
  },
  {
    "objectID": "posts/2019-07-17_college-football-bar-chart-races/index.html#bar-chart-races",
    "href": "posts/2019-07-17_college-football-bar-chart-races/index.html#bar-chart-races",
    "title": "College Football Bar Chart Races",
    "section": "Bar Chart Races",
    "text": "Bar Chart Races\nThis project is inspired by the Athletic’s fantastic series by Matt Brown and Michael Weinreb that looks back on 150 years of college football by summarizing each decade. One of my favorite parts of the series is how it describes the rise and fall of various schools over time, such as the early dominance of the Ivy League prior to World War I, Oklahoma in the 1950s, and Miami in the 1980s.\nThe series also got me to think about how all these trends in program success could be visualized, and a bar chart race seemed like the obvious option to try out. I heard about bar chart races from John Burn-Murdoch’s interview on the PolicyViz podcast where he discussed his innovative bar chart race of city populations dating back to 1500 CE. Burn-Murdoch’s chart was created in D3, but I’ve been looking to get some experience with the gganimate R package."
  },
  {
    "objectID": "posts/2019-07-17_college-football-bar-chart-races/index.html#data-source-and-wrangling",
    "href": "posts/2019-07-17_college-football-bar-chart-races/index.html#data-source-and-wrangling",
    "title": "College Football Bar Chart Races",
    "section": "Data source and wrangling",
    "text": "Data source and wrangling\nIn order to show win totals for college football programs over time, I needed team record data for all teams for more than a century. Fortunately, Sports Reference kindly provides detailed data on each season for all teams that played major college football. I patiently scraped the win-loss records of each college football program from their team pages.\nA quick note on “major schools:” Sports Reference has a somewhat subjective definition of major schools that they adopted from James Howell that looks at whether a school played 50% or more of its games against football bowl subdivision (FBS) level opponents during a season. Some schools gain or lose major status over time. For example, Yale hasn’t been a major school in football since 1981 by this definition, even though they still play football today. For this project, I only included seasons of major schools.\nWith that out of the way, let’s load the R packages.\n\nlibrary(tidyverse)\nlibrary(zoo)\nlibrary(RcppRoll)\nlibrary(gganimate)\nlibrary(shadowtext)\n\nNow let’s look at the variables in the data from Sports Reference.\n\nglimpse(team_record_raw)\n\nRows: 13,616\nColumns: 17\n$ school   <chr> \"Air Force\", \"Air Force\", \"Air Force\", \"Air Force\", \"Air Forc~\n$ year     <chr> \"2019\", \"2018\", \"2017\", \"2016\", \"2015\", \"2014\", \"2013\", \"2012~\n$ conf     <chr> \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\", \"MWC\"~\n$ w        <chr> \"\", \"5\", \"5\", \"10\", \"8\", \"10\", \"2\", \"6\", \"7\", \"9\", \"8\", \"8\", ~\n$ l        <chr> \"\", \"7\", \"7\", \"3\", \"6\", \"3\", \"10\", \"7\", \"6\", \"4\", \"5\", \"5\", \"~\n$ t        <chr> \"\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0~\n$ pct      <chr> \"\", \".417\", \".417\", \".769\", \".571\", \".769\", \".167\", \".462\", \"~\n$ srs      <chr> \"\", \"-1.72\", \"-4.77\", \"4.09\", \"0.36\", \"2.20\", \"-14.88\", \"-7.3~\n$ sos      <chr> \"\", \"-3.80\", \"-1.02\", \"-4.53\", \"-3.14\", \"-4.88\", \"-3.22\", \"-6~\n$ ap_pre   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"~\n$ ap_high  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"23\", \"\", \"\", \"\", \"\", \"\",~\n$ ap_post  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"~\n$ coach_es <chr> \"Troy Calhoun (0-0)\", \"Troy Calhoun (5-7)\", \"Troy Calhoun (5-~\n$ bowl     <chr> \"\", \"\", \"\", \"Arizona Bowl-W\", \"Armed Forces Bowl-L\", \"Famous ~\n$ notes    <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"~\n$ from     <chr> \"1957\", \"1957\", \"1957\", \"1957\", \"1957\", \"1957\", \"1957\", \"1957~\n$ to       <chr> \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019~\n\n\nThe primary variables I used for this project are school, year, conf (conference membership), w (wins), l (losses), and notes. Within the notes variable is information about whether the school had its record adjusted by the NCAA. For example, USC had to forgo all 12 of its wins from the 2005 season. To line up with the official NCAA records, I parsed the notes field and adjusted team record data accordingly.\nSome schools had gaps in their data for when they did not play as a major school or didn’t play at all, so I used tidyr::complete and zoo::na.locf to fill in those years with records of 0 wins and 0 losses.\nFinally, I used RcppRoll::roll_sumr to generate 10-year rolling win and loss totals, along with cumulative win and loss totals.\n\nteam_record_prep <- team_record_raw %>%\n  mutate_at(vars(year, w, l, t, pct, srs, sos, ap_pre, ap_high, ap_post),\n            list(~ as.numeric(.))) %>%\n  mutate(adjustment = ifelse(str_detect(notes, \"^(record adjusted to )\"), str_squish((\n    substr(notes, 20, 25)\n  )), NA)) %>%\n  separate(\n    col = adjustment,\n    sep = \"\\\\-\",\n    into = c(\"w_adj\", \"l_adj\", \"t_adj\")\n  ) %>%\n  #record if adjusted by NCAA\n  mutate_at(vars(w_adj, l_adj, t_adj), list(~ as.numeric(.))) %>%\n  mutate(\n    w = ifelse(is.na(w_adj), w, w_adj),\n    l = ifelse(is.na(l_adj), l, l_adj),\n    t = ifelse(is.na(t_adj), t, t_adj)\n  ) %>%\n  filter(year <= 2018) %>%\n  complete(year, school) %>% #provide all school/year combinations to account for years that a school takes off\n  group_by(school) %>%\n  arrange(desc(from)) %>%\n  mutate_at(vars(to, from), list(~ na.locf(.))) %>%\n  ungroup() %>%\n  filter(year >= from) %>% #remove school/year combinations before the school began playing\n  mutate(played_season = ifelse(is.na(w), 0, 1)) %>%\n  mutate_at(vars(w, l, t), list(~ ifelse(is.na(.), 0, .))) %>% #assign 0-0-0 for seasons not played\n  arrange(school, year) %>%\n  group_by(school) %>%\n  mutate(\n    cumulative_wins = cumsum(w),\n    cumulative_losses = cumsum(l),\n    rolling_wins_10yr =  roll_sumr(w, n = 10),\n    rolling_losses_10yr = roll_sumr(l, n = 10),\n    conf = na.locf(conf)\n  ) %>%\n  ungroup() %>%\n  mutate_at(vars(rolling_wins_10yr, rolling_losses_10yr), list(~ ifelse(is.na(.), 0, .)))\n\nIn the last step of data preparation, I ranked each school on their rolling 10-year and cumulative totals for wins and losses. Since I am only going to show the “race” among the leaders for each year, I chose to keep only the top 10 schools for each measure. I also trimmed Washington & Jefferson’s name down a bit. Apologies to the Presidents!\n\nteam_record_final <- team_record_prep %>%\n  group_by(year) %>%\n  mutate(\n    rank_cm_wins = rank(desc(cumulative_wins), ties.method = \"first\"),\n    rank_cm_losses = rank(desc(cumulative_losses), ties.method = \"first\"),\n    rank_r10_wins = rank(desc(rolling_wins_10yr), ties.method = \"first\"),\n    rank_r10_losses = rank(desc(rolling_losses_10yr), ties.method = \"first\"),\n    school = ifelse(school == \"Washington & Jefferson\", \"Wash & Jefferson\", school)\n  ) %>%\n  ungroup() %>%\n  filter_at(\n    vars(rank_cm_wins, rank_cm_losses, rank_r10_wins, rank_r10_losses),\n    any_vars(. %in% 1:10)\n  ) %>%\n  select(\n    year,\n    school,\n    conf,\n    cumulative_wins,\n    cumulative_losses,\n    rolling_wins_10yr,\n    rolling_losses_10yr,\n    rank_cm_wins,\n    rank_cm_losses,\n    rank_r10_wins,\n    rank_r10_losses\n  ) %>% \n  arrange(school, year)\n\nHere’s how the final dataset looks:\n\nglimpse(team_record_final)\n\nRows: 4,274\nColumns: 11\n$ year                <dbl> 2013, 2014, 2016, 2017, 2018, 1924, 1925, 1926, 19~\n$ school              <chr> \"Akron\", \"Akron\", \"Akron\", \"Akron\", \"Akron\", \"Alab~\n$ conf                <chr> \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"MAC\", \"Southern\", \"So~\n$ cumulative_wins     <dbl> 121, 126, 139, 146, 150, 127, 137, 146, 151, 157, ~\n$ cumulative_losses   <dbl> 196, 203, 215, 222, 230, 54, 54, 54, 58, 61, 64, 6~\n$ rolling_wins_10yr   <dbl> 38, 37, 38, 41, 40, 61, 65, 68, 68, 74, 72, 72, 76~\n$ rolling_losses_10yr <dbl> 82, 84, 83, 82, 83, 19, 17, 14, 16, 19, 21, 20, 17~\n$ rank_cm_wins        <int> 148, 147, 143, 142, 143, 31, 29, 25, 27, 27, 28, 2~\n$ rank_cm_losses      <int> 129, 126, 127, 127, 125, 61, 63, 66, 63, 64, 61, 6~\n$ rank_r10_wins       <int> 106, 108, 111, 109, 112, 8, 7, 4, 4, 3, 3, 3, 3, 3~\n$ rank_r10_losses     <int> 9, 9, 8, 9, 7, 69, 82, 93, 90, 85, 75, 79, 91, 91,~"
  },
  {
    "objectID": "posts/2019-07-17_college-football-bar-chart-races/index.html#setting-up-the-animation-theme",
    "href": "posts/2019-07-17_college-football-bar-chart-races/index.html#setting-up-the-animation-theme",
    "title": "College Football Bar Chart Races",
    "section": "Setting up the animation theme",
    "text": "Setting up the animation theme\nTo make the graphic look a little bit like a football field, I set the background to be “Astroturf Green” and much of the text and lines to be white. Emily Kuehler’s post was a huge help with setting up a theme and getting gganimate to work properly. Go check out her post for some awesome Grand Slam tennis and NBA scoring bar chart races!\n\nmy_background <- '#196F0C' #Astroturf Green\nmy_theme <- theme(\n  rect = element_rect(fill = my_background),\n  plot.background = element_rect(fill = my_background, color = NA),\n  panel.background = element_rect(fill = my_background, color = NA),\n  panel.border = element_blank(),\n  plot.title = element_text(face = 'bold', size = 20, color = 'white'),\n  plot.subtitle = element_text(size = 14, color = 'white'),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.major.x = element_line(color = 'white'),\n  panel.grid.minor.x = element_line(color = 'white'),\n  legend.position = 'none',\n  plot.caption = element_text(size = 10, color = 'white'),\n  axis.ticks = element_blank(),\n  axis.text.y =  element_blank(),\n  axis.text = element_text(color = 'white')\n)\n\ntheme_set(theme_light() + my_theme)"
  },
  {
    "objectID": "posts/2019-07-17_college-football-bar-chart-races/index.html#creating-some-plots",
    "href": "posts/2019-07-17_college-football-bar-chart-races/index.html#creating-some-plots",
    "title": "College Football Bar Chart Races",
    "section": "Creating some plots!",
    "text": "Creating some plots!\nIn each of the plots, I used the shadowtext::geom_shadowtext function to add a black drop shadow behind the year to increase the contrast against the axis lines in the background. I also had to do a lot of testing with nudge_y to get the school labels in a position that worked both for schools with long names and those with short ones.\n\nRolling 10-year winners\n\nrolling_wins_chart <- team_record_final %>%\n  filter(rank_r10_wins %in% 1:10 & rolling_wins_10yr > 0) %>%\n  ggplot(aes(rank_r10_wins * -1, group = school)) +\n  geom_tile(aes(\n    y = rolling_wins_10yr / 2,\n    height = rolling_wins_10yr,\n    width = 0.9,\n    fill = conf\n  ),\n  alpha = 0.9) +\n  geom_text(\n    aes(y = rolling_wins_10yr, label = school),\n    nudge_y = -20,\n    nudge_x = .2,\n    size = 4\n  ) +\n  geom_text(\n    aes(y = rolling_wins_10yr, label = conf),\n    nudge_y = -20,\n    nudge_x = -.2,\n    size = 2.5\n  ) +\n  geom_text(aes(y = rolling_wins_10yr, label = as.character(rolling_wins_10yr)), nudge_y = 5) +\n  geom_shadowtext(aes(\n    x = -10,\n    y = 118,\n    label = paste0(year)\n  ),\n  size = 8,\n  color = 'white') +\n  coord_cartesian(clip = \"off\", expand = FALSE) +\n  coord_flip() +\n  labs(\n    title = 'Most College Football Wins',\n    subtitle = 'Ten Year Rolling Total of Major Program Games',\n    caption = 'bar colors represent conferences\\ndata source: Sports Reference | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nThere is a lot of noise from year to year among the top 10 by rolling win total (as you would expect!), but the plot really does show the rise of certain programs (Notre Dame, Oklahoma, Miami, Alabama). Also, it’s fascinating to see the variation over time in win totals necessary to be at the top.\nI’m also pleased with how the conference membership is displayed and how it clearly shows when a school changes conferences, like when Nebraska moved from the Big 8 to the Big 12 in 1990s.\n\n\n\n… and rolling 10-year losers\n\nrolling_losses_chart <- team_record_final %>%\n  filter(rank_r10_losses %in% 1:10 & rolling_losses_10yr > 0) %>%\n  ggplot(aes(rank_r10_losses * -1, group = school)) +\n  geom_tile(\n    aes(\n      y = rolling_losses_10yr / 2,\n      height = rolling_losses_10yr,\n      width = 0.9,\n      fill = conf\n    ),\n    alpha = 0.9\n  ) +\n  geom_text(\n    aes(y = rolling_losses_10yr, label = school),\n    nudge_y = -15,\n    nudge_x = .2,\n    size = 4\n  ) +\n  geom_text(\n    aes(y = rolling_losses_10yr, label = conf),\n    nudge_y = -15,\n    nudge_x = -.2,\n    size = 2.5\n  ) +\n  geom_text(aes(y = rolling_losses_10yr, label = as.character(rolling_losses_10yr)), nudge_y = 5) +\n  geom_shadowtext(aes(\n    x = -10,\n    y = 105,\n    label = paste0(year)\n  ),\n  size = 8,\n  color = 'white') +\n  coord_cartesian(clip = \"off\", expand = FALSE) +\n  coord_flip() +\n  labs(\n    title = 'Most College Football Losses',\n    subtitle = 'Ten Year Rolling Total of Major Program Games',\n    caption = 'bar colors represent conferences\\ndata source: Sports Reference | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nK-State fans might want to look away.\n\n\n\nAll time winners\n\nalltime_wins_chart <- team_record_final %>%\n  filter(rank_cm_wins %in% 1:10 & cumulative_wins > 0) %>%\n  ggplot(aes(rank_cm_wins * -1, group = school)) +\n  geom_tile(aes(\n    y = cumulative_wins / 2,\n    height = cumulative_wins,\n    width = 0.9,\n    fill = conf\n  ),\n  alpha = 0.9) +\n  geom_text(\n    aes(y = cumulative_wins, label = school),\n    nudge_y = -90,\n    nudge_x = .2,\n    size = 4\n  ) +\n  geom_text(\n    aes(y = cumulative_wins, label = conf),\n    nudge_y = -90,\n    nudge_x = -.2,\n    size = 2.5\n  ) +\n  geom_text(aes(y = cumulative_wins, label = as.character(cumulative_wins)), nudge_y = 25) +\n  geom_shadowtext(aes(\n    x = -10,\n    y = 925,\n    label = paste0(year)\n  ),\n  size = 8,\n  color = 'white') +\n  coord_cartesian(clip = \"off\", expand = FALSE) +\n  coord_flip() +\n  labs(\n    title = 'Most College Football Losses',\n    subtitle = 'Cumulative Total of Major Program Games',\n    caption = 'bar colors represent conferences\\ndata source: Sports Reference | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n\nTo wrap this up, I also generated a plot of cumulative wins. This plot clearly shows how much of a lead the Ivy League developed from their early start and how they all dropped out of the top 10 once they stopped playing major football in 1981."
  },
  {
    "objectID": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#visualizing-marathon-races",
    "href": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#visualizing-marathon-races",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Visualizing marathon races",
    "text": "Visualizing marathon races\nWith marathons and other large gatherings canceled or delayed by the COVID-19 pandemic, I’ve been feeling extra nostalgic for past running experiences in some of my favorite places. At the top of the list is the Big Sur Marathon that runs along California State Route 1 from Big Sur Station to Carmel. The course starts near the southern edge of the Redwood forest, rises to the top of Hurricane Point overlooking the Pacific, crosses iconic Bixby Creek Bridge, and passes over numerous hills and turns before reaching the finish. Big Sur is a challenging spring marathon, and while I finished in just over 4 hours in 2019, it was definitely the most difficult race I’ve run. Having a live piano performance at Bixby helped though.\n\nLike other runners, I tracked my progress with GPS, using the RunKeeper app on my phone. This project will use the GPS location data from RunKeeper and visualize the course using the rayshader R package, which can provide a 3-dimensional view of a landscape or other surface. Tyler Morgan-Wall recently posted an excellent guide for combining elevation data with satellite imagery to create 3D renderings of landscapes. Examples from Sebastian Engel-Wolf, Simon Coulombe, and Francois Keck of using animation with rayshader also provided plenty of guidance and inspiration."
  },
  {
    "objectID": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#data-sources-and-prep",
    "href": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#data-sources-and-prep",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Data sources and prep",
    "text": "Data sources and prep\nThis project will need the GPS data from RunKeeper, satellite imagery data, and elevation data. Morgan-Wall’s post includes the step-by-step process for downloading the SRTM elevation data and imagery data from USGS. I won’t repeat those steps here, but I’ve pre-downloaded the data for the Big Sur region of California.\nLet’s get to it and load the R packages.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rayshader)\nlibrary(raster)\nlibrary(sf)\nlibrary(mapview)\nlibrary(magick)\n\nFirst, let’s load the elevation data. I downloaded data for two bordering areas, so I’ll merge them together.\n\nelevation1 <- raster::raster(file.path(path_to_data, \"N36W122.hgt\"))\nelevation2 <- raster::raster(file.path(path_to_data, \"N36W123.hgt\"))\n\nbigsur_elevation <- raster::merge(elevation1,elevation2)\nplot(bigsur_elevation)\n\n\n\n\nNext, let’s load the satellite data, which comes in three files that are separate layers for the image. After loading the layers, we’ll combine them and apply a color adjustment. It will need more refinement, but the Monterey peninsula is visible.\n\nbigsur_r <- raster::raster(file.path(path_to_data, \"LC08_L1TP_044035_20191026_20191030_01_T1_B4.TIF\"))\nbigsur_g <- raster::raster(file.path(path_to_data, \"LC08_L1TP_044035_20191026_20191030_01_T1_B3.TIF\"))\nbigsur_b <- raster::raster(file.path(path_to_data, \"LC08_L1TP_044035_20191026_20191030_01_T1_B2.TIF\"))\n\nbigsur_rbg_corrected <- sqrt(raster::stack(bigsur_r, bigsur_g, bigsur_b))\nraster::plotRGB(bigsur_rbg_corrected)\n\n\n\n\nFinally, let’s load the GPS data from RunKeeper, which comes as a GPX file. With a quick view using mapView, we can see the tracked points from the GPS file are in the right locations.\n\nbigsur_gpx <-\n  st_read(\n    file.path(path_to_data, \"RK_gpx_2019-04-28_0643.gpx\"),\n    layer = \"track_points\",\n    quiet = TRUE\n  ) %>%\n  dplyr::select(track_seg_point_id, ele, time, geometry)\n\nmapView(bigsur_gpx)\n\n\n\n\n\n\nNow that each of the three files are loaded, we need to get them all on the same projection system to get them to play nicely together. In this case, I applied the CRS from the imagery files to the other two files. We will also use the boundaries of the GPS file to crop the dimensions of the other files.\n\nbigsur_extent <- \n  bigsur_gpx %>%\n  st_transform(., raster::crs(bigsur_r)) %>%\n  as_Spatial() %>%\n  extent() + 1e4\n\nbigsur_extent\n\nclass      : Extent \nxmin       : 590082.3 \nxmax       : 614524.6 \nymin       : 4007102 \nymax       : 4049264 \n\nrasterOptions(chunksize=1e+06, maxmemory=1e+08)\nbigsur_elevation_utm <- raster::projectRaster(bigsur_elevation, crs = crs(bigsur_r), method = \"bilinear\")\ncrs(bigsur_elevation_utm)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=10 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"WGS 84 / UTM zone 10N\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 10N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-123,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 126Â°W and 120Â°W, northern hemisphere between equator and 84Â°N, onshore and offshore. Canada - British Columbia (BC); Northwest Territories (NWT); Nunavut; Yukon. United States (USA) - Alaska (AK).\"],\n        BBOX[0,-126,84,-120]],\n    ID[\"EPSG\",32610]] \n\nbigsur_rgb_cropped <- raster::crop(bigsur_rbg_corrected, bigsur_extent)\nelevation_cropped <- raster::crop(bigsur_elevation_utm, bigsur_extent)\n\nFollowing Morgan-Wall’s guide, we make a few more adjustments to the imagery data. We now have a nicely cropped image of the marathon course with good contrast and color balance.\n\nnames(bigsur_rgb_cropped) <- c(\"r\",\"g\",\"b\")\n\nbigsur_r_cropped <- rayshader::raster_to_matrix(bigsur_rgb_cropped$r)\nbigsur_g_cropped <- rayshader::raster_to_matrix(bigsur_rgb_cropped$g)\nbigsur_b_cropped <- rayshader::raster_to_matrix(bigsur_rgb_cropped$b)\n\nbigsurel_matrix <- rayshader::raster_to_matrix(elevation_cropped)\n\nbigsur_rgb_array <- array(0,dim=c(nrow(bigsur_r_cropped),ncol(bigsur_r_cropped),3))\n\nbigsur_rgb_array[,,1] <- bigsur_r_cropped/255 #Red layer\nbigsur_rgb_array[,,2] <- bigsur_g_cropped/255 #Blue layer\nbigsur_rgb_array[,,3] <- bigsur_b_cropped/255 #Green layer\n\nbigsur_rgb_array <- aperm(bigsur_rgb_array, c(2,1,3))\n\nbigsur_rgb_contrast <- scales::rescale(bigsur_rgb_array,to=c(0,1))\nplot_map(bigsur_rgb_contrast)"
  },
  {
    "objectID": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#enhancing-and-plotting-the-gps-data",
    "href": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#enhancing-and-plotting-the-gps-data",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Enhancing and plotting the GPS data",
    "text": "Enhancing and plotting the GPS data\nThe GPS data only includes elevation, time, and coordinate information right now…\n\nbigsur_gpx %>% \n  head(n = 5) %>% \n  as_tibble()\n\n# A tibble: 5 x 4\n  track_seg_point_id   ele time                            geometry\n               <int> <dbl> <dttm>                       <POINT [°]>\n1                  0  92.7 2019-04-28 08:45:22  (-121.781 36.24761)\n2                  1  92.7 2019-04-28 08:45:22  (-121.781 36.24761)\n3                  2  92.9 2019-04-28 08:45:33 (-121.7811 36.24765)\n4                  3  93   2019-04-28 08:45:49 (-121.7812 36.24767)\n5                  4  93.1 2019-04-28 08:45:53 (-121.7813 36.24771)\n\n\nBut we can enhance it to calculate distance between each of the points, along with cumulative distance and time elapsed.\n\nbigsur_gpx_enr <-\n  bigsur_gpx %>% \n  st_transform(., raster::crs(bigsur_r)) %>%\n  mutate(time = as_datetime(time),\n         elapsed_time = ifelse(dplyr::row_number() == 1, 0,\n                               time - lag(time)),\n         distance_from_prior = sf::st_distance(\n           geometry,\n           lag(geometry),\n           by_element = TRUE),\n         distance_from_prior = ifelse(is.na(distance_from_prior), 0, distance_from_prior),\n         cumul_time = cumsum(elapsed_time),\n         cumul_dist = cumsum(distance_from_prior)\n  ) %>% \n  mutate(row = row_number()) %>% \n  as_Spatial() %>%\n  data.frame() %>%\n  mutate(lon = coords.x1,\n         lat = coords.x2,\n         dep = ele + 25)\n\nWe can also plot the enhanced data to show Hurricane Point near the midpoint of the race (the orange vertical bar), all the times I stopped to take photos, and how my pace variation increased a lot in the last few miles as I started to wear out. The final elapsed time and distance values turned out a little larger than the actual race stats, since I started the GPS before the starting line and after the finish line.\n\nbigsur_gpx_enr %>% \n  transmute(cumul_miles = measurements::conv_unit(cumul_dist, \"m\", \"mi\"),\n         Elevation = measurements::conv_unit(ele, \"m\", \"ft\"),\n         Pace = elapsed_time / 60 / measurements::conv_unit(distance_from_prior, \"m\", \"mi\")) %>% \n  pivot_longer(Elevation:Pace, names_to = \"measure\", values_to = \"value\") %>% \n  ggplot(aes(cumul_miles, value, color = measure)) +\n    facet_wrap(~measure, ncol = 1, scale = \"free_y\") +\n  geom_line(size = 1.5) +\n  geom_vline(xintercept = 12.2, color = \"orange\", size = 4, alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Big Sur Marathon GPX\",\n       x = \"Distance (miles)\",\n       y = NULL,\n       color = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#creating-a-video-animation-of-the-marathon",
    "href": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#creating-a-video-animation-of-the-marathon",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Creating a video animation of the marathon",
    "text": "Creating a video animation of the marathon\nTo put together a video animation of the course, we will need to generate a large number of frames that will later be rendered into a video clip. Each frame will have a specific shot angle and displayed progress on the course.\nFor generating all the values of the angles, I used the transition_values function from Will Bishop.\n\ntransition_values <- function(from, to, steps = 10, \n                              one_way = FALSE, type = \"cos\") {\n  if (!(type %in% c(\"cos\", \"lin\")))\n    stop(\"type must be one of: 'cos', 'lin'\")\n  \n  range <- c(from, to)\n  middle <- mean(range)\n  half_width <- diff(range)/2\n  \n  # define scaling vector starting at 1 (between 1 to -1)\n  if (type == \"cos\") {\n    scaling <- cos(seq(0, 2*pi / ifelse(one_way, 2, 1), length.out = steps))\n  } else if (type == \"lin\") {\n    if (one_way) {\n      xout <- seq(1, -1, length.out = steps)\n    } else {\n      xout <- c(seq(1, -1, length.out = floor(steps/2)), \n                seq(-1, 1, length.out = ceiling(steps/2)))\n    }\n    scaling <- approx(x = c(-1, 1), y = c(-1, 1), xout = xout)$y \n  }\n  \n  middle - half_width * scaling\n}\n\nThe video will be 24 seconds long at 60 frames per second, so 1,440 frames will need to be created. Here are the values used to create all the angles:\n\nn_frames <- 60 * 24\ngpx_rows <- seq(1, nrow(bigsur_gpx_enr), length.out = n_frames) %>% round()\nzscale <- 15\nthetavalues <- transition_values(from = 280, \n                                 to = 130,\n                                 steps = n_frames,        \n                                 one_way = TRUE, \n                                 type = \"lin\")\nphivalues <- transition_values(from = 70, \n                               to = 20, \n                               steps = n_frames,\n                               one_way = FALSE, \n                               type = \"cos\")\nzoomvalues <- transition_values(from = 0.8, \n                                to = 0.3, \n                                steps = n_frames,\n                                one_way = FALSE, \n                                type = \"cos\")\n\nAnother challenge is plotting the GPS coordinates as a 3D line that progresses over time. After trying out a few options, I generated a new add_3d_line function based on Vinay Udyawer’s KUD3D package. This function is essentially a wrapper that takes the coordinate data, calculates point distances, and creates a line using rgl::lines3d.\n\nadd_3d_line <-\n  function(ras,\n           det,\n           zscale,\n           lonlat = FALSE,\n           col = \"red\",\n           alpha = 0.8,\n           size = 2,\n           ...) {\n    e <- extent(ras)\n    cell_size_x <-\n      raster::pointDistance(c(e@xmin, e@ymin), c(e@xmax, e@ymin), lonlat = lonlat) / ncol(ras)\n    cell_size_y <-\n      raster::pointDistance(c(e@xmin, e@ymin), c(e@xmin, e@ymax), lonlat = lonlat) / nrow(ras)\n    distances_x <-\n      raster::pointDistance(c(e@xmin, e@ymin), cbind(det$lon, rep(e@ymin, nrow(det))), lonlat = lonlat) / cell_size_x\n    distances_y <-\n      raster::pointDistance(c(e@xmin, e@ymin), cbind(rep(e@xmin, nrow(det)), det$lat), lonlat = lonlat) / cell_size_y\n    \n      rgl::lines3d(\n        x = distances_y - (nrow(ras)/2),\n        y = det$dep / zscale,\n        z = abs(distances_x) - (ncol(ras)/2),\n        color = col,\n        alpha = alpha,\n        lwd = size,\n        ...\n      )\n  }\n\nTo bring all the elevation, imagery, and GPS data together, I needed to rotate the imagery and elevation data 90 degrees, so I created a new version of the imagery file and rotated the elevation data using the matlab::rot90 function. I then added labels for the start and finish points of the race by manually finding the coordinates with the best fit.\nWithin the loop function, I calculated the cumulative distance and elapsed time for each frame and adjusted the camera for the shot angles. Using the new add_3d_line function, I then overlay the 3D path of the race progress over the landscape. After each snapshot is captured, I then remove the path using rgl::rgl.pop to reduce the amount of memory consumed by the creating all the frames.\n\nimage_write(image_rotate(image_read(bigsur_rgb_contrast), 90), file.path(path_to_data, \"rotated.png\"))\noverlay_img <- png::readPNG(file.path(path_to_data, \"rotated.png\"), info = TRUE)\n\nplot_3d(overlay_img, \n        matlab::rot90(bigsurel_matrix, k = 1),\n        zscale = zscale, \n        fov = 0,\n        theta = thetavalues[1],\n        phi = phivalues[1],\n        windowsize = c(1000,800),\n        zoom = zoomvalues[1],\n        water=FALSE,          \n        background = \"#F2E1D0\", \n        shadowcolor = \"#523E2B\")\n\nrender_label(\n  elevation_cropped,\n  x = 155,\n  y = 785,\n  z = 1000,\n  zscale = zscale,\n  text = \"Start\",\n  freetype = FALSE,\n  textcolor = \"#EF9D3C\",\n  linecolor = \"#EF9D3C\",\n  dashed = TRUE\n)\nrender_label(\n  elevation_cropped,\n  x = 1200,\n  y = 300,\n  z = 1000,\n  zscale = zscale,\n  text = \"Finish\",\n  freetype = FALSE,\n  textcolor = \"#EF9D3C\",\n  linecolor = \"#EF9D3C\",\n  dashed = TRUE\n)\n\nfor (i in seq_len(n_frames)) {\n\n  gpx_frame <- bigsur_gpx_enr %>% filter(row <= gpx_rows[i])\n  \n  elapsed_time <-\n    gpx_frame %>% pull(cumul_time) %>% max(.) %>% hms::as_hms(.) %>% as.character(.)\n  elapsed_dist <-\n    gpx_frame %>% pull(cumul_dist) %>% max(.) %>% measurements::conv_unit(., \"m\", \"mi\") %>% round(., 1) %>% as.character(.)\n  \n  render_camera(theta = thetavalues[i],\n                phi = phivalues[i],\n                zoom = zoomvalues[i])\n  \n  gpx_frame %>%\n    add_3d_line(\n      ras = elevation_cropped,\n      det = .,\n      zscale = 15,\n      latlon = FALSE,\n      alpha = 0.6,\n      size = 4,\n      col = \"#FFCC00\"\n    )  \n  \n  Sys.sleep(2.5)\n  \n  render_snapshot(filename = file.path(path_to_frames, paste0(\"bigsur\", i, \".png\")), \n                  title_text = glue::glue(\"Big Sur Marathon | April 28, 2019 | time: {elapsed_time} | distance: {elapsed_dist} miles\"),\n                  title_bar_color = \"#022533\", title_color = \"white\", title_bar_alpha = 1)\n  \n  rgl::rgl.pop(type = \"shapes\")\n  gc()\n}\nrgl::rgl.close()\n\nFinally, after all the frames are captured, I rendered the PNG files using ffmpeg within a system() call to create a new mp4 file.\n\nsetwd(file.path(path_to_frames))\nsystem(\"ffmpeg -framerate 60 -i bigsur%d.png -pix_fmt yuv420p bigsur_marathon.mp4\")\n\nVideo"
  },
  {
    "objectID": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#wrapping-up",
    "href": "posts/2020-05-24_rayshading-the-big-sur-marathon/index.html#wrapping-up",
    "title": "Rayshading the Big Sur Marathon",
    "section": "Wrapping up",
    "text": "Wrapping up\nOverall, this video clip turned out much better than I expected. The suite of R packages to work with geographic data is really impressive, and my learning curve was lowered thanks to a number of excellent step-by-step guides. Rayshader is also a great way to take your geographic data to the next level. While I’m still really new to rayshader, I look forward to using it in future projects."
  },
  {
    "objectID": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html",
    "href": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "",
    "text": "This weekend, I attempted to solve my first FiveThirtyEight Riddler challenge. Many of these challenges require a bit more probability theory than I’m comfortable with, but this week’s classic challenge hit a subject that I care too much about: college basketball national champions and the bragging rights that come from beating the champ:\n\nOn Sunday, the Baylor Lady Bears won the 2019 NCAA women’s basketball championship, and on Monday, the Virginia Cavaliers did the same on the men’s side. But what about all of the unsung transitive champions? For example, earlier in the season, Florida State beat Virginia, thereby laying claim to a transitive championship for the Seminoles. And Boston College beat Florida State, claiming one for the Eagles. And IUPUI beat Boston College, and Ball State beat IUPUI, and so on and so on. Baylor, meanwhile, only lost once, to Stanford, who lost to five teams, and so on. How many transitive national champions were there this season in the women’s and men’s games? Or, maybe more descriptively, how many teams weren’t transitive national champions? You should include tournament losses in your calculations.\n\nThe author (Oliver Roeder) then supplies links to the results of women’s and men’s basketball for the 2018-2019 season from masseyratings.com. On first inspection of the game results, they seem to be in a text format that could be scrapped. In addition, I noticed that the women’s link includes 27,266 games, while the men’s link contains only 6,048. The women’s results page includes several junior colleges and even my alma mater Hastings College, who I know compete at the NAIA level, not NCAA Division I. The men’s results page includes only Division I teams, and since the challenge only mentions Baylor and Virginia, I’m assuming we want to compare Division I transitive champions. I used another link to pull the women’s results for 5,638 Division I games."
  },
  {
    "objectID": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html#scraping-the-game-results-data",
    "href": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html#scraping-the-game-results-data",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "Scraping the game results data",
    "text": "Scraping the game results data\nI decided to tackle this challenge using the tidyverse family of R packages that can scrap the data and wrangle it into a tidy format for further analysis.\nOne major challenge is that I wrote a function to parse the college names and scores from the Massey Rating text. This involves some very gnarly regular expression writing as you can see.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rvest)\n\n\nbb_games_process <- function(x){\n  read_html(x) %>% \n    html_node(xpath = '/html/body/pre/text()[1]') %>% \n    html_text() %>% \n    enframe() %>% \n    separate_rows(value, sep = \"\\n\") %>% \n    mutate(value = str_squish(value),\n           game = substring(value, 12),\n           date = ymd(substr(value,1,10)),\n           win_team = str_extract(game, \"^[\\\\@]{0,1}[:alpha:]{1,}[:blank:]{0,1}[:punct:]{0,1}[:blank:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}\"),\n           win_pts = as.integer(str_extract(game, \"(?<=[:alpha:]{1,100}[:blank:]{1})[:digit:]{1,3}\")),\n           lose_team = str_extract(game, \"(?<=[:digit:]{1,3}[:blank:]{1})[\\\\@]{0,1}[:alpha:]{1,}[:blank:]{0,1}[:punct:]{0,1}[:blank:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,}[:punct:]{0,1}[:alpha:]{0,1}\"),\n           lose_pts = as.integer(str_extract(game, \"(?<=[:digit:]{1,3}[:blank:]{1}[\\\\@]{0,1}[:alpha:]{1,100}[:blank:]{0,1}[:punct:]{0,1}[:blank:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,100}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{0,1}[:alpha:]{1,100}[:punct:]{0,1}[:alpha:]{0,1}[:blank:]{1})[:digit:]{1,3}\"))) %>% \n    select(-name, -value) %>% \n    filter(!is.na(date)) %>% \n    mutate(home_team = if_else(str_detect(win_team, \"\\\\@\"), win_team, \n                               if_else(str_detect(lose_team, \"\\\\@\"), lose_team, \"Neutral Court\"))) %>% \n    mutate_at(vars(contains('team')), list(~str_remove(., \"\\\\@\"))) %>% \n    cbind(x)\n}\n\n#wbb_url <- \"https://www.masseyratings.com/scores.php?s=305973&sub=305973&all=1\" #Original URL (All WBB games)\nwbb_url <- \"https://www.masseyratings.com/scores.php?s=305973&sub=11590&all=1\" #Corrected URL (only D1 WBB games)\nmbb_url <- \"https://www.masseyratings.com/scores.php?s=cb2019&sub=ncaa-d1&all=1&sch=1\" #Original URL (only D1 MBB games)\nurls <- c(wbb_url, mbb_url)\n\nbb_games <- map_dfr(urls, bb_games_process) %>% \n  as_tibble() %>% \n  mutate(sport = if_else(x == wbb_url, \"WBB\", \"MBB\")) %>% \n  select(-x)\n\nbb_games %>% tail(n=5)\n\n# A tibble: 5 x 8\n  game                  date       win_t~1 win_pts lose_~2 lose_~3 home_~4 sport\n  <chr>                 <date>     <chr>     <int> <chr>     <int> <chr>   <chr>\n1 Texas 81 Lipscomb 66~ 2019-04-04 Texas        81 Lipsco~      66 Neutra~ MBB  \n2 South Florida 77 @De~ 2019-04-05 South ~      77 DePaul       65 DePaul  MBB  \n3 Virginia 63 Auburn 6~ 2019-04-06 Virgin~      63 Auburn       62 Neutra~ MBB  \n4 Texas Tech 61 Michig~ 2019-04-06 Texas ~      61 Michig~      51 Neutra~ MBB  \n5 Virginia 85 Texas Te~ 2019-04-08 Virgin~      85 Texas ~      77 Neutra~ MBB  \n# ... with abbreviated variable names 1: win_team, 2: lose_team, 3: lose_pts,\n#   4: home_team\n\n\nUsing this prepped data, we can identify that there were 543 teams that played in women’s games and 650 that played in men’s games during the last season."
  },
  {
    "objectID": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html#calculating-the-number-of-transitive-champions",
    "href": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html#calculating-the-number-of-transitive-champions",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "Calculating the number of transitive champions",
    "text": "Calculating the number of transitive champions\nTo identify each “transitive champion” in each sport, I looked for where the nation champion lost and pulled a vector of the opponent(s) who beat the champion during the season. I then looped (ugh, I know, I know) through multiple rounds to see who defeated those teams, and so on and so forth. With each loop, I also pasted the number of unique transitive champions who were identified in each round into a data frame for further analysis.\n\nSetup\n\nrounds <- 25\nwbb_n_champ <- \"Baylor\"\nmbb_n_champ <- \"Virginia\"\n\n\n\nWBB calculations\n\nwbb_t_champs <- bb_games %>% \n  filter(sport == \"WBB\" & lose_team %in% wbb_n_champ) %>% \n  pull(win_team) %>% \n  unique()\n\nwbb_degree_sep <- wbb_t_champs %>% length\n\nfor(x in 1:rounds){\n  wbb_t_champ_beaters <- bb_games %>% \n    filter(sport == \"WBB\" & lose_team %in% wbb_t_champs) %>% \n    pull(win_team) %>% \n    unique()\n  wbb_t_champs <- c(wbb_t_champs, wbb_t_champ_beaters)\n  wbb_t_champs <- wbb_t_champs[!wbb_t_champs == wbb_n_champ] #Remove the national champion from the transitive champs vector\n  wbb_degree_sep <- rbind(wbb_degree_sep, wbb_t_champs %>% unique() %>% length)\n}\n\nwbb_transitive_champs <- wbb_degree_sep %>% \n  as.data.frame() %>% \n  cbind(total_wbb_teams) %>%\n  rename(transitive_champions = V1,\n         total_teams = total_wbb_teams) %>% \n  mutate(degree_of_separation = row_number(),\n         transitive_champ_pct = transitive_champions / total_teams,\n         sport = \"WBB\") \n\n\n\nMBB calculations\n\nmbb_t_champs <- bb_games %>% \n  filter(sport == \"MBB\" & lose_team %in% mbb_n_champ) %>% \n  pull(win_team) %>% \n  unique()\n\nmbb_degree_sep <- mbb_t_champs %>% length\n\nfor(x in 1:rounds){\n  mbb_t_champ_beaters <- bb_games %>% \n    filter(sport == \"MBB\" & lose_team %in% mbb_t_champs) %>% \n    pull(win_team) %>% \n    unique()\n  mbb_t_champs <- c(mbb_t_champs, mbb_t_champ_beaters)\n  mbb_t_champs <- mbb_t_champs[!mbb_t_champs == mbb_n_champ] #Remove the national champion from the transitive champs vector\n  mbb_degree_sep <- rbind(mbb_degree_sep, mbb_t_champs %>% unique() %>% length)\n}\n\nmbb_transitive_champs <- mbb_degree_sep %>% \n  as.data.frame() %>% \n  cbind(total_mbb_teams) %>%\n  rename(transitive_champions = V1,\n         total_teams = total_mbb_teams) %>% \n  mutate(degree_of_separation = row_number(),\n         transitive_champ_pct = transitive_champions / total_teams,\n         sport = \"MBB\") \n\n\n\nBringing the transitive champion data together\n\ntransitive_champs <- bind_rows(\n  wbb_transitive_champs,\n  mbb_transitive_champs) %>% \n  mutate(sport = as_factor(sport))"
  },
  {
    "objectID": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html#results",
    "href": "posts/2019-04-14_fivethirtyeight-riddler-challenge-transitive-champions/index.html#results",
    "title": "FiveThirtyEight Riddler Challenge: ‘’Transitive Champions’’",
    "section": "Results!",
    "text": "Results!\nFor the 2018-2019, I identified the following number of “transitive champs”:\n\nWomen’s Basketball: 360 transitive champions\nMen’s Basketball: 358 transitive champions\n\nEach sport reached the total number of transitive champs within 8 degrees of separation of the national champion. However, transitive champs comprise 66% of total women’s Division I basketball teams compared to 55% in the men’s game, as the plot below shows. This could be due to an effect of major conference teams playing each other more in men’s basketball (limiting opportunities for minor conference teams to grab a transitive championship), but that hypothesis would have to be tested in further analysis.\n\ntransitive_champs %>% \n  ggplot(aes(x=degree_of_separation, y=transitive_champ_pct, color = sport)) +\n  labs(title = \"How many basketball teams beat a team, who beat a team, \\nwho.... beat the national champ?\",\n       subtitle = \"Analysis of 2018-2019 college basketball games\") + \n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  facet_grid(rows = vars(sport), scales = \"free\") +\n  scale_x_continuous(\"Degrees of Separation from Actual National Champion\") +\n  scale_y_continuous(\"Cumulative % of Teams\", labels = scales::percent) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2022-10-03_table-contest-amtrak/index.html",
    "href": "posts/2022-10-03_table-contest-amtrak/index.html",
    "title": "Table Contest 2022 - Amtrak",
    "section": "",
    "text": "https://fonts.google.com/specimen/Recursive\nhttps://data-usdot.opendata.arcgis.com/datasets/usdot::amtrak-routes/explore?location=37.808960%2C-122.262498%2C11.00\nhttps://data-usdot.opendata.arcgis.com/datasets/usdot::amtrak-stations-1/explore?location=43.819125%2C-91.163876%2C7.85\nhttps://www.bts.dot.gov/browse-statistical-products-and-data/state-transportation-statistics/amtrak-ridership"
  },
  {
    "objectID": "posts/2022-10-03_table-contest-amtrak/index.html#routes",
    "href": "posts/2022-10-03_table-contest-amtrak/index.html#routes",
    "title": "Table Contest 2022 - Amtrak",
    "section": "Routes",
    "text": "Routes\nRoute\n\nName (wiki link)\nLogo?\nColor\nTo/From\nDistance (Hours)\nFrequency (daily/weekly/etc)\nJourney time\nTypes of cars\nMap with Stations\nRidership 2012-2021\n\n\nlibrary(htmltools)\n\nhtmltools::tags$link(\n  href = \"https://fonts.googleapis.com/css?family=Recursive:400,600,700&display=swap\", \n  rel = \"stylesheet\")\n\n\n\namtrak_table <- \n  reactable(\n    routes,\n    searchable = TRUE,\n    highlight = TRUE,\n    wrap = FALSE,\n    defaultPageSize = 43,\n    columns = list(\n      name = colDef(\n        name = \"Route\",\n        resizable = TRUE,\n        cell = function(value, index) {\n          tags$a(\n            href = paste0('https://en.wikipedia.org/', routes[index, \"url\"]), \n            target = \"_blank\", \n            value)\n        },\n        minWidth = 200\n      ),\n      route = colDef(\n        name = \"Route\",\n        resizable = TRUE,\n        html = TRUE,\n        minWidth = 100\n      ),\n      daily_round_trips = colDef(\n        name = \"Daily Trips\"\n      ),\n      fy2021passengers_1 = colDef(\n        name = \"Passengers (FY2021)\",\n        defaultSortOrder = \"desc\",\n        format = colFormat(separators = TRUE),\n        width = 120,\n        class = \"number\"\n      ),\n      distance = colDef(\n        name = \"Distance (miles)\"\n      ),\n      time = colDef(\n        name = \"Travel time\"\n      ),\n      url = colDef(show = FALSE),\n      type = colDef(show = FALSE),\n      numbers = colDef(show = FALSE),\n      route_miles = colDef(show = FALSE),\n      operation = colDef(show = FALSE),\n      train_set = colDef(show = FALSE),\n      cars = colDef(show = FALSE)\n    )\n  )\n\ndiv(amtrak_table)\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\namtrak_route_plot <- \n  amtrak_rout_sf %>% \n  mutate(\n    PLOT = map(\n      geometry,\n      ~ggplotly(ggplot(data = .x) +\n          geom_sf() +\n          theme_void())\n      )) %>% \n  st_drop_geometry()\n\nreactable(amtrak_route_plot %>% \n            select(NAME), details = function(index) {\n              amtrak_route_plot$PLOT[[index]]\n            })\n\nreactable(amtrak_route_plot %>% select(STATION), details = function(index) {\n  htmltools::plotTag(amtrak_route_plot$PLOT[[index]], alt=\"plots\")\n})\n\nreactable(\n  amtrak_route_plot,\n  columns = list(\n    route = colDef(cell = function(PLOT){\n      p <- ggplotly(PLOT, height = 100)\n      return(p)\n    })\n  )\n)\n\n%>% \n  reactable()\n\namtrak_rout_sf %>% \n  ggplot() +\n  geom_sf(color = \"purple\") +\n  theme_void()\n\nreactable(amtrak_rout_sf, columns = list(\n  PLOT = colDef(cell = function(value){\n    subRoute <- amtrak_rout_sf[amtrak_rout_sf$NAME==value,]\n    p<-ggplotly(\n      ggplot(subRoute, aes(color = NAME)) + geom_sf()+ theme_void(),\n      height = 100\n    )\n    return(p)\n  })\n))\n\nStations\n\nIcon\nName (wiki link)\nLocation (State)\nStation Type\nConnections\nEstablished Year\nNotes (seasonal, etc.)\nLocal map\nRidership 2012-2021\n\n\nreactable(wiki_routes_df, details = function(index) {\n  station_data <- wiki_stations_df %>% unnest(route)\n  station_data <- station_data[station_data$route == wiki_routes_df$name[index], ]\n  htmltools::div(style = \"padding: 1rem\",\n    reactable(station_data, outlined = TRUE)\n  )\n})\n\n\nsearch_icon <- function(fill = \"none\") {\n    # Icon from https://boxicons.com\n    svg <- sprintf('<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\"><path fill=\"%s\" d=\"M10 18c1.85 0 3.54-.64 4.9-1.69l4.4 4.4 1.4-1.42-4.39-4.4A8 8 0 102 10a8 8 0 008 8.01zm0-14a6 6 0 11-.01 12.01A6 6 0 0110 4z\"/></svg>', fill)\n    sprintf(\"url('data:image/svg+xml;charset=utf-8,%s')\", URLencode(svg))\n}\n\ntest <- browsable(tags$i(search_icon()))"
  },
  {
    "objectID": "posts/2022-12-22_posit-table-contest-amtrak/index.html",
    "href": "posts/2022-12-22_posit-table-contest-amtrak/index.html",
    "title": "Posit Table Contest Submission: Amtrak System",
    "section": "",
    "text": "The following is a repost of my submission in the 2022 Posit Table Contest, which was recognized as one of the runners up in the contest. The original tutorial, stand-alone table, and code repository are available if interested. Huge thanks to the Posit team for hosting this contest.\nTake me directly to the full table in Section 5."
  },
  {
    "objectID": "posts/2022-12-22_posit-table-contest-amtrak/index.html#introduction",
    "href": "posts/2022-12-22_posit-table-contest-amtrak/index.html#introduction",
    "title": "Posit Table Contest Submission: Amtrak System",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this tutorial, I’ll walkthrough the process of developing a table in R using the reactable and reactablefmtr packages and other supplemental packages and functions. The goal is to create a table that describes the Amtrak system, with detail about the routes and the stations. The table displays a route on each row with the included stations along that route available as expandable details. To help visualize the stations and routes, the table includes embedded graphics generated by ggplot2.\n\n1.1 Load packages and prepared data\nThe data used for this table comes from multiple sources including the US Department of Transportation, Wikipedia, and TrainWeb.org. The data preparation script and the generated data files are available in the Github repository.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(reactable)\nlibrary(reactablefmtr)\nlibrary(sf)\nlibrary(lwgeom)\nlibrary(viridis)\nlibrary(ggbump)\nlibrary(tigris)\nlibrary(htmltools)\n\nstations_df <- read_rds(here::here(\"data\", \"amtrak\", \"stations.rds\"))\nroutes_df <- read_rds(here::here(\"data\", \"amtrak\", \"routes.rds\"))\nriders_df <- read_rds(here::here(\"data\", \"amtrak\", \"riders.rds\"))\n\n\n\n\n1.2 Prepare state/provincial boundary data\nTo generate maps of the United States and Canada, the tigris package provides US Census Bureau boundaries of states and the canadianmaps package includes data on provinces.\n\n\nShow the code\nstates_sf <- \n  states(cb = T, progress_bar = F) %>% \n  filter(\n    STUSPS %in% c(state.abb, \"DC\"),\n    !STUSPS %in% c(\"HI\", \"AK\")) %>% \n  select(ST_PROV = STUSPS)\n\nsf_use_s2(FALSE)\n\nprovs_sf <- \n  canadianmaps::PROV %>% \n  st_make_valid() %>% \n  st_transform(crs = st_crs(states_sf)) %>% \n  select(ST_PROV = PT)\n\nstates_prov_sf <- \n  bind_rows(\n    states_sf,\n    provs_sf\n  )"
  },
  {
    "objectID": "posts/2022-12-22_posit-table-contest-amtrak/index.html#routes",
    "href": "posts/2022-12-22_posit-table-contest-amtrak/index.html#routes",
    "title": "Posit Table Contest Submission: Amtrak System",
    "section": "2 Routes",
    "text": "2 Routes\nWe can see that the routes data set includes all 43 active Amtrak routes and 9 variables, such as the beginning and ending cities, passenger volume in fiscal year 2021, length, duration, and which kind of train cars the route includes.\n\n\nShow the code\nglimpse(routes_df)\n\n\nRows: 43\nColumns: 9\n$ name              <chr> \"Northeast Regional\", \"Acela\", \"Pacific Surfliner\", …\n$ route             <chr> \"Boston/Springfield-New York-Washington-Norfolk/Newp…\n$ daily_round_trips <chr> \"18 (weekday), 15 (weekend)\", \"16 (weekday), 4 (Sat)…\n$ fy2021_passengers <dbl> 3960000, 897600, 841000, 613200, 434100, 394300, 354…\n$ route_miles       <chr> \"644 (Newport News);679 (Norfolk);682 (Roanoke)\", \"4…\n$ time              <chr> \"14h 0m\", \"6h 30m \", \"8h 30m\", \"8h 25m\", \"6h 15m (Oa…\n$ url               <chr> \"https://en.wikipedia.org/wiki/Northeast_Regional\", …\n$ cars              <chr> \"Business, Coach, Cafe\", \"First Class, Business, Caf…\n$ geometry          <GEOMETRY [°]> MULTILINESTRING ((-77.47296..., MULTILINEST…\n\n\n\n2.1 Generate route maps for the table\nThe route data set also includes route geometry that we can use for geospatial analysis. In the code below, we create the routes and routes_map dataframes which will be used for the table. routes_map is generated using the route_map_fcn function which generates a ggplot2 map graphic using the route, station, and state/province geometries. We also generated a separate map graphic for the table header (rendered below).\n\n\nShow the code\nroutes <- \n  routes_df %>% \n  mutate(\n    map_plot = NA\n  ) %>% \n  arrange(desc(fy2021_passengers)) %>% \n  bind_cols(\n    route_color = turbo(nrow(routes_df))\n  ) %>% \n  st_drop_geometry() %>% \n  arrange(name)\n\nroute_map_fcn <- function(route_sf, stations_sf, states_sf) {\n  \n  route_mer <- st_set_geometry(route_sf, route_sf$geometry) %>% st_transform(crs = 3857)\n  stations_mer <- stations_sf %>% st_transform(crs = 3857)\n  states_mer <- states_sf %>% st_transform(crs = 3857)\n  route_circle <- st_minimum_bounding_circle(st_simplify(route_mer, dTolerance = 1000))\n  route_bbox <- st_bbox(route_circle) %>% st_as_sfc()\n  route_name <- route_sf$name\n  \n  gg <- \n    (ggplot() +\n       geom_sf(data = states_mer, linetype = \"dashed\", fill = \"gray90\", size = 0.25) +\n       geom_sf(data = route_mer, aes(color = route_color), size = 1.75) +\n       geom_sf(data = stations_mer, shape = 21, size = 1.25, fill = 'white', stroke = 0.5) +\n       scale_color_identity() +\n       coord_sf(\n         xlim = st_coordinates(route_bbox)[c(1,2),1], # min & max of x values\n         ylim = st_coordinates(route_bbox)[c(2,3),2]) + # min & max of y values\n       theme_void() +\n       theme(\n         legend.position = 'none',\n         panel.background = element_rect(fill = 'transparent', color = NA),\n         plot.background = element_rect(fill = 'transparent', color = NA)))\n  \n  result <- tibble(route = route_name, plot = list(gg))\n  \n  return(result)\n}\n\nroutes_stations_sf <- \n  stations_df %>% \n  filter(route != \"Winter Park Express\") %>% \n  group_by(route) %>% \n  summarize() %>% \n  ungroup() %>% \n  arrange(route)\n\nroutes_map_sf <-\n  routes %>%\n  inner_join(\n    routes_df %>% select(name, geometry),\n    by = \"name\") %>%\n  select(\n    name,\n    route_color,\n    geometry) %>% \n  arrange(name)\n\nroutes_map_sf <- st_set_geometry(routes_map_sf, routes_map_sf$geometry)\n\nroutes_map <-\n  map2_dfr(\n    .x = group_split(routes_map_sf, name),\n    .y = group_split(routes_stations_sf, route),\n    .f = ~route_map_fcn(route_sf = .x, stations_sf = .y, states_sf = states_prov_sf))\n\nstate_route_map <- \n  ggplot() +\n  geom_sf(data = states_prov_sf, linetype = \"dashed\", fill = \"gray90\", linewidth = 0.25) +\n  geom_sf(data = routes_map_sf, aes(color = route_color), linewidth = 1.5, alpha = 1.0) +\n  geom_sf(data = routes_stations_sf, shape = 21, size = 1, fill = 'white', stroke = 0.50) +\n  coord_sf(xlim = c(-124.763, -66.949), ylim = c(24.523, 51), expand = FALSE) +\n  scale_color_identity() +\n  theme_void() +\n  theme(legend.position = 'none')\n\nstate_route_map\n\n\n\n\n\n\n\n2.2 Create functions to generate icons representing available train cars\nEach of the routes uses one or more different types of train cars (dining, coach, etc.). To display this information, we use the function below to replace the word descriptions with icons representing whether each type of train car is present for the route.\n\n\nShow the code\nicons <- function(icon, color, size = 30, empty = FALSE) {\n  \n  fill_color <- grDevices::adjustcolor(color, alpha.f = 1.0)\n  empty_color <- grDevices::adjustcolor(color, alpha.f = 0.3)\n  \n  htmltools::tagAppendAttributes(\n    shiny::icon(icon),\n    style = paste0(\"font-size:\", size, \"px\", \"; color:\", if (empty) empty_color else fill_color),\n    \"aria-hidden\" = \"true\"\n    )\n}\n\ntrain_icons <- function(vals) {\n  \n  if(is.na(vals)) {\n    \n    coach <- span(icons(\"train\", \"gray10\", empty = T), title = \"Coach Not Available\", style = \"margin: 5px;\")\n    diner <- span(icons(\"utensils\", \"gray10\", empty = T), title = \"Diner/Cafe Not Available\", style = \"margin: 5px;\")\n    sleeper <- span(icons(\"bed\", \"gray10\", empty = T), title = \"Sleeper Not Available\", style = \"margin: 5px;\")\n    business <- span(icons(\"briefcase\", \"gray10\", empty = T), title = \"Business Not Available\", style = \"margin: 5px;\")\n    first_class <- span(icons(\"money-check-dollar\", \"gray10\", empty = T), title = \"First Class Not Available\", style = \"margin: 5px;\")\n    auto <- span(icons(\"car-side\", \"gray10\", empty = T), title = \"Auto Transport Not Available\", style = \"margin: 5px;\")\n    \n  } else {\n  \n    if (str_detect(vals, \"Coach\")) {\n      coach <- span(icons(\"chair\", \"gray10\", empty = F), title = \"Coach Available\", style = \"margin: 5px;\")\n    } else {\n      coach <- span(icons(\"chair\", \"gray10\", empty = T), title = \"Coach Not Available\", style = \"margin: 5px;\")\n    }\n    if (str_detect(vals, \"Dinner|Dinette|Cafe|Bistro\")) {\n      diner <- span(icons(\"utensils\", \"gray10\", empty = F), title = \"Diner/Cafe Available\", style = \"margin: 5px;\")\n    } else {\n      diner <- span(icons(\"utensils\", \"gray10\", empty = T), title = \"Diner/Cafe Not Available\", style = \"margin: 5px;\")\n    }\n    if (str_detect(vals, \"Sleeper\")) {\n      sleeper <- span(icons(\"bed\", \"gray10\", empty = F), title = \"Sleeper Available\", style = \"margin: 5px;\")\n    } else {\n      sleeper <- span(icons(\"bed\", \"gray10\", empty = T), title = \"Sleeper Not Available\", style = \"margin: 5px;\")\n    }\n    if (str_detect(vals, \"Business\")) {\n      business <- span(icons(\"briefcase\", \"gray10\", empty = F), title = \"Business Available\", style = \"margin: 5px;\")\n    } else {\n      business <- span(icons(\"briefcase\", \"gray10\", empty = T), title = \"Business Not Available\", style = \"margin: 5px;\")\n    }\n    if (str_detect(vals, \"First Class\")) {\n      first_class <- span(icons(\"money-check-dollar\", \"gray10\", empty = F), title = \"First Class Available\", style = \"margin: 5px;\")\n    } else {\n      first_class <- span(icons(\"money-check-dollar\", \"gray10\", empty = T), title = \"First Class Not Available\", style = \"margin: 5px;\")\n    }\n    if (str_detect(vals, \"Auto\")) {\n      auto <- span(icons(\"car-side\", \"gray10\", empty = F), title = \"Auto Transport Available\", style = \"margin: 5px;\")\n    } else {\n      auto <- span(icons(\"car-side\", \"gray10\", empty = T), title = \"Auto Transport Not Available\", style = \"margin: 5px;\")\n    }\n    \n  }\n  \n  div(coach, diner, sleeper, business, first_class, auto)\n  \n}\n\n\n\n\n2.3 Generate sample routes reactable table\nWe can now use the route data frames created above to generate a sample reactable table, using the first 5 rows of the routes data. In this table, we create a bar chart of passenger data using reactablefmtr, display the train car icons, include a map graphic, and format the other columns.\n\n\nShow the code\nreactable(\n    data = routes %>% slice(1:5),\n    highlight = TRUE,\n    wrap = TRUE,\n    defaultPageSize = 5,\n    style = list(\n      fontFamily = \"Recursive, sans-serif\", \n      fontSize = \"0.875rem\"),\n    defaultColDef = colDef(\n      vAlign = \"center\",\n      align = \"center\",\n      headerVAlign = \"center\",\n      sortable = FALSE),\n    width = 1250,\n    defaultSorted = \"fy2021_passengers\",\n    columns = list(\n      name = colDef(\n        name = \"Route\",\n        align = \"left\",\n        html = TRUE,\n        cell = function(value, index) {\n          rte <- tags$strong(tags$a(href = as.character(routes[index, \"url\"]), target = \"_blank\", value))\n          rte_cities <- as.character(routes[index, \"route\"])\n          cities <- div(style = list(float = \"left\", fontSize = \"0.7rem\"), rte_cities)\n          if (value == \"Adirondack\") {\n            sup <- tags$sup(\"*\")\n          } else if (value %in% c(\"Berkshire Flyer\", \"Valley Flyer\")) {\n            sup <- tags$sup(\"**\") \n          } else {\n            sup <- NULL\n          }\n          tagList(rte, sup, tags$br(), cities)\n        },\n        width = 250,\n        sortable = TRUE\n      ),\n      daily_round_trips = colDef(\n        name = \"Daily Trips\",\n        html = TRUE,\n        cell = function(value) {\n          str_replace_all(value, \"\\\\,\", \"<br>\")\n        },\n        width = 150,\n        style = list(fontSize = \"0.8rem\")\n      ),\n      fy2021_passengers = colDef(\n        name = \"Passengers (FY 2021)\",\n        defaultSortOrder = \"desc\",\n        cell = data_bars(\n          routes,\n          fill_color_ref = \"route_color\",\n          text_position = \"above\",\n          number_fmt = scales::comma,\n          background = \"lightgray\"\n        ),\n        width = 125,\n        sortable = TRUE\n      ),\n      route_miles = colDef(\n        name = \"Distance (miles)\",\n        html = TRUE,\n        cell = function(value) {\n          str_replace_all(value, \"\\\\;\", \"<br>\")\n        },\n        width = 150,\n        style = list(fontSize = \"0.8rem\")\n      ),\n      time = colDef(\n        name = \"Journey Time\",\n        html = TRUE,\n        cell = function(value) {\n          str_replace_all(value, \"\\\\;\", \"<br>\")\n        },\n        width = 150,\n        style = list(fontSize = \"0.8rem\")\n      ),\n      cars = colDef(\n        name = \"Available Train Cars\",\n        cell = function(value) {\n          train_icons(value)\n        },\n        width = 175\n      ),\n      map_plot = colDef(\n        name = \"Route Map\",\n        cell = function(value, index){\n          htmltools::plotTag(\n            routes_map$plot[[index]],\n            alt = 'plots',\n            height = 100,\n            width = 100,\n            deviceArgs = list(bg = 'transparent'))\n          },\n        width = 200\n      ),\n      route = colDef(show = FALSE),\n      url = colDef(show = FALSE),\n      route_color = colDef(show = FALSE)\n    )\n) %>% \n  div(\n    .,\n    style = css(\n    'text-align' = 'center')\n  )"
  },
  {
    "objectID": "posts/2022-12-22_posit-table-contest-amtrak/index.html#stations-and-ridership",
    "href": "posts/2022-12-22_posit-table-contest-amtrak/index.html#stations-and-ridership",
    "title": "Posit Table Contest Submission: Amtrak System",
    "section": "3 Stations and ridership",
    "text": "3 Stations and ridership\nThe stations data set includes all 876 active Amtrak route/station combinations and variables such as the station location, what other routes the station serves, when the station opened, and what type of station it is. The data also includes a variable indicating what type of station junction it is along the route (beginning, middle, split, end, etc.).\nThe ridership data includes ridership by station from fiscal year 2005 to 2021.\n\n\nShow the code\nglimpse(stations_df)\n\n\nRows: 876\nColumns: 13\n$ route             <chr> \"Acela\", \"Acela\", \"Acela\", \"Acela\", \"Acela\", \"Acela\"…\n$ stop_num          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ junction_type     <chr> \"sta\", \"cont\", \"cont\", \"cont\", \"cont\", \"cont\", \"cont…\n$ station_name      <chr> \"Boston South (BOS)\", \"Boston Back Bay (BBY)\", \"West…\n$ station_abbr      <chr> \"BOS\", \"BBY\", \"RTE\", \"PVD\", \"NLC\", \"NHV\", \"STM\", \"NY…\n$ state_or_province <chr> \"MA\", \"MA\", \"MA\", \"RI\", \"CT\", \"CT\", \"CT\", \"NY\", \"NJ\"…\n$ country           <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\"…\n$ station_routes    <list> <\"Lake Shore Limited\", \"Northeast Regional\">, <\"Lak…\n$ other_routes      <chr> \"Lake Shore Limited; Northeast Regional\", \"Lake Shor…\n$ opened            <dbl> 1899, 1860, 1953, 1986, 1848, 1920, 1849, 1910, 1935…\n$ station_type      <chr> \"Station Building (with waiting room)\", \"Station Bui…\n$ url               <chr> \"https://en.wikipedia.org/wiki/South_Station\", \"http…\n$ geometry          <POINT [°]> POINT (-71.0553 42.35232), POINT (-71.07583 42…\n\n\nShow the code\nglimpse(riders_df)\n\n\nRows: 8,908\nColumns: 3\n$ station_abbr <chr> \"ABE\", \"ABE\", \"ABE\", \"ABE\", \"ABE\", \"ABE\", \"ABE\", \"ABE\", \"…\n$ year         <int> 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 201…\n$ riders       <dbl> 23438, 18008, 38702, 45052, 44495, 41114, 39878, 43987, 4…\n\n\n\n3.1 Join station and ridership data and create subway-style route diagrams\nSince the stations in a route may be along different branches, it’s helpful to display the data using a route diagram. Some Wikipedia articles (see example) include helpful subway-style route diagrams to display different types of junctions. The route_diagram_fcn below generate specific diagram for each type of junction in ggplot2 in the stations_diag dataframe, which will be used with the stations dataframe in the table.\n\n\nShow the code\nroute_diagram_fcn <- function(node_type, line_color) {\n  \n  if (node_type == \"1-2_split\") {\n    lines_df <-\n      tibble(\n        x = c(-1, 0, 0, 0),\n        y = c(-3, 0, -3, 3),\n        group = c(1, 1, 1, 1)\n      )\n  } else if (node_type == \"1-2_split_aft\") {\n    lines_df <-\n      tibble(\n        x = c(-1, 0, 0, 0),\n        y = c(-3, -1.5, -3, 3),\n        group = c(1, 1, 1, 1)\n      )\n  } else if (node_type == \"2-1_comb\") {\n    lines_df <-\n      tibble(\n        x = c(-1, 0, 0, 0),\n        y = c(3, 0,-3, 3),\n        group = c(1, 1, 1, 1)\n      )\n  } else if (node_type == \"sta\") {\n    lines_df <-\n      tibble(x = c(0, 0),\n             y = c(0, -3),\n             group = c(1, 1))\n  } else if (node_type == \"beg_bypass\") {\n    lines_df <-\n      tibble(x = c(-1, 0, 0),\n             y = c(-3, 0, 3),\n             group = c(1, 1, 1))\n  } else if (node_type == \"cont\") {\n    lines_df <-\n      tibble(x = c(0, 0, 0),\n             y = c(0, 3,-3),\n             group = c(1, 1, 1))\n  } else if (node_type == \"cont_aft_bypass\") {\n    lines_df <-\n      tibble(x = c(-1, 0, 0),\n             y = c(3, 0, -3),\n             group = c(1, 1, 1))\n  } else if (node_type == \"cont_w_bypass\") {\n    lines_df <-\n      tibble(x = c(0, 0, -1, -1),\n             y = c(3, -3, 3, -3),\n             group = c(1, 1, 2, 2))\n  } else if (node_type == \"end\") {\n    lines_df <-\n      tibble(x = c(0, 0),\n             y = c(0, 3),\n             group = c(1, 1))\n  } else if (node_type == \"end_w_bypass\") {\n    lines_df <-\n      tibble(x = c(0, 0, -1, -1),\n             y = c(0, 3, 3, -3),\n             group = c(1, 1, 2, 2))\n  } else if (node_type == \"sta_w_bypass\") {\n    lines_df <-\n      tibble(x = c(0, 0, -1, -1),\n             y = c(0, -3, 3, -3),\n             group = c(1, 1, 2, 2))\n  }\n  \n  pts_df <- \n  tibble(\n    x = 0,\n    y = 0\n  )\n  \n  ggplot() +\n    geom_bump(\n      data = lines_df, \n      aes(x, y, group = group), \n      linewidth = 4, \n      color = line_color, \n      direction = \"y\") +\n    geom_point(\n      data = pts_df, \n      aes(x, y), \n      shape = 21, \n      size = 10, \n      stroke = 2,\n      color = \"gray20\", \n      fill = \"white\") +\n    scale_color_identity() +\n    scale_x_continuous(expand = c(0, 0)) + \n    scale_y_continuous(expand = c(0, 0)) +\n    coord_cartesian(xlim = c(-2, 2), ylim = c(-3, 3)) +\n    theme_void()\n}\n\nriders <- \n  riders_df %>% \n  separate_rows(station_abbr, sep = \"\\\\/\") %>% \n  arrange(year) %>% \n  group_by(station_abbr) %>% \n  summarize(riders = list(riders)) %>% \n  ungroup()\n\nstations <- \n  stations_df %>% \n  st_drop_geometry() %>% \n  inner_join(\n    routes %>% select(route = name, route_color),\n    by = \"route\"\n  ) %>% \n  inner_join(\n    tibble(\n      state_or_province = c(state.abb, \"DC\", \"ON\", \"QC\", \"BC\"),\n      st_prov_name = c(state.name, \"District of Columbia\", \"Ontario\", \"Quebec\", \"British Columbia\")\n    ),\n    by = \"state_or_province\"\n  ) %>% \n  left_join(\n    riders, by = \"station_abbr\"\n  ) %>% \n  select(-station_abbr) %>% \n  transmute(\n    route,\n    route_color,\n    junction_type,\n    station_name,\n    url,\n    st_prov_name,\n    country,\n    riders = modify_if(riders, ~is.null(.), ~rep(NA_real_, 17)),\n    opened,\n    station_type,\n    station_routes = map(station_routes, ~str_subset(.x, \"Winter Park Express\", negate = T)),\n    other_routes = modify_if(station_routes, ~length(.) == 0, ~NA_character_),\n    other_routes = map_chr(other_routes, ~glue::glue_collapse(.x, sep = \"; \"))\n  )\n\nstations_diag <-\n  stations %>%\n  select(route, station_name, junction_type, route_color) %>%\n  mutate(PLOT = map2(junction_type, route_color, ~route_diagram_fcn(node_type = .x, line_color = .y)))\n\n\n\n\n3.2 Generate sample stations reactable table\nSimilar to the routes table above, we can use the stations data frames created above to generate a sample reactable table, using the stations on the Northeast Regional route. In this table, we embed the route diagram for the station, create a sparkline chart of passenger data using reactablefmtr, display the connecting routes using a details breakout, and format the other columns.\n\n\nShow the code\nstat_data <- stations %>% filter(route == \"Northeast Regional\")\nstat_route_color <- stat_data %>% pull(route_color) %>% unique()\nstat_plot <- inner_join(stations_diag, stat_data, by = c(\"route\", \"station_name\"))\n      \nreactable(\n  stat_data,\n  outlined = FALSE,\n  theme = reactableTheme(\n    cellPadding = \"0px 6px\",\n    style = list(\".rt-tr-details\" = list(\"text-align\" = \"right\"))\n  ),\n  style = list(fontFamily = \"Recursive, sans-serif\",\n               fontSize = \"0.875rem\"),\n  defaultPageSize = nrow(stat_data),\n  defaultColDef = colDef(\n    vAlign = \"center\",\n    align = \"center\",\n    headerVAlign = \"center\"\n  ),\n  width = 1250,\n  sortable = FALSE,\n  columns = list(\n    junction_type = colDef(\n      name = \"\",\n      resizable = FALSE,\n      align = \"left\",\n      cell = function(value, index) {\n        htmltools::plotTag(\n          stat_plot$PLOT[[index]],\n          alt = 'plots',\n          height = 100,\n          width = 100\n        )\n      },\n      width = 100\n    ),\n    station_name = colDef(\n      name = \"Station\",\n      resizable = TRUE,\n      align = \"left\",\n      cell = function(value, index) {\n        stat_url <-\n          tags$a(href = as.character(stat_data[index, \"url\"]), target = \"_blank\", value)\n        state <-\n          as.character(stat_data[index, \"st_prov_name\"])\n        country <- as.character(stat_data[index, \"country\"])\n        flag_url <- paste0(\n          \"https://raw.githubusercontent.com/catamphetamine/country-flag-icons/master/flags/1x1/\",\n          country,\n          \".svg\"\n        )\n        flag_img <-\n          image <-\n          img(src = flag_url, style = \"width:45px;height:15px;\", alt = country)\n        state_div <-\n          div(style = list(float = \"left\", fontSize = \"0.7rem\"),)\n        tagList(stat_url, tags$br(), state, flag_img)\n      },\n      minWidth = 250\n    ),\n    riders = colDef(\n      name = \"Station Ridership (FY 2005-2021)\",\n      cell = react_sparkline(\n        stat_data,\n        decimals = 0,\n        tooltip_type = 2,\n        height = 100,\n        show_area = TRUE,\n        line_width = 2,\n        area_color_ref = \"route_color\",\n        area_opacity = 0.5,\n        margin = margin(10, 5, 10, 0)\n      ),\n      width = 450\n    ),\n    opened = colDef(\n      name = \"Year Opened (Rebuilt)\",\n      cell = color_tiles(\n        data = stat_data,\n        colors = stat_route_color %>% shades::saturation(seq(0.2, 1, 0.2)) %>% as.character(),\n        opacity = 0.7,\n        bold_text = FALSE,\n        box_shadow = FALSE\n      ),\n      width = 150\n    ),\n    station_type = colDef(\n      name = \"Station Type\",\n      cell = function(value, index) {\n        if (is.na(value)) {\n          station_icon <- 'train'\n        } else if (value == \"Station Building (with waiting room)\") {\n          station_icon <- 'building-user'\n        } else if (value == \"Platform with Shelter\") {\n          station_icon <- 'people-roof'\n        } else {\n          station_icon <- 'train'\n        }\n        span(icons(station_icon, \"gray10\", empty = F),\n             title = value,\n             style = \"margin: 5px;\")\n      },\n      width = 125\n    ),\n    station_routes = colDef(\n      name = \"Connecting Routes\",\n      html = TRUE,\n      cell = function(value, index) {\n        if (length(value) == 0) {\n          \"NA\"\n        } else {\n          paste0(length(value), \" routes\")\n        }\n      },\n      details = function(index) {\n        if (length(stat_data$station_routes[index][[1]]) > 0) {\n          connections <- stat_data$other_routes[index]\n          paste0(\"Connecting to: \", connections)\n        }\n      },\n      width = 125\n    ),\n    route = colDef(show = FALSE),\n    stop_num = colDef(show = FALSE),\n    url = colDef(show = FALSE),\n    state_or_province = colDef(show = FALSE),\n    country = colDef(show = FALSE),\n    other_routes = colDef(show = FALSE),\n    route_color = colDef(show = FALSE),\n    st_prov_name = colDef(show = FALSE)\n  )\n) %>% \n  div(\n    .,\n    style = css(\n    'text-align' = 'center')\n  )"
  },
  {
    "objectID": "posts/2022-12-22_posit-table-contest-amtrak/index.html#generate-nested-reactable-table-including-routes-and-stations",
    "href": "posts/2022-12-22_posit-table-contest-amtrak/index.html#generate-nested-reactable-table-including-routes-and-stations",
    "title": "Posit Table Contest Submission: Amtrak System",
    "section": "4 Generate nested reactable table including routes and stations",
    "text": "4 Generate nested reactable table including routes and stations\nFinally, we can bring together the routes and stations data into one reactable table. After creating the amtrak_table object, we add headers and footers using the prependContent and appendContent functions in the htmlwidgets package. The header includes the map graphic of the whole Amtrak system as well as the title and subtitle. The footer includes footnotes and source information.\n\n\nShow the code\namtrak_table <- \n  reactable(\n    data = routes,\n    highlight = TRUE,\n    wrap = TRUE,\n    defaultPageSize = nrow(routes),\n    style = list(\n      fontFamily = \"Recursive, sans-serif\", \n      fontSize = \"0.875rem\"),\n    defaultColDef = colDef(\n      vAlign = \"center\",\n      align = \"center\",\n      headerVAlign = \"center\",\n      sortable = FALSE),\n    width = 1250,\n    defaultSorted = \"fy2021_passengers\",\n    columns = list(\n      name = colDef(\n        name = \"Route\",\n        align = \"left\",\n        html = TRUE,\n        cell = function(value, index) {\n          rte <- tags$strong(tags$a(href = as.character(routes[index, \"url\"]), target = \"_blank\", value))\n          rte_cities <- as.character(routes[index, \"route\"])\n          cities <- div(style = list(float = \"left\", fontSize = \"0.7rem\"), rte_cities)\n          if (value == \"Adirondack\") {\n            sup <- tags$sup(\"*\")\n          } else if (value %in% c(\"Berkshire Flyer\", \"Valley Flyer\")) {\n            sup <- tags$sup(\"**\") \n          } else {\n            sup <- NULL\n          }\n          tagList(rte, sup, tags$br(), cities)\n        },\n        width = 250,\n        sortable = TRUE\n      ),\n      daily_round_trips = colDef(\n        name = \"Daily Trips\",\n        html = TRUE,\n        cell = function(value) {\n          str_replace_all(value, \"\\\\,\", \"<br>\")\n        },\n        width = 150,\n        style = list(fontSize = \"0.8rem\")\n      ),\n      fy2021_passengers = colDef(\n        name = \"Passengers (FY 2021)\",\n        defaultSortOrder = \"desc\",\n        cell = data_bars(\n          routes,\n          fill_color_ref = \"route_color\",\n          text_position = \"above\",\n          number_fmt = scales::comma,\n          background = \"lightgray\"\n        ),\n        width = 125,\n        sortable = TRUE\n      ),\n      route_miles = colDef(\n        name = \"Distance (miles)\",\n        html = TRUE,\n        cell = function(value) {\n          str_replace_all(value, \"\\\\;\", \"<br>\")\n        },\n        width = 150,\n        style = list(fontSize = \"0.8rem\")\n      ),\n      time = colDef(\n        name = \"Journey Time\",\n        html = TRUE,\n        cell = function(value) {\n          str_replace_all(value, \"\\\\;\", \"<br>\")\n        },\n        width = 150,\n        style = list(fontSize = \"0.8rem\")\n      ),\n      cars = colDef(\n        name = \"Available Train Cars\",\n        cell = function(value) {\n          train_icons(value)\n        },\n        width = 175\n      ),\n      map_plot = colDef(\n        name = \"Route Map\",\n        cell = function(value, index){\n          htmltools::plotTag(\n            routes_map$plot[[index]],\n            alt = 'plots',\n            height = 100,\n            width = 100,\n            deviceArgs = list(bg = 'transparent'))\n          },\n        width = 200\n      ),\n      route = colDef(show = FALSE),\n      url = colDef(show = FALSE),\n      route_color = colDef(show = FALSE)\n    ),\n    details = function(index) {\n      station_data <- stations[stations$route == routes$name[index], ]\n      station_route_color <- station_data %>% pull(route_color) %>% unique()\n      htmltools::div(\n        style = \"padding: 0rem\",\n        reactable(\n          station_data, \n          outlined = FALSE,\n          theme = reactableTheme(\n            cellPadding = \"0px 6px\",\n            style = list(\".rt-tr-details\" = list(\"text-align\" = \"right\"))\n          ),\n          defaultPageSize = nrow(station_data),\n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\",\n            headerVAlign = \"center\"),\n          width = 1250,\n          sortable = FALSE,\n          columns = list(\n            junction_type = colDef(\n              name = \"\",\n              resizable = FALSE,\n              align = \"left\",\n              cell = function(value, index){\n                station_plot <- inner_join(stations_diag, station_data, by = c(\"route\", \"station_name\"))\n                htmltools::plotTag(\n                  station_plot$PLOT[[index]],\n                  alt = 'plots',\n                  height = 100,\n                  width = 100)\n                },\n              width = 100\n            ),\n            station_name = colDef(\n              name = \"Station\",\n              resizable = TRUE,\n              align = \"left\",\n              cell = function(value, index) {\n                stat_url <- tags$a(href = as.character(station_data[index, \"url\"]), target = \"_blank\", value)\n                state <- as.character(station_data[index, \"st_prov_name\"])\n                country <- as.character(station_data[index, \"country\"])\n                flag_url <- paste0(\n                  \"https://raw.githubusercontent.com/catamphetamine/country-flag-icons/master/flags/1x1/\", \n                  country, \".svg\")\n                flag_img <- image <- img(src = flag_url, style = \"width:45px;height:15px;\", alt = country)\n                state_div <- div(style = list(float = \"left\", fontSize = \"0.7rem\"), )\n                tagList(stat_url, tags$br(), state, flag_img)\n                },\n              minWidth = 250\n              ),\n            riders = colDef(\n              name = \"Station Ridership (FY 2005-2021)\",\n              cell = react_sparkline(\n                station_data,\n                decimals = 0,\n                tooltip_type = 2,\n                height = 100,\n                show_area = TRUE,\n                line_width = 2,\n                area_color_ref = \"route_color\",\n                area_opacity = 0.5,\n                margin = margin(10, 5, 10, 0)\n                ),\n              width = 450),\n            opened = colDef(\n              name = \"Year Opened (Rebuilt)\",\n               cell = color_tiles(\n                 data = station_data,\n                 colors = station_route_color %>% shades::saturation(seq(0.2, 1, 0.2)) %>% as.character(),\n                 opacity = 0.7,\n                 bold_text = FALSE,\n                 box_shadow = FALSE\n                 ),\n              width = 150\n              ),\n            station_type = colDef(\n              name = \"Station Type\",\n              cell = function(value, index) {\n                if (is.na(value)) {\n                  station_icon <- 'train'\n                } else if (value == \"Station Building (with waiting room)\") {\n                  station_icon <- 'building-user'\n                } else if (value == \"Platform with Shelter\") {\n                  station_icon <- 'people-roof'\n                } else {\n                  station_icon <- 'train'\n                }\n                span(icons(station_icon, \"gray10\", empty = F), title = value, style = \"margin: 5px;\")\n              },\n              width = 125\n            ),\n            station_routes = colDef(\n              name = \"Connecting Routes\",\n              html = TRUE,\n              cell = function(value, index) {\n                if (length(value) == 0) {\n                  \"NA\"\n                } else {\n                  paste0(length(value), \" routes\")\n                }\n              },\n              details = function(index) {\n                if (length(station_data$station_routes[index][[1]]) > 0) {\n                  connections <- station_data$other_routes[index]\n                  paste0(\"Connecting to: \", connections)\n                  }\n                },\n              width = 125\n              ),\n            route = colDef(show = FALSE),\n            stop_num = colDef(show = FALSE),\n            url = colDef(show = FALSE),\n            state_or_province = colDef(show = FALSE),\n            country = colDef(show = FALSE),\n            other_routes = colDef(show = FALSE),\n            route_color = colDef(show = FALSE),\n            st_prov_name = colDef(show = FALSE)\n          ))\n      )\n      }\n  )\n\n\n\n\nShow the code\namtrak_table_final <- \n  amtrak_table %>%\n  # add title, subtitle, and map\n  htmlwidgets::prependContent(\n    tags$div(\n      tags$link(\n        href = \"https://fonts.googleapis.com/css?family=Recursive:400,600,700&display=swap\", \n        rel = \"stylesheet\"),\n      tags$div(\n        tags$div(\n          \"All Aboard!\", \n        style = css(\n          'font-size' = '60pt', \n          'font-weight' = 'bold', \n          'font-family' = 'Recursive', \n          'text-align' = 'left',\n          'margin-bottom' = 0,\n          'padding-left' = '10px',\n          'vertical-align' = 'middle')\n        ),\n        tags$div(\n          \"Exploring the Amtrak Passenger Rail System\", \n          style = css(\n            'font-family' = 'Recursive',\n            'margin-bottom' = 0,\n            'margin-top' = 0,\n            'font-size' = '28pt',\n            'text-align' = 'left',\n            color = '#8C8C8C',\n            'padding-left' = '10px')\n          ),\n        style = css(width = '70%')\n      ),\n      tags$div(\n        plotTag(\n          state_route_map,\n          alt = \"Map of all Amtrak routes\",\n          height = 200\n          ),\n        style = css(width = '30%')),\n      style = css(\n        width = '1250px',\n        display = 'inline-flex'))) %>%\n  # add footnotes and source notes\n  htmlwidgets::appendContent(\n    tags$div(\n      tags$link(\n        href = \"https://fonts.googleapis.com/css?family=Recursive:400,600,700&display=swap\", \n        rel = \"stylesheet\"),\n      tags$sup(\"*\"), \n      \"Amtrak suspended Adirondack service in July 2021, and no resumption date has been set as of October 2022.\",\n      tags$br(),\n      tags$sup(\"**\"), \n      \"Berkshire Flyer seasonal service began in 2022, and Valley Flyer service began in 2019.\",\n      style = css(\n        display = 'inline-block',\n        'text-align' = 'left',\n        'font-family' = 'Recursive',\n        color = 'black', \n        'font-size' = '9pt',\n        'border-bottom-style' = 'solid',\n        'border-top-style' = 'solid',\n        width = '1250px',\n        'padding-bottom' = '8px',\n        'padding-top' = '8px',\n        'padding-left' = '10px',\n        'border-color' = '#DADADA')),\n    tags$div(\n      tags$link(\n        href = \"https://fonts.googleapis.com/css?family=Roboto:400,600,700&display=swap\", \n        rel = \"stylesheet\"),\n      tags$div(\n        \"Data Sources: Wikipedia, US Dept of Transportation, US Census Bureau, TrainWeb.org, and OpenStreetMaps | \",\n        style = css(\n          display = 'inline-block', \n          'vertical-align' = 'middle')),\n      tags$div(\n        shiny::icon(\"twitter\"), \n        style = css(\n          display = 'inline-block', \n          'vertical-align' = 'middle')),\n      tags$div(\n        tags$a(\"@joshfangmeier\", href = \"https://twitter.com/joshfangmeier\", target = \"_blank\"),\n        style = css(\n          display = 'inline-block', \n          'vertical-align' = 'middle')),\n      tags$div(\n        shiny::icon(\"github\"), \n        style = css(\n          display = 'inline-block', \n          'vertical-align' = 'middle')),\n      tags$div(\n        tags$a(\"jfangmeier\", href = \"https://github.com/jfangmeier\", target = \"_blank\"), \n        style = css(\n          display = 'inline-block', \n          'vertical-align' = 'middle')),\n      style = css(\n        'text-align' = 'left',\n        'font-family' = 'Roboto', \n        color = '#8C8C8C', \n        'font-size' = '10pt', \n        width = '1250px', \n        'padding-top' = '8px', \n        'padding-left' = '10px',\n        display = 'inline-block', \n        'vertical-align' = 'middle')\n      )\n  )"
  },
  {
    "objectID": "posts/2022-12-22_posit-table-contest-amtrak/index.html#sec-table",
    "href": "posts/2022-12-22_posit-table-contest-amtrak/index.html#sec-table",
    "title": "Posit Table Contest Submission: Amtrak System",
    "section": "5 Display the table",
    "text": "5 Display the table\n\n\n\n\n\n\n\nAll Aboard!\nExploring the Amtrak Passenger Rail System\n\n\n\n\n\n\n\n\n*\nAmtrak suspended Adirondack service in July 2021, and no resumption date has been set as of October 2022.\n\n**\nBerkshire Flyer seasonal service began in 2022, and Valley Flyer service began in 2019.\n\n\n\nData Sources: Wikipedia, US Dept of Transportation, US Census Bureau, TrainWeb.org, and OpenStreetMaps | \n\n\n\n\n@joshfangmeier\n\n\n\n\n\njfangmeier"
  }
]